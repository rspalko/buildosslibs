/*****************************************************************************/
// File: avx2_stripe_transfer.cpp [scope = APPS/SUPPORT]
// Version: Kakadu, V7.9
// Author: David Taubman
// Last Revised: 8 January, 2017
/*****************************************************************************/
// Copyright 2001, David Taubman, The University of New South Wales (UNSW)
// The copyright owner is Unisearch Ltd, Australia (commercial arm of UNSW)
// Neither this copyright statement, nor the licensing details below
// may be removed from this file or dissociated from its contents.
/*****************************************************************************/
// Licensee: Open Systems Integration; Inc
// License number: 01368
// The licensee has been granted a NON-COMMERCIAL license to the contents of
// this source file.  A brief summary of this license appears below.  This
// summary is not to be relied upon in preference to the full text of the
// license agreement, accepted at purchase of the license.
// 1. The Licensee has the right to install and use the Kakadu software and
//    to develop Applications for the Licensee's own use.
// 2. The Licensee has the right to Deploy Applications built using the
//    Kakadu software to Third Parties, so long as such Deployment does not
//    result in any direct or indirect financial return to the Licensee or
//    any other Third Party, which further supplies or otherwise uses such
//    Applications.
// 3. The Licensee has the right to distribute Reusable Code (including
//    source code and dynamically or statically linked libraries) to a Third
//    Party, provided the Third Party possesses a license to use the Kakadu
//    software, and provided such distribution does not result in any direct
//    or indirect financial return to the Licensee.
/******************************************************************************
Description:
   Provides SIMD implementations to accelerate the conversion and transfer of
data between the line buffers generated by `kdu_multi_synthesis' or
`kdu_multi_analysis' and the (possibly interleaved) application-supplied
sample buffers supplied to `kdu_stripe_decompressor::pull_stripe' or
`kdu_stripe_compressor::push_stripe'.  This file provides conversion
functions that require AVX2 support.
******************************************************************************/
#include "kdu_arch.h"

#if ((!defined KDU_NO_AVX2) && (defined KDU_X86_INTRINSICS))

#ifdef _MSC_VER
#  include <intrin.h>
#else
#  include <immintrin.h>
#endif // !_MSC_VER

#include <assert.h>

namespace kd_supp_simd {
  using namespace kdu_core;

#define KDU_STRIPE_STORE_PREF_STREAMING ((int) 1)
  /* Direct copy of the definition found in "kdu_stripe_decompressor.h". */

// Shuffle control for converting interleaved colour components to
// contiguous groups of components, within each 128-bit lane.  The suffix
// "_556" means that the interleaved sequence (starting from the MSB position)
// [R B G R B G ... B G R] is converted to 5 blues in the MSBs followed
// by 5 greens and then 6 reds in the LSBs.  Similarly, the suffix "_565"
// means that we are converting the interleaved sequence [G R B G R ... R B G],
// while the "_655" suffix converts [B G R B G ... G R B] to 6 blues in the
// MSB positions, followed by 5 greens and then 5 reds in the LSB positions.
static kdu_byte kd_shuffle_r556[32]; // These are all initialized from
static kdu_byte kd_shuffle_r565[32]; // `avx2_stripe_transfer_static_init'
static kdu_byte kd_shuffle_r655[32]; // when it is safe to do so.

// Shuffle control for converting contiguous groups of colour components
// to fully interleaved components within each 128-bit lane.  The suffix
// "_556" means that the contiguous group starts out with 5 blues in the
// MSBs, then 5 greens and then 6 reds in the LSBs -- the interleaved
// pattern, starting from the MSB position, will be [R B G R B G ... B G R].
// Similarly, the suffix "_565" produces [G R B G R ... R B G], while the
// "_655" suffix yields [B G R B G ... G R B].
static kdu_byte kd_shuffle_556[32]; // Also initialized from
static kdu_byte kd_shuffle_565[32]; // `avx2_stripe_transfer_static_init'
static kdu_byte kd_shuffle_655[32];

// Shuffle control for converting 4-way interleaved data to 4 groups of
// samples from the same channel, within each 128-bit lane
static kdu_byte kd_shuffle_r4444[32]; // Also initialized from
                                      // `avx2_stripe_transfer_static_init'

// Dword permutation control for interleaving corresponding dwords from each
// 128-bit lane of a 256-bit vector
static kdu_uint32 kd_ilv_dwords[8]; // See `avx2_stripe_transfer_static_init'

// Dword permutation control for reversing the above permutation
static kdu_uint32 kd_dilv_dwords[8]; // See `avx2_stripe_transfer_static_init'


/* ========================================================================= */
/*                         Safe Static Initializers                          */
/* ========================================================================= */

/*****************************************************************************/
/* EXTERN              avx2_stripe_transfer_static_init                      */
/*****************************************************************************/

void avx2_stripe_transfer_static_init()
{ // Static initializers are potentially dangerous, so we initialize here
  kdu_byte shuffle_r556[16] =
    { 0, 3, 6, 9, 12, 15, 1, 4, 7, 10, 13, 2, 5, 8, 11, 14 };
  kdu_byte shuffle_r565[16] =
    { 2, 5, 8, 11, 14, 0, 3, 6, 9, 12, 15, 1, 4, 7, 10, 13 };
  kdu_byte shuffle_r655[16] =
    { 1, 4, 7, 10, 13, 2, 5, 8, 11, 14, 0, 3, 6, 9, 12, 15 };
  
  kdu_byte shuffle_556[16]  =
    { 0, 6, 11, 1, 7, 12, 2, 8, 13, 3, 9, 14, 4, 10, 15, 5 };
  kdu_byte shuffle_565[16] =
    { 5, 11, 0, 6, 12, 1, 7, 13, 2, 8, 14, 3, 9, 15, 4, 10 };
  kdu_byte shuffle_655[16] =
    { 10, 0, 5, 11, 1, 6, 12, 2, 7, 13, 3, 8, 14, 4, 9, 15 };
  
  kdu_byte shuffle_r4444[16] =
    { 0, 4, 8, 12, 1, 5, 9, 13, 2, 6, 10, 14, 3, 7, 11, 15 };
  
  kdu_uint32 ilv_dwords[8] = { 0, 4, 1, 5, 2, 6, 3, 7 };
  kdu_uint32 dilv_dwords[8] = { 0, 2, 4, 6, 1, 3, 5, 7 };

  int n;
  for (n=0; n < 16; n++)
    { 
      kd_shuffle_r556[n] = kd_shuffle_r556[16+n] = shuffle_r556[n];
      kd_shuffle_r565[n] = kd_shuffle_r565[16+n] = shuffle_r565[n];
      kd_shuffle_r655[n] = kd_shuffle_r655[16+n] = shuffle_r655[n];
      
      kd_shuffle_556[n] = kd_shuffle_556[16+n] = shuffle_556[n];
      kd_shuffle_565[n] = kd_shuffle_565[16+n] = shuffle_565[n];
      kd_shuffle_655[n] = kd_shuffle_655[16+n] = shuffle_655[n];
      
      kd_shuffle_r4444[n] = kd_shuffle_r4444[16+n] = shuffle_r4444[n];
    }
  for (n=0; n < 8; n++)
    { 
      kd_ilv_dwords[n] = ilv_dwords[n];
      kd_dilv_dwords[n] = dilv_dwords[n];
    }
}


/* ========================================================================= */
/*               SIMD functions used by `kdu_stripe_compressor'              */
/* ========================================================================= */

/*****************************************************************************/
/* EXTERN               avx2_int16_from_uint8_ilv1                           */
/*****************************************************************************/

void
  avx2_int16_from_uint8_ilv1(kdu_int16 **dst, kdu_byte *src, int width,
                             int src_precision, int tgt_precision,
                             bool is_absolute, bool src_signed)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(!src_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    tgt_precision = KDU_FIX_POINT;
  int offset = (src_signed) ? 0 : (1 << (src_precision - 1));
  int pre_upshift = 16 - src_precision;
  int post_downshift = 16 - tgt_precision;

  __m256i vec_off = _mm256_set1_epi8((kdu_byte)offset);
  __m128i vec_left = _mm_cvtsi32_si128(pre_upshift);
  __m128i vec_right = _mm_cvtsi32_si128(post_downshift);
  __m256i v1, v2;

  // Process all but the last 1-32 input bytes using aligned stores
  kdu_byte *sp = src;
  kdu_int16 *dp = dst[0];
  for (; width > 32; width -= 32, sp += 32, dp += 32)
    { 
      // Load 32 byte vector and swap components so that the low lane has
      // bytes [0-7] and [16-23] while the high lane has [8-15] and [24-31]
      v2 = _mm256_loadu_si256((__m256i *) sp);
      v2 = _mm256_permute4x64_epi64(v2, 0xD8);
      v2 = _mm256_sub_epi8(v2,vec_off);

      // Unpack bytes [0-7] and [8-15] to the 2 lanes of v1
      v1 = _mm256_unpacklo_epi8(v2,v2);
      
      // Unpack bytes [16-23] and [24-31] to the 2 lanes of v2
      v2 = _mm256_unpackhi_epi8(v2,v2);
      
      // Finish by upshifting, downshifting and saving the results
      v1 = _mm256_sll_epi16(v1,vec_left);
      v2 = _mm256_sll_epi16(v2,vec_left);
      ((__m256i *) dp)[0] = _mm256_sra_epi16(v1,vec_right);
      ((__m256i *) dp)[1] = _mm256_sra_epi16(v2,vec_right);
    }

  // Finish with 2 unaligned stores, after first shifting the source and
  // destination buffers back sufficiently to ensure that we neither overwrite
  // the output buffer nor overread the input buffer.
  int backtrack = ((-width) & 31);
  sp -= backtrack; dp -= backtrack;
  v2 = _mm256_loadu_si256((__m256i *) sp);
  v2 = _mm256_permute4x64_epi64(v2, 0xD8);
  v2 = _mm256_sub_epi8(v2,vec_off);
  v1 = _mm256_unpacklo_epi8(v2,v2);
  v2 = _mm256_unpackhi_epi8(v2,v2);
  v1 = _mm256_sll_epi16(v1, vec_left);
  v2 = _mm256_sll_epi16(v2,vec_left);
  _mm256_storeu_si256((__m256i *)(dp+0),_mm256_sra_epi16(v1, vec_right));
  _mm256_storeu_si256((__m256i *)(dp+16),_mm256_sra_epi16(v2, vec_right));
}

/*****************************************************************************/
/* EXTERN               avx2_int16_from_uint8_ilv3                           */
/*****************************************************************************/

void
  avx2_int16_from_uint8_ilv3(kdu_int16 **dst, kdu_byte *src, int width,
                             int src_precision, int tgt_precision,
                             bool is_absolute, bool src_signed)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(!src_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    tgt_precision = KDU_FIX_POINT;
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int pre_upshift = 16 - src_precision;
  int post_downshift = 16 - tgt_precision;

  __m256i vec_shuffle_556 = _mm256_loadu_si256((__m256i *) kd_shuffle_r556);
  __m256i vec_shuffle_565 = _mm256_loadu_si256((__m256i *) kd_shuffle_r565);
  __m256i vec_shuffle_655 = _mm256_loadu_si256((__m256i *) kd_shuffle_r655);
  __m256i vec_off =   _mm256_set1_epi8((kdu_byte)offset);
  __m128i vec_left =  _mm_cvtsi32_si128(pre_upshift);
  __m128i vec_right = _mm_cvtsi32_si128(post_downshift);
  __m256i v1, v2, v3, vt, vb;

  // Process all but the last 1-32 input triplets using aligned stores
  kdu_int16 *dp1 = dst[0];
  kdu_int16 *dp2 = dst[1];
  kdu_int16 *dp3 = dst[2];
  kdu_byte *sp = src;
  for (; width > 32; width-=32, sp+=96, dp1+=32, dp2+=32, dp3+=32)
    { 
      // Load and de-interleave 3 vectors of bytes so that each lane of
      // v1 holds [5B | 5G | 6R], each lane of v2 holds [5B | 6G | 5R] and
      // each lane of v3 holds [6B | 5G | 5R], all listed from MSB to LSB 
      vt = _mm256_loadu_si256((__m256i *) sp);
      vb = _mm256_loadu_si256((__m256i *)(sp+32));
      v3 = _mm256_loadu_si256((__m256i *)(sp+64));
      v1 = _mm256_permute2x128_si256(vt, vb, 0x30); // v1 holds dqwords 0, 3
      v2 = _mm256_permute2x128_si256(vt, v3, 0x21); // v2 holds dqwords 1, 4
      v3 = _mm256_permute2x128_si256(vb, v3, 0x30); // v3 holds dqwords 2, 5
      v1 = _mm256_shuffle_epi8(v1, vec_shuffle_556); // [5B | 5G | 6R] per lane
      v2 = _mm256_shuffle_epi8(v2, vec_shuffle_565); // [5B | 6G | 5R] per lane
      v3 = _mm256_shuffle_epi8(v3, vec_shuffle_655); // [6B | 5G | 5R] per lane

      // Combine v1, v2 and v3, separately in each lane, so that each lance of
      // vt holds 16R, each lane of vb holds 16G and each lane of v3 holds 16B
      vt = _mm256_slli_si256(v1, 10);    // Move 6 reds to to MSBs
      vt = _mm256_alignr_epi8(v2, vt, 5);  v2 = _mm256_srli_si256(v2, 5);
      vt = _mm256_alignr_epi8(v3, vt, 5);  v3 = _mm256_srli_si256(v3, 5);
      vt = _mm256_sub_epi8(vt, vec_off); // Level adjused reds in vt
      vb = _mm256_slli_si256(v1, 5);     // Move 5 greens to MSBs
      vb = _mm256_alignr_epi8(v2, vb, 6);  v2 = _mm256_srli_si256(v2, 6);
      vb = _mm256_alignr_epi8(v3, vb, 5);  v3 = _mm256_srli_si256(v3, 5);
      vb = _mm256_sub_epi8(vb, vec_off); // Level adjusted greens in vb      
      v2 = _mm256_alignr_epi8(v2, v1, 5);  v3 = _mm256_alignr_epi8(v3, v2, 6);
      v3 = _mm256_sub_epi8(v3, vec_off); // Level adjusted blues in v3

      // Rearrange the lanes of vt, vb and v3 so that in each case the low lane
      // has bytes [0-7] and [16-23] while the high lane has [8-15] and [24-31] 
      vt = _mm256_permute4x64_epi64(vt, 0xD8);
      vb = _mm256_permute4x64_epi64(vb, 0xD8);
      v3 = _mm256_permute4x64_epi64(v3, 0xD8);

      // Expand, shift and write the channel data
      v1 = _mm256_unpacklo_epi8(vt,vt);    vt = _mm256_unpackhi_epi8(vt, vt);
      v1 = _mm256_sll_epi16(v1,vec_left);  vt = _mm256_sll_epi16(vt, vec_left);
      ((__m256i *) dp1)[0] = _mm256_sra_epi16(v1,vec_right);
      ((__m256i *) dp1)[1] = _mm256_sra_epi16(vt,vec_right);
      v2 = _mm256_unpacklo_epi8(vb,vb);    vb = _mm256_unpackhi_epi8(vb,vb);
      v2 = _mm256_sll_epi16(v2,vec_left);  vb = _mm256_sll_epi16(vb,vec_left);
      ((__m256i *) dp2)[0] = _mm256_sra_epi16(v2,vec_right);
      ((__m256i *) dp2)[1] = _mm256_sra_epi16(vb,vec_right);
      v1 = _mm256_unpacklo_epi8(v3,v3);    v3 = _mm256_unpackhi_epi8(v3,v3);
      v1 = _mm256_sll_epi16(v1,vec_left);  v3 = _mm256_sll_epi16(v3,vec_left);
      ((__m256i *) dp3)[0] = _mm256_sra_epi16(v1,vec_right);
      ((__m256i *) dp3)[1] = _mm256_sra_epi16(v3,vec_right);
    }

  // Finish with 6 unaligned stores, after first shifting the source and
  // destination buffers back sufficiently to ensure that we neither overwrite
  // the output buffers nor overread the input buffer.
  int backtrack = ((-width) & 31);
  sp -= 3*backtrack; dp1 -= backtrack; dp2 -= backtrack; dp3 -= backtrack;
  vt = _mm256_loadu_si256((__m256i *) sp);
  vb = _mm256_loadu_si256((__m256i *)(sp+32));
  v3 = _mm256_loadu_si256((__m256i *)(sp+64));
  v1 = _mm256_permute2x128_si256(vt, vb, 0x30); // v1 holds dqwords 0, 3
  v2 = _mm256_permute2x128_si256(vt, v3, 0x21); // v2 holds dqwords 1, 4
  v3 = _mm256_permute2x128_si256(vb, v3, 0x30); // v3 holds dqwords 2, 5
  v1 = _mm256_shuffle_epi8(v1, vec_shuffle_556); // [5B | 5G | 6R] per lane
  v2 = _mm256_shuffle_epi8(v2, vec_shuffle_565); // [5B | 6G | 5R] per lane
  v3 = _mm256_shuffle_epi8(v3, vec_shuffle_655); // [6B | 5G | 5R] per lane
  vt = _mm256_slli_si256(v1, 10);    // Move 6 reds to to MSBs
  vt = _mm256_alignr_epi8(v2, vt, 5);  v2 = _mm256_srli_si256(v2, 5);
  vt = _mm256_alignr_epi8(v3, vt, 5);  v3 = _mm256_srli_si256(v3, 5);
  vt = _mm256_sub_epi8(vt, vec_off); // Level adjused reds in vt
  vb = _mm256_slli_si256(v1, 5);     // Move 5 greens to MSBs
  vb = _mm256_alignr_epi8(v2, vb, 6);  v2 = _mm256_srli_si256(v2, 6);
  vb = _mm256_alignr_epi8(v3, vb, 5);  v3 = _mm256_srli_si256(v3, 5);
  vb = _mm256_sub_epi8(vb, vec_off); // Level adjusted greens in vb      
  v2 = _mm256_alignr_epi8(v2, v1, 5);  v3 = _mm256_alignr_epi8(v3, v2, 6);
  v3 = _mm256_sub_epi8(v3, vec_off); // Level adjusted blues in v3
  vt = _mm256_permute4x64_epi64(vt, 0xD8);
  vb = _mm256_permute4x64_epi64(vb, 0xD8);
  v3 = _mm256_permute4x64_epi64(v3, 0xD8);
  v1 = _mm256_unpacklo_epi8(vt,vt);    vt = _mm256_unpackhi_epi8(vt, vt);
  v1 = _mm256_sll_epi16(v1,vec_left);  vt = _mm256_sll_epi16(vt, vec_left);
  _mm256_storeu_si256((__m256i *)(dp1+0),_mm256_sra_epi16(v1,vec_right));
  _mm256_storeu_si256((__m256i *)(dp1+16),_mm256_sra_epi16(vt,vec_right));
  v2 = _mm256_unpacklo_epi8(vb,vb);    vb = _mm256_unpackhi_epi8(vb,vb);
  v2 = _mm256_sll_epi16(v2,vec_left);  vb = _mm256_sll_epi16(vb,vec_left);
  _mm256_storeu_si256((__m256i *)(dp2+0),_mm256_sra_epi16(v2,vec_right));
  _mm256_storeu_si256((__m256i *)(dp2+16),_mm256_sra_epi16(vb,vec_right));
  v1 = _mm256_unpacklo_epi8(v3,v3);    v3 = _mm256_unpackhi_epi8(v3,v3);
  v1 = _mm256_sll_epi16(v1,vec_left);  v3 = _mm256_sll_epi16(v3,vec_left);
  _mm256_storeu_si256((__m256i *)(dp3+0),_mm256_sra_epi16(v1,vec_right));
  _mm256_storeu_si256((__m256i *)(dp3+16),_mm256_sra_epi16(v3,vec_right));
}

/*****************************************************************************/
/* EXTERN                avx2_int16_from_uint8_ilv4                          */
/*****************************************************************************/

void
  avx2_int16_from_uint8_ilv4(kdu_int16 **dst, kdu_byte *src, int width,
                             int src_precision, int tgt_precision,
                             bool is_absolute, bool src_signed)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(!src_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    tgt_precision = KDU_FIX_POINT;
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int pre_upshift = 16 - src_precision;
  int post_downshift = 16 - tgt_precision;

  __m256i byte_shuffle = _mm256_loadu_si256((__m256i *) kd_shuffle_r4444);
  __m256i dword_shuffle = _mm256_loadu_si256((__m256i *) kd_ilv_dwords);
  __m256i vec_off =   _mm256_set1_epi8((kdu_byte)offset);
  __m128i vec_left =  _mm_cvtsi32_si128(pre_upshift);
  __m128i vec_right = _mm_cvtsi32_si128(post_downshift);
  __m256i v1, v2, v3;

  // Process all but the last 1-16 input quartets using aligned stores
  kdu_int16 *dp1 = dst[0];
  kdu_int16 *dp2 = dst[1];
  kdu_int16 *dp3 = dst[2];
  kdu_int16 *dp4 = dst[3];
  kdu_byte *sp = src;
  for (; width > 16; width-=16, sp+=64, dp1+=16, dp2+=16, dp3+=16, dp4+=16)
    { 
      // Load two vectors and reorganize so that each 128-bit lane contains
      // [4A | 4B | 4G | 4R] from MSB to LSB 
      v1 = _mm256_loadu_si256((__m256i *) sp);
      v2 = _mm256_loadu_si256((__m256i *)(sp+32));
      v1 = _mm256_shuffle_epi8(v1, byte_shuffle);
      v2 = _mm256_shuffle_epi8(v2, byte_shuffle);

      // Rearrange dwords in each lane so that the entire vector holds
      // [8A | 8B | 8G | 8R], from MSB to LSB
      v1 = _mm256_permutevar8x32_epi32(v1, dword_shuffle);
      v2 = _mm256_permutevar8x32_epi32(v2, dword_shuffle);

      // Perform the byte level arithmetic
      v1 = _mm256_sub_epi8(v1, vec_off);  v2 = _mm256_sub_epi8(v2, vec_off);

      // Rearrange the 128-bit lanes of v1 and v2 so as to get
      // [8G | 8R | 8G | 8R] to v3 and [8A | 8B | 8A | 8B] to v2
      v3 = _mm256_permute2x128_si256(v1, v2, 0x20);
      v2 = _mm256_permute2x128_si256(v1, v2, 0x31);

      // Unpack v3 to get 16R's from the low part and 16 G's from the high part
      v1 = _mm256_unpacklo_epi8(v3, v3);   v3 = _mm256_unpackhi_epi8(v3, v3);
      v1 = _mm256_sll_epi16(v1, vec_left); v3 = _mm256_sll_epi16(v3, vec_left);
      ((__m256i *) dp1)[0] = _mm256_sra_epi16(v1, vec_right);
      ((__m256i *) dp2)[0] = _mm256_sra_epi16(v3, vec_right);
      
      // Unpack v2 to get 16B's from the low part and 16 A's from the high part
      v1 = _mm256_unpacklo_epi8(v2, v2);   v2 = _mm256_unpackhi_epi8(v2, v2);
      v1 = _mm256_sll_epi16(v1, vec_left); v2 = _mm256_sll_epi16(v2, vec_left);
      ((__m256i *) dp3)[0] = _mm256_sra_epi16(v1, vec_right);
      ((__m256i *) dp4)[0] = _mm256_sra_epi16(v2, vec_right);
    }
  
  // Finish with 4 unaligned stores, after first shifting the source and
  // destination buffers back sufficiently to ensure that we neither overwrite
  // the output buffers nor overread the input buffer.
  int backtrack = ((-width) & 15);
  sp -= 4*backtrack;
  dp1 -= backtrack; dp2 -= backtrack; dp3 -= backtrack; dp4 -= backtrack;
  v1 = _mm256_loadu_si256((__m256i *) sp);
  v2 = _mm256_loadu_si256((__m256i *)(sp+32));
  v1 = _mm256_shuffle_epi8(v1, byte_shuffle);
  v2 = _mm256_shuffle_epi8(v2, byte_shuffle);
  v1 = _mm256_permutevar8x32_epi32(v1, dword_shuffle);
  v2 = _mm256_permutevar8x32_epi32(v2, dword_shuffle);
  v1 = _mm256_sub_epi8(v1, vec_off);  v2 = _mm256_sub_epi8(v2, vec_off);  
  v3 = _mm256_permute2x128_si256(v1, v2, 0x20);
  v2 = _mm256_permute2x128_si256(v1, v2, 0x31);
  v1 = _mm256_unpacklo_epi8(v3, v3);   v3 = _mm256_unpackhi_epi8(v3, v3);
  v1 = _mm256_sll_epi16(v1, vec_left); v3 = _mm256_sll_epi16(v3, vec_left);
  _mm256_storeu_si256((__m256i *)(dp1+0),_mm256_sra_epi16(v1, vec_right));
  _mm256_storeu_si256((__m256i *)(dp1+0),_mm256_sra_epi16(v3, vec_right));
  v1 = _mm256_unpacklo_epi8(v2, v2);   v2 = _mm256_unpackhi_epi8(v2, v2);
  v1 = _mm256_sll_epi16(v1, vec_left); v2 = _mm256_sll_epi16(v2, vec_left);
  _mm256_storeu_si256((__m256i *)(dp3+0),_mm256_sra_epi16(v1, vec_right));
  _mm256_storeu_si256((__m256i *)(dp4+0),_mm256_sra_epi16(v2, vec_right));
}

/*****************************************************************************/
/* EXTERN                avx2_floats_from_uint8_ilv1                         */
/*****************************************************************************/

void
  avx2_floats_from_uint8_ilv1(float **dst, kdu_byte *src, int width,
                              int src_precision, int tgt_precision,
                              bool is_absolute, bool src_signed)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(!src_signed); // Function prototype generic; we don't do signed bytes
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int upshift = 16 - src_precision; // Note: we upshift to 16-bit precision,
    // then store the result in the high part of a dword before conversion.
  float scale = 1.0F  / (((float)(1<<16)) * ((float)(1<<16)));
  
  __m256i permd_ctl = _mm256_loadu_si256((__m256i *) kd_dilv_dwords);
  __m256i vec_off = _mm256_set1_epi8((kdu_byte)offset);
  __m128i vec_left = _mm_cvtsi32_si128(upshift);
  __m256 vec_scale = _mm256_set1_ps(scale);
  __m256i v1, v2, v3, v4;
  __m256 v1f, v2f, v3f, v4f;

  // Process all but the last 1-32 input bytes using aligned stores
  kdu_byte *sp = src;
  float *dp = dst[0];
  for (; width > 32; width -= 32, sp += 32, dp += 32)
    { 
      // Load 32 byte vector and rearrange so that src byte positions appear
      // as follows, in MSB to LSB order.
      // [ 31:28 23:20 | 15:12 7:4 | 27:24 19:16 | 11:8 3:0]    
      v4 = _mm256_loadu_si256((__m256i *) sp);
      v4 = _mm256_permutevar8x32_epi32(v4, permd_ctl);
      v4 = _mm256_sub_epi8(v4, vec_off);

      // Unpack bytes to words and apply upshift
      v2 = _mm256_unpacklo_epi8(v4, v4); // Leaves [15:12   7:4 | 11:8    3:0]
      v4 = _mm256_unpackhi_epi8(v4, v4); // Leaves [31:28 23:20 | 27:24 19:16]
      v2 = _mm256_sll_epi16(v2, vec_left);
      v4 = _mm256_sll_epi16(v4, vec_left);

      // Unpack words to dwords, zeroing the low half of each dword
      __m256i zero = _mm256_setzero_si256();
      v1 = _mm256_unpacklo_epi16(zero, v2); // v1 = [ 7  6  5  4 |  3  2 1 0]
      v2 = _mm256_unpackhi_epi16(zero, v2); // v2 = [15 14 13 12 | 11 10 9 8]
      v3 = _mm256_unpacklo_epi16(zero, v4); // Same as v1 but add 16
      v4 = _mm256_unpackhi_epi16(zero, v4); // Same as v2 but add 16

      // Convert and scale the dwords/floats in each vector
      v1f = _mm256_cvtepi32_ps(v1);        v2f = _mm256_cvtepi32_ps(v2);
      v3f = _mm256_cvtepi32_ps(v3);        v4f = _mm256_cvtepi32_ps(v4);
      v1f = _mm256_mul_ps(v1f, vec_scale); v2f = _mm256_mul_ps(v2f, vec_scale);
      v3f = _mm256_mul_ps(v3f, vec_scale); v4f = _mm256_mul_ps(v4f, vec_scale);
      ((__m256 *) dp)[0] = v1f;  ((__m256 *) dp)[1] = v2f;
      ((__m256 *) dp)[2] = v3f;  ((__m256 *) dp)[3] = v4f;
    }
  
  // Finish with 4 unaligned stores, after first shifting the source and
  // destination buffers back sufficiently to ensure that we neither overwrite
  // the output buffer nor overread the input buffer.
  int backtrack = ((-width) & 31);
  sp -= backtrack; dp -= backtrack;
  v4 = _mm256_loadu_si256((__m256i *) sp);
  v4 = _mm256_permutevar8x32_epi32(v4, permd_ctl);
  v4 = _mm256_sub_epi8(v4, vec_off);
  v2 = _mm256_unpacklo_epi8(v4, v4); // Leaves [15:12   7:4 | 11:8    3:0]
  v4 = _mm256_unpackhi_epi8(v4, v4); // Leaves [31:28 23:20 | 27:24 19:16]
  v2 = _mm256_sll_epi16(v2, vec_left);
  v4 = _mm256_sll_epi16(v4, vec_left);
  __m256i zero = _mm256_setzero_si256();
  v1 = _mm256_unpacklo_epi16(zero, v2); // v1 = [ 7  6  5  4 |  3  2 1 0]
  v2 = _mm256_unpackhi_epi16(zero, v2); // v2 = [15 14 13 12 | 11 10 9 8]
  v3 = _mm256_unpacklo_epi16(zero, v4); // Same as v1 but add 16
  v4 = _mm256_unpackhi_epi16(zero, v4); // Same as v2 but add 16
  v1f = _mm256_cvtepi32_ps(v1);        v2f = _mm256_cvtepi32_ps(v2);
  v3f = _mm256_cvtepi32_ps(v3);        v4f = _mm256_cvtepi32_ps(v4);
  v1f = _mm256_mul_ps(v1f, vec_scale); v2f = _mm256_mul_ps(v2f, vec_scale);
  v3f = _mm256_mul_ps(v3f, vec_scale); v4f = _mm256_mul_ps(v4f, vec_scale);
  _mm256_storeu_ps(dp+0 ,v1f);   _mm256_storeu_ps(dp+8 ,v2f);
  _mm256_storeu_ps(dp+16,v3f);   _mm256_storeu_ps(dp+24,v4f);
}

/*****************************************************************************/
/* EXTERN                avx2_floats_from_uint8_ilv3                         */
/*****************************************************************************/

void
  avx2_floats_from_uint8_ilv3(float **dst, kdu_byte *src, int width,
                              int src_precision, int tgt_precision,
                              bool is_absolute, bool src_signed)
{ // NB: This function should compile to use 16 registers in 64-bit mode
  // without any need for extra memory transactions.  In 32-bit mode, a good
  // compiler will figure out that the parameter registers can be reloaded on
  // demand, so there still should not be much need to save temporary results
  // to memory within the loop.
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(!src_signed); // Function prototype generic; we don't do signed bytes
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int upshift = 16 - src_precision;
  float scale = 1.0F  / (((float)(1<<16)) * ((float)(1<<16)));

  __m256i permd_ctl = _mm256_loadu_si256((__m256i *) kd_dilv_dwords);
  __m256i vec_shuffle_556 = _mm256_loadu_si256((__m256i *) kd_shuffle_r556);
  __m256i vec_shuffle_565 = _mm256_loadu_si256((__m256i *) kd_shuffle_r565);
  __m256i vec_shuffle_655 = _mm256_loadu_si256((__m256i *) kd_shuffle_r655);
  __m256i vec_off =   _mm256_set1_epi8((kdu_byte)offset);
  __m128i vec_left =  _mm_cvtsi32_si128(upshift);
  __m256  vec_scale = _mm256_set1_ps(scale);
  __m256i v1, v2, v3, va, vb, vc, t1, t2;
  __m256 v1f, v2f, v3f, v4f;

  // Process all but the last 1-32 input triplets using aligned stores
  float *dp1 = dst[0];
  float *dp2 = dst[1];
  float *dp3 = dst[2];
  kdu_byte *sp = src;
  for (; width > 32; width-=32, sp+=96, dp1+=32, dp2+=32, dp3+=32)
    { 
      // Load and de-interleave 3 vectors of bytes so that each lane of
      // v1 holds [5B | 5G | 6R], each lane of v2 holds [5B | 6G | 5R] and
      // each lane of v3 holds [6B | 5G | 5R], all listed from MSB to LSB 
      va = _mm256_loadu_si256((__m256i *) sp);
      vb = _mm256_loadu_si256((__m256i *)(sp+32));
      v3 = _mm256_loadu_si256((__m256i *)(sp+64));
      v1 = _mm256_permute2x128_si256(va, vb, 0x30); // v1 holds dqwords 0, 3
      v2 = _mm256_permute2x128_si256(va, v3, 0x21); // v2 holds dqwords 1, 4
      v3 = _mm256_permute2x128_si256(vb, v3, 0x30); // v3 holds dqwords 2, 5
      v1 = _mm256_shuffle_epi8(v1, vec_shuffle_556); // [5B | 5G | 6R] per lane
      v2 = _mm256_shuffle_epi8(v2, vec_shuffle_565); // [5B | 6G | 5R] per lane
      v3 = _mm256_shuffle_epi8(v3, vec_shuffle_655); // [6B | 5G | 5R] per lane

      // Combine v1, v2 and v3, separately in each lane, so that each lance of
      // va holds 16R, each lane of vb holds 16G and each lane of vc holds 16B
      va = _mm256_slli_si256(v1, 10);    // Move 6 reds to to MSBs
      va = _mm256_alignr_epi8(v2, va, 5);  v2 = _mm256_srli_si256(v2, 5);
      va = _mm256_alignr_epi8(v3, va, 5);  v3 = _mm256_srli_si256(v3, 5);
      va = _mm256_sub_epi8(va, vec_off); // Level adjused reds in va
      vb = _mm256_slli_si256(v1, 5);     // Move 5 greens to MSBs
      vb = _mm256_alignr_epi8(v2, vb, 6);  v2 = _mm256_srli_si256(v2, 6);
      vb = _mm256_alignr_epi8(v3, vb, 5);  v3 = _mm256_srli_si256(v3, 5);
      vb = _mm256_sub_epi8(vb, vec_off); // Level adjusted greens in vb      
      v2 = _mm256_alignr_epi8(v2, v1, 5);  v3 = _mm256_alignr_epi8(v3, v2, 6);
      vc = _mm256_sub_epi8(v3, vec_off); // Level adjusted blues in vc

      // Rearrange elements in va, vb and vc, in exactly the same way as
      // `avx2_floats_from_uint8_ilv1', where more explanation is provided.
      va = _mm256_permutevar8x32_epi32(va, permd_ctl); // even dwords go to low
      vb = _mm256_permutevar8x32_epi32(vb, permd_ctl); // lanes and odd dwords
      vc = _mm256_permutevar8x32_epi32(vc, permd_ctl); // go to high lanes

      // Unpack bytes to words, upshift, then unpack words to dwords, zeroing
      // the low half of each dword, then convert, scale and write the float
      // vectors.  We do these steps first for the 16R's in va, then for the
      // 16G's in vb and finally for the 16B's in vc.  The steps for each
      // batch are identical to those in `...floats_from_uint8_ilv1'.
      __m256i vz = _mm256_setzero_si256();
      v1 = _mm256_unpacklo_epi8(va, va);   va = _mm256_unpackhi_epi8(va, va);
      v1 = _mm256_sll_epi16(v1, vec_left); va = _mm256_sll_epi16(va, vec_left);
      t1 = _mm256_unpacklo_epi16(vz, v1); v1 = _mm256_unpackhi_epi16(vz, v1);
      t2 = _mm256_unpacklo_epi16(vz, va); va = _mm256_unpackhi_epi16(vz, va);
      v1f = _mm256_cvtepi32_ps(t1);        v2f = _mm256_cvtepi32_ps(v1);
      v3f = _mm256_cvtepi32_ps(t2);        v4f = _mm256_cvtepi32_ps(va);
      v1f = _mm256_mul_ps(v1f, vec_scale); v2f = _mm256_mul_ps(v2f, vec_scale);
      v3f = _mm256_mul_ps(v3f, vec_scale); v4f = _mm256_mul_ps(v4f, vec_scale);
      ((__m256 *) dp1)[0] = v1f;  ((__m256 *) dp1)[1] = v2f;
      ((__m256 *) dp1)[2] = v3f;  ((__m256 *) dp1)[3] = v4f;

      v2 = _mm256_unpacklo_epi8(vb, vb);   vb = _mm256_unpackhi_epi8(vb, vb);
      v2 = _mm256_sll_epi16(v2, vec_left); vb = _mm256_sll_epi16(vb, vec_left);
      t1 = _mm256_unpacklo_epi16(vz, v2); v2 = _mm256_unpackhi_epi16(vz, v2);
      t2 = _mm256_unpacklo_epi16(vz, vb); vb = _mm256_unpackhi_epi16(vz, vb);
      v1f = _mm256_cvtepi32_ps(t1);        v2f = _mm256_cvtepi32_ps(v2);
      v3f = _mm256_cvtepi32_ps(t2);        v4f = _mm256_cvtepi32_ps(vb);
      v1f = _mm256_mul_ps(v1f, vec_scale); v2f = _mm256_mul_ps(v2f, vec_scale);
      v3f = _mm256_mul_ps(v3f, vec_scale); v4f = _mm256_mul_ps(v4f, vec_scale);
      ((__m256 *) dp2)[0] = v1f;  ((__m256 *) dp2)[1] = v2f;
      ((__m256 *) dp2)[2] = v3f;  ((__m256 *) dp2)[3] = v4f;

      v3 = _mm256_unpacklo_epi8(vc, vc);   vc = _mm256_unpackhi_epi8(vc, vc);
      v3 = _mm256_sll_epi16(v3, vec_left); vc = _mm256_sll_epi16(vc, vec_left);
      t1 = _mm256_unpacklo_epi16(vz, v3); v3 = _mm256_unpackhi_epi16(vz, v3);
      t2 = _mm256_unpacklo_epi16(vz, vc); vc = _mm256_unpackhi_epi16(vz, vc);
      v1f = _mm256_cvtepi32_ps(t1);        v2f = _mm256_cvtepi32_ps(v3);
      v3f = _mm256_cvtepi32_ps(t2);        v4f = _mm256_cvtepi32_ps(vc);
      v1f = _mm256_mul_ps(v1f, vec_scale); v2f = _mm256_mul_ps(v2f, vec_scale);
      v3f = _mm256_mul_ps(v3f, vec_scale); v4f = _mm256_mul_ps(v4f, vec_scale);
      ((__m256 *) dp3)[0] = v1f;  ((__m256 *) dp3)[1] = v2f;
      ((__m256 *) dp3)[2] = v3f;  ((__m256 *) dp3)[3] = v4f;
    }
  
  // Finish with 12 unaligned stores, after first shifting the source and
  // destination buffers back sufficiently to ensure that we neither overwrite
  // the output buffers nor overread the input buffer.
  int backtrack = ((-width) & 31);
  sp -= 3*backtrack; dp1 -= backtrack; dp2 -= backtrack; dp3 -= backtrack;
  va = _mm256_loadu_si256((__m256i *) sp);
  vb = _mm256_loadu_si256((__m256i *)(sp+32));
  v3 = _mm256_loadu_si256((__m256i *)(sp+64));
  v1 = _mm256_permute2x128_si256(va, vb, 0x30); // v1 holds dqwords 0, 3
  v2 = _mm256_permute2x128_si256(va, v3, 0x21); // v2 holds dqwords 1, 4
  v3 = _mm256_permute2x128_si256(vb, v3, 0x30); // v3 holds dqwords 2, 5
  v1 = _mm256_shuffle_epi8(v1, vec_shuffle_556); // [5B | 5G | 6R] per lane
  v2 = _mm256_shuffle_epi8(v2, vec_shuffle_565); // [5B | 6G | 5R] per lane
  v3 = _mm256_shuffle_epi8(v3, vec_shuffle_655); // [6B | 5G | 5R] per lane
  va = _mm256_slli_si256(v1, 10);    // Move 6 reds to to MSBs
  va = _mm256_alignr_epi8(v2, va, 5);  v2 = _mm256_srli_si256(v2, 5);
  va = _mm256_alignr_epi8(v3, va, 5);  v3 = _mm256_srli_si256(v3, 5);
  va = _mm256_sub_epi8(va, vec_off); // Level adjused reds in va
  vb = _mm256_slli_si256(v1, 5);     // Move 5 greens to MSBs
  vb = _mm256_alignr_epi8(v2, vb, 6);  v2 = _mm256_srli_si256(v2, 6);
  vb = _mm256_alignr_epi8(v3, vb, 5);  v3 = _mm256_srli_si256(v3, 5);
  vb = _mm256_sub_epi8(vb, vec_off); // Level adjusted greens in vb      
  v2 = _mm256_alignr_epi8(v2, v1, 5);  v3 = _mm256_alignr_epi8(v3, v2, 6);
  vc = _mm256_sub_epi8(v3, vec_off); // Level adjusted blues in vc  
  va = _mm256_permutevar8x32_epi32(va, permd_ctl); // even dwords go to low
  vb = _mm256_permutevar8x32_epi32(vb, permd_ctl); // lanes and odd dwords
  vc = _mm256_permutevar8x32_epi32(vc, permd_ctl); // go to high lanes
  __m256i vz = _mm256_setzero_si256();
  v1 = _mm256_unpacklo_epi8(va, va);   va = _mm256_unpackhi_epi8(va, va);
  v1 = _mm256_sll_epi16(v1, vec_left); va = _mm256_sll_epi16(va, vec_left);
  t1 = _mm256_unpacklo_epi16(vz, v1); v1 = _mm256_unpackhi_epi16(vz, v1);
  t2 = _mm256_unpacklo_epi16(vz, va); va = _mm256_unpackhi_epi16(vz, va);
  v1f = _mm256_cvtepi32_ps(t1);        v2f = _mm256_cvtepi32_ps(v1);
  v3f = _mm256_cvtepi32_ps(t2);        v4f = _mm256_cvtepi32_ps(va);
  v1f = _mm256_mul_ps(v1f, vec_scale); v2f = _mm256_mul_ps(v2f, vec_scale);
  v3f = _mm256_mul_ps(v3f, vec_scale); v4f = _mm256_mul_ps(v4f, vec_scale);
  _mm256_storeu_ps(dp1+0, v1f);        _mm256_storeu_ps(dp1+8, v2f);
  _mm256_storeu_ps(dp1+16,v3f);        _mm256_storeu_ps(dp1+24,v4f);
  v2 = _mm256_unpacklo_epi8(vb, vb);   vb = _mm256_unpackhi_epi8(vb, vb);
  v2 = _mm256_sll_epi16(v2, vec_left); vb = _mm256_sll_epi16(vb, vec_left);
  t1 = _mm256_unpacklo_epi16(vz, v2); v2 = _mm256_unpackhi_epi16(vz, v2);
  t2 = _mm256_unpacklo_epi16(vz, vb); vb = _mm256_unpackhi_epi16(vz, vb);
  v1f = _mm256_cvtepi32_ps(t1);        v2f = _mm256_cvtepi32_ps(v2);
  v3f = _mm256_cvtepi32_ps(t2);        v4f = _mm256_cvtepi32_ps(vb);
  v1f = _mm256_mul_ps(v1f, vec_scale); v2f = _mm256_mul_ps(v2f, vec_scale);
  v3f = _mm256_mul_ps(v3f, vec_scale); v4f = _mm256_mul_ps(v4f, vec_scale);
  _mm256_storeu_ps(dp2+0, v1f);        _mm256_storeu_ps(dp2+8, v2f);
  _mm256_storeu_ps(dp2+16,v3f);        _mm256_storeu_ps(dp2+24,v4f);
  v3 = _mm256_unpacklo_epi8(vc, vc);   vc = _mm256_unpackhi_epi8(vc, vc);
  v3 = _mm256_sll_epi16(v3, vec_left); vc = _mm256_sll_epi16(vc, vec_left);
  t1 = _mm256_unpacklo_epi16(vz, v3); v3 = _mm256_unpackhi_epi16(vz, v3);
  t2 = _mm256_unpacklo_epi16(vz, vc); vc = _mm256_unpackhi_epi16(vz, vc);
  v1f = _mm256_cvtepi32_ps(t1);        v2f = _mm256_cvtepi32_ps(v3);
  v3f = _mm256_cvtepi32_ps(t2);        v4f = _mm256_cvtepi32_ps(vc);
  v1f = _mm256_mul_ps(v1f, vec_scale); v2f = _mm256_mul_ps(v2f, vec_scale);
  v3f = _mm256_mul_ps(v3f, vec_scale); v4f = _mm256_mul_ps(v4f, vec_scale);
  _mm256_storeu_ps(dp3+0, v1f);        _mm256_storeu_ps(dp3+8, v2f);
  _mm256_storeu_ps(dp3+16,v3f);        _mm256_storeu_ps(dp3+24,v4f);
}

/*****************************************************************************/
/* EXTERN                avx2_floats_from_uint8_ilv4                         */
/*****************************************************************************/

void
  avx2_floats_from_uint8_ilv4(float **dst, kdu_byte *src, int width,
                              int src_precision, int tgt_precision,
                              bool is_absolute, bool src_signed)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(!src_signed); // Function prototype generic; we don't do signed bytes
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int upshift = 16 - src_precision;
  float scale = 1.0F  / (((float)(1<<16)) * ((float)(1<<16)));

  __m256i byte_shuffle = _mm256_loadu_si256((__m256i *) kd_shuffle_r4444);
  __m256i vec_off =   _mm256_set1_epi8((kdu_byte)offset);
  __m128i vec_left =  _mm_cvtsi32_si128(upshift);
  __m256 vec_scale = _mm256_set1_ps(scale);
  __m256i v1, v2, v3, v4;
  __m256 v1f, v2f, v3f, v4f;

  // Process all but the last 1-16 input quartets using aligned stores
  float *dp1 = dst[0];
  float *dp2 = dst[1];
  float *dp3 = dst[2];
  float *dp4 = dst[3];
  kdu_byte *sp = src;
  for (; width > 8; width-=8, sp+=32, dp1+=8, dp2+=8, dp3+=8, dp4+=8)
    { 
      // Load vector and reorganize so that each 128-bit lane contains
      // [4A | 4B | 4G | 4R] from MSB to LSB 
      v4 = _mm256_loadu_si256((__m256i *) sp);
      v4 = _mm256_shuffle_epi8(v4, byte_shuffle);
      v4 = _mm256_sub_epi8(v4, vec_off);

      // Unpack to get v2 = [4G | 4R | 4G | 4R] words, from MSB to LSB and
      // v4 = [4A | 4B | 4A | 4B ] words, from MSB to LSB; then apply shifts
      v2 = _mm256_unpacklo_epi32(v4, v4);
      v4 = _mm256_unpackhi_epi32(v4, v4);
      v2 = _mm256_sll_epi16(v2, vec_left);
      v4 = _mm256_sll_epi16(v4, vec_left);

      // Unpack words to dwords, zeroing the low half of each dword
      __m256i zero = _mm256_setzero_si256();
      v1 = _mm256_unpacklo_epi16(zero, v2); // [4R | 4R] as dwords
      v2 = _mm256_unpackhi_epi16(zero, v2); // [4G | 4G] as dwords
      v3 = _mm256_unpacklo_epi16(zero, v4); // [4B | 4B] as dwords
      v4 = _mm256_unpackhi_epi16(zero, v4); // [4A | 4A] as dwords

      // Convert and scale the dwords/floats in each vector
      v1f = _mm256_cvtepi32_ps(v1);        v2f = _mm256_cvtepi32_ps(v2);
      v3f = _mm256_cvtepi32_ps(v3);        v4f = _mm256_cvtepi32_ps(v4);
      v1f = _mm256_mul_ps(v1f, vec_scale); v2f = _mm256_mul_ps(v2f, vec_scale);
      v3f = _mm256_mul_ps(v3f, vec_scale); v4f = _mm256_mul_ps(v4f, vec_scale);
      ((__m256 *) dp1)[0] = v1f;           ((__m256 *) dp2)[0] = v2f;
      ((__m256 *) dp3)[0] = v3f;           ((__m256 *) dp4)[0] = v4f;
    }
  
  // Finish with 4 unaligned stores, after first shifting the source and
  // destination buffers back sufficiently to ensure that we neither overwrite
  // the output buffers nor overread the input buffer.
  int backtrack = ((-width) & 7);
  sp -= 4*backtrack;
  dp1 -= backtrack; dp2 -= backtrack; dp3 -= backtrack; dp4 -= backtrack;
  v4 = _mm256_loadu_si256((__m256i *) sp);
  v4 = _mm256_shuffle_epi8(v4, byte_shuffle);
  v4 = _mm256_sub_epi8(v4, vec_off);
  v2 = _mm256_unpacklo_epi32(v4, v4);
  v4 = _mm256_unpackhi_epi32(v4, v4);
  v2 = _mm256_sll_epi16(v2, vec_left);
  v4 = _mm256_sll_epi16(v4, vec_left);
  __m256i zero = _mm256_setzero_si256();
  v1 = _mm256_unpacklo_epi16(zero, v2); // [4R | 4R] as dwords
  v2 = _mm256_unpackhi_epi16(zero, v2); // [4G | 4G] as dwords
  v3 = _mm256_unpacklo_epi16(zero, v4); // [4B | 4B] as dwords
  v4 = _mm256_unpackhi_epi16(zero, v4); // [4A | 4A] as dwords
  v1f = _mm256_cvtepi32_ps(v1);        v2f = _mm256_cvtepi32_ps(v2);
  v3f = _mm256_cvtepi32_ps(v3);        v4f = _mm256_cvtepi32_ps(v4);
  v1f = _mm256_mul_ps(v1f, vec_scale); v2f = _mm256_mul_ps(v2f, vec_scale);
  v3f = _mm256_mul_ps(v3f, vec_scale); v4f = _mm256_mul_ps(v4f, vec_scale);
  _mm256_storeu_ps(dp1,v1f);           _mm256_storeu_ps(dp2,v2f);
  _mm256_storeu_ps(dp3,v3f);           _mm256_storeu_ps(dp4,v4f);
}

/*****************************************************************************/
/* EXTERN                avx2_int16_from_int16_ilv1                          */
/*****************************************************************************/

void
  avx2_int16_from_int16_ilv1(kdu_int16 **dst, kdu_int16 *src, int width,
                             int src_precision, int tgt_precision,
                             bool is_absolute, bool src_signed)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  if (!is_absolute)
    tgt_precision = KDU_FIX_POINT;
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int pre_upshift = 16 - src_precision;
  int post_downshift = 16 - tgt_precision;

  __m256i vec_off =   _mm256_set1_epi16((kdu_int16)offset);
  __m128i vec_left =  _mm_cvtsi32_si128(pre_upshift);
  __m128i vec_right = _mm_cvtsi32_si128(post_downshift);
  __m256i v1, v2;

  // Process all but the last 1-32 input words using aligned stores
  kdu_int16 *dp = dst[0];
  kdu_int16 *sp = src;
  for (; width > 32; width-=32, sp+=32, dp+=32)
    { 
      v1 = _mm256_loadu_si256((__m256i *) sp);
      v2 = _mm256_loadu_si256((__m256i *)(sp+16));
      v1 = _mm256_add_epi16(v1, vec_off);  v2 = _mm256_add_epi16(v2, vec_off);
      v1 = _mm256_sll_epi16(v1, vec_left); v2 = _mm256_sll_epi16(v2, vec_left);
      ((__m256i *) dp)[0] = _mm256_sra_epi16(v1, vec_right);
      ((__m256i *) dp)[1] = _mm256_sra_epi16(v2, vec_right);
    }
  
  // Finish with 2 unaligned stores, after first shifting the source and
  // destination buffers back sufficiently to ensure that we neither overwrite
  // the output buffer nor overread the input buffer.
  int backtrack = ((-width) & 31);
  sp -= backtrack; dp -= backtrack;
  v1 = _mm256_loadu_si256((__m256i *) sp);
  v2 = _mm256_loadu_si256((__m256i *)(sp+16));
  v1 = _mm256_add_epi16(v1, vec_off);  v2 = _mm256_add_epi16(v2, vec_off);
  v1 = _mm256_sll_epi16(v1, vec_left); v2 = _mm256_sll_epi16(v2, vec_left);
  _mm256_storeu_si256((__m256i *)(dp+0),_mm256_sra_epi16(v1, vec_right));
  _mm256_storeu_si256((__m256i *)(dp+16),_mm256_sra_epi16(v2, vec_right));                 
}

/*****************************************************************************/
/* EXTERN                avx2_int32_from_int16_ilv1                          */
/*****************************************************************************/

void
  avx2_int32_from_int16_ilv1(kdu_int32 **dst, kdu_int16 *src, int width,
                             int src_precision, int tgt_precision,
                             bool is_absolute, bool src_signed)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(is_absolute);
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int pre_upshift = 16 - src_precision;
  int post_downshift = 32 - tgt_precision;

  __m256i vec_off = _mm256_set1_epi16((kdu_int16)offset);
  __m128i vec_left = _mm_cvtsi32_si128(pre_upshift);
  __m128i vec_right = _mm_cvtsi32_si128(post_downshift);
  __m256i v1, v2, v3, v4;

  // Process all but the last 1-32 input words using aligned stores
  kdu_int32 *dp = dst[0];
  kdu_int16 *sp = src;
  for (; width > 32; width-=32, sp+=32, dp+=32)
    { 
      // Load two inputs, offset and shift, while preparing the input vectors
      // for lane-by-lane unpacking into 32-bit ints.  The prepared vectors
      // have the following organization, expressed MSB to LSB in terms of
      // the source word positions that are represented:
      // [ 15 14 13 12  7 6 5 4 | 11 10 9 8  3 2 1 0]
      v2 = _mm256_loadu_si256((__m256i *) sp);
      v4 = _mm256_loadu_si256((__m256i *)(sp+16));
      v2 = _mm256_permute4x64_epi64(v2, 0xD8);
      v4 = _mm256_permute4x64_epi64(v4, 0xD8);
      v2 = _mm256_add_epi16(v2, vec_off);  v4 = _mm256_add_epi16(v4, vec_off);
      v2 = _mm256_sll_epi16(v2, vec_left); v4 = _mm256_sll_epi16(v4, vec_left);
      
      // Unpack into integers, downshift and save
      __m256i zero = _mm256_setzero_si256();
      v1 = _mm256_unpacklo_epi16(zero, v2);
      v2 = _mm256_unpackhi_epi16(zero, v2);
      v3 = _mm256_unpacklo_epi16(zero, v4);
      v4 = _mm256_unpackhi_epi16(zero, v4);
      ((__m256i *) dp)[0] = _mm256_sra_epi32(v1, vec_right);
      ((__m256i *) dp)[1] = _mm256_sra_epi32(v2, vec_right);
      ((__m256i *) dp)[2] = _mm256_sra_epi32(v3, vec_right);
      ((__m256i *) dp)[3] = _mm256_sra_epi32(v4, vec_right);
    }
  
  // Finish with 4 unaligned stores, after first shifting the source and
  // destination buffers back sufficiently to ensure that we neither overwrite
  // the output buffer nor overread the input buffer.
  int backtrack = ((-width) & 31);
  sp -= backtrack; dp -= backtrack;
  v2 = _mm256_loadu_si256((__m256i *) sp);
  v4 = _mm256_loadu_si256((__m256i *)(sp+16));
  v2 = _mm256_permute4x64_epi64(v2, 0xD8);
  v4 = _mm256_permute4x64_epi64(v4, 0xD8);
  v2 = _mm256_add_epi16(v2, vec_off);  v4 = _mm256_add_epi16(v4, vec_off);
  v2 = _mm256_sll_epi16(v2, vec_left); v4 = _mm256_sll_epi16(v4, vec_left);
  
  // Unpack into integers, downshift and save
  __m256i zero = _mm256_setzero_si256();
  v1 = _mm256_unpacklo_epi16(zero, v2);
  v2 = _mm256_unpackhi_epi16(zero, v2);
  v3 = _mm256_unpacklo_epi16(zero, v4);
  v4 = _mm256_unpackhi_epi16(zero, v4);
  _mm256_storeu_si256((__m256i *)(dp+0 ),_mm256_sra_epi32(v1, vec_right));
  _mm256_storeu_si256((__m256i *)(dp+8 ),_mm256_sra_epi32(v2, vec_right));
  _mm256_storeu_si256((__m256i *)(dp+16),_mm256_sra_epi32(v3, vec_right));
  _mm256_storeu_si256((__m256i *)(dp+24),_mm256_sra_epi32(v4, vec_right));
}

/*****************************************************************************/
/* EXTERN                avx2_floats_from_int16_ilv1                         */
/*****************************************************************************/

void
  avx2_floats_from_int16_ilv1(float **dst, kdu_int16 *src, int width,
                              int src_precision, int tgt_precision,
                              bool is_absolute, bool src_signed)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(!is_absolute);
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int upshift = 16 - src_precision;
  float scale = 1.0F  / (((float)(1<<16)) * ((float)(1<<16)));

  __m256 vec_scale = _mm256_set1_ps(scale);
  __m256i vec_off = _mm256_set1_epi16((kdu_int16)offset);
  __m128i vec_left =  _mm_cvtsi32_si128(upshift);
  __m256i v1, v2, v3, v4;
  __m256 v1f, v2f, v3f, v4f;

  // Process all but the last 1-32 input words using aligned stores
  float *dp = dst[0];
  kdu_int16 *sp = src;
  for (; width > 32; width-=32, sp+=32, dp+=32)
    {
      // Load two inputs, offset and shift, while preparing the input vectors
      // for lane-by-lane unpacking into 32-bit ints.  The prepared vectors
      // have the following organization, expressed MSB to LSB in terms of
      // the source word positions that are represented:
      // [ 15 14 13 12  7 6 5 4 | 11 10 9 8  3 2 1 0]
      v2 = _mm256_loadu_si256((__m256i *) sp);
      v4 = _mm256_loadu_si256((__m256i *)(sp+16));
      v2 = _mm256_permute4x64_epi64(v2, 0xD8);
      v4 = _mm256_permute4x64_epi64(v4, 0xD8);
      v2 = _mm256_add_epi16(v2, vec_off);  v4 = _mm256_add_epi16(v4, vec_off);
      v2 = _mm256_sll_epi16(v2, vec_left); v4 = _mm256_sll_epi16(v4, vec_left);

      // Unpack into integers, convert, scale and save
      __m256i zero = _mm256_setzero_si256();
      v1 = _mm256_unpacklo_epi16(zero, v2);
      v2 = _mm256_unpackhi_epi16(zero, v2);
      v3 = _mm256_unpacklo_epi16(zero, v4);
      v4 = _mm256_unpackhi_epi16(zero, v4);
      v1f = _mm256_cvtepi32_ps(v1);    v2f = _mm256_cvtepi32_ps(v2);
      v3f = _mm256_cvtepi32_ps(v3);    v4f = _mm256_cvtepi32_ps(v4);
      ((__m256 *) dp)[0] = _mm256_mul_ps(v1f, vec_scale);
      ((__m256 *) dp)[1] = _mm256_mul_ps(v2f, vec_scale);
      ((__m256 *) dp)[2] = _mm256_mul_ps(v3f, vec_scale);
      ((__m256 *) dp)[3] = _mm256_mul_ps(v4f, vec_scale);
    }
  
  // Finish with 4 unaligned stores, after first shifting the source and
  // destination buffers back sufficiently to ensure that we neither overwrite
  // the output buffer nor overread the input buffer.
  int backtrack = ((-width) & 31);
  sp -= backtrack; dp -= backtrack;
  v2 = _mm256_loadu_si256((__m256i *) sp);
  v4 = _mm256_loadu_si256((__m256i *)(sp+16));
  v2 = _mm256_permute4x64_epi64(v2, 0xD8);
  v4 = _mm256_permute4x64_epi64(v4, 0xD8);
  v2 = _mm256_add_epi16(v2, vec_off);  v4 = _mm256_add_epi16(v4, vec_off);
  v2 = _mm256_sll_epi16(v2, vec_left); v4 = _mm256_sll_epi16(v4, vec_left);
  __m256i zero = _mm256_setzero_si256();
  v1 = _mm256_unpacklo_epi16(zero, v2);
  v2 = _mm256_unpackhi_epi16(zero, v2);
  v3 = _mm256_unpacklo_epi16(zero, v4);
  v4 = _mm256_unpackhi_epi16(zero, v4);
  v1f = _mm256_cvtepi32_ps(v1);    v2f = _mm256_cvtepi32_ps(v2);
  v3f = _mm256_cvtepi32_ps(v3);    v4f = _mm256_cvtepi32_ps(v4);
  _mm256_storeu_ps(dp+0,_mm256_mul_ps(v1f, vec_scale));
  _mm256_storeu_ps(dp+8,_mm256_mul_ps(v2f, vec_scale));
  _mm256_storeu_ps(dp+16,_mm256_mul_ps(v3f, vec_scale));
  _mm256_storeu_ps(dp+24,_mm256_mul_ps(v4f, vec_scale));
}

/*****************************************************************************/
/* EXTERN                avx2_floats_from_floats_ilv1                        */
/*****************************************************************************/

void
  avx2_floats_from_floats_ilv1(float **dst, float *src, int width,
                               int src_precision, int tgt_precision,
                               bool is_absolute, bool src_signed)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(!is_absolute);
  float scale = 1.0F; // Amount to scale from src to unit range
  while (src_precision < 0)
    { src_precision += 16; scale *= (float)(1<<16); }
  while (src_precision > 16)
    { src_precision -= 16; scale *= 1.0F / (float)(1<<16); }
  scale *= 1.0F / (float)(1<<src_precision);
  float offset = (src_signed)?0.0F:0.5F;

  __m256 vec_scale = _mm256_set1_ps(scale);
  __m256 vec_off = _mm256_set1_ps(offset);
  __m256 v1f, v2f, v3f, v4f;
  
  // Process all but the last 1-32 input words using aligned stores
  float *dp = dst[0];
  float *sp = src;
  for (; width > 32; width-=32, dp+=32, sp+=32)
    { 
      v1f = _mm256_loadu_ps(sp);           v2f = _mm256_loadu_ps(sp+8);
      v3f = _mm256_loadu_ps(sp+16);        v4f = _mm256_loadu_ps(sp+24);
      ((__m256 *) dp)[0] = _mm256_fmsub_ps(v1f, vec_scale, vec_off);
      ((__m256 *) dp)[1] = _mm256_fmsub_ps(v2f, vec_scale, vec_off);
      ((__m256 *) dp)[2] = _mm256_fmsub_ps(v3f, vec_scale, vec_off);
      ((__m256 *) dp)[3] = _mm256_fmsub_ps(v4f, vec_scale, vec_off);
    }
  
  // Finish with 4 unaligned stores, after first shifting the source and
  // destination buffers back sufficiently to ensure that we neither overwrite
  // the output buffer nor overread the input buffer.
  int backtrack = ((-width) & 31);
  sp -= backtrack; dp -= backtrack;
  v1f = _mm256_loadu_ps(sp);           v2f = _mm256_loadu_ps(sp+8);
  v3f = _mm256_loadu_ps(sp+16);        v4f = _mm256_loadu_ps(sp+24);
  _mm256_storeu_ps(dp+0,_mm256_fmsub_ps(v1f, vec_scale, vec_off));
  _mm256_storeu_ps(dp+8,_mm256_fmsub_ps(v2f, vec_scale, vec_off));
  _mm256_storeu_ps(dp+16,_mm256_fmsub_ps(v3f, vec_scale, vec_off));
  _mm256_storeu_ps(dp+24,_mm256_fmsub_ps(v4f, vec_scale, vec_off));
}


/* ========================================================================= */
/*             SIMD functions used by `kdu_stripe_decompressor'              */
/* ========================================================================= */

/*****************************************************************************/
/* EXTERN               avx2_int16_to_uint8_rs_ilv1                          */
/*****************************************************************************/

void avx2_int16_to_uint8_rs_ilv1(kdu_byte *dst, kdu_int16 **src, int width,
                                 int precision, int orig_precision,
                                 bool is_absolute, bool dst_signed,
                                 int preferences)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(!dst_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    orig_precision = KDU_FIX_POINT;
  assert((orig_precision >= precision) && (orig_precision < 16));
    // NB: orig_precision cannot be 16 or the saturating shift below will
    // cause truncation of valid values.
  int downshift = orig_precision - precision;
  int offset_val = (1 << (orig_precision-1)) + ((1<<downshift) >> 1);
  int post_max_val = (1<<precision) - 1; // Max value after conversion

  __m256i vec_off = _mm256_set1_epi16((kdu_int16)offset_val);
  __m128i vec_shift = _mm_cvtsi32_si128(downshift);
  __m256i vec_max = _mm256_set1_epi8((kdu_byte)post_max_val);
  __m256i v1, v2;

  // Generate all but the last 1 to 32 outputs
  kdu_int16 *sp = src[0];
  kdu_byte *dp = dst;
  if (preferences & KDU_STRIPE_STORE_PREF_STREAMING)
    { // Process first vector and then backtrack to align streaming stores
      int advance = 32 - (_addr_to_kdu_int32(dst) & 31);
      v1=_mm256_loadu_si256((__m256i *)(sp+0));
      v2=_mm256_loadu_si256((__m256i *)(sp+16));
      v1 = _mm256_adds_epi16(v1, vec_off);
      v2 = _mm256_adds_epi16(v2, vec_off);
      v1 = _mm256_sra_epi16(v1, vec_shift);
      v2 = _mm256_sra_epi16(v2, vec_shift);
      v1 = _mm256_packus_epi16(v1, v2);  v1 = _mm256_min_epu8(v1, vec_max);
      v1 = _mm256_permute4x64_epi64(v1, 0xD8);
      _mm256_storeu_si256((__m256i *) dp, v1);
      
      width -= advance;  sp += advance;  dp += advance;
      assert(!(_addr_to_kdu_int32(dp) & 31));
      if (downshift == 0)
        { // Special (reversible) case to be accelerated further
          for (; width > 64; width-=64, dp+=64, sp+=64)
            { // Loop unrolled once to hide cost of counter/pointer updates
              v1=_mm256_loadu_si256((__m256i *)(sp+0));
              v2=_mm256_loadu_si256((__m256i *)(sp+16));
              v1 = _mm256_adds_epi16(v1, vec_off);
              v2 = _mm256_adds_epi16(v2, vec_off);
              v1 = _mm256_packus_epi16(v1, v2);
              v1 = _mm256_min_epu8(v1, vec_max);
              v1 = _mm256_permute4x64_epi64(v1, 0xD8);
              _mm256_stream_si256((__m256i *)(dp+0), v1);
              v1=_mm256_loadu_si256((__m256i *)(sp+32));
              v2=_mm256_loadu_si256((__m256i *)(sp+48));
              v1 = _mm256_adds_epi16(v1, vec_off);
              v2 = _mm256_adds_epi16(v2, vec_off);
              v1 = _mm256_packus_epi16(v1, v2);
              v1 = _mm256_min_epu8(v1, vec_max);
              v1 = _mm256_permute4x64_epi64(v1, 0xD8);
              _mm256_stream_si256((__m256i *)(dp+32), v1);              
            }
        }
      else if ((downshift == 5) && (post_max_val == 255))
        { // Special (KDU_FIX_POINT to 8 bit) case to be accelerated further
          for (; width > 64; width-=64, dp+=64, sp+=64)
            { // Loop unrolled once to hide cost of counter/pointer updates
              v1=_mm256_loadu_si256((__m256i *)(sp+0));
              v2=_mm256_loadu_si256((__m256i *)(sp+16));
              v1 = _mm256_adds_epi16(v1, vec_off);
              v2 = _mm256_adds_epi16(v2, vec_off);
              v1 = _mm256_srai_epi16(v1, 5);
              v2 = _mm256_srai_epi16(v2, 5);
              v1 = _mm256_packus_epi16(v1, v2);
              v1 = _mm256_permute4x64_epi64(v1, 0xD8);
              _mm256_stream_si256((__m256i *)(dp+0), v1);
              v1=_mm256_loadu_si256((__m256i *)(sp+32));
              v2=_mm256_loadu_si256((__m256i *)(sp+48));
              v1 = _mm256_adds_epi16(v1, vec_off);
              v2 = _mm256_adds_epi16(v2, vec_off);
              v1 = _mm256_srai_epi16(v1, 5);
              v2 = _mm256_srai_epi16(v2, 5);
              v1 = _mm256_packus_epi16(v1, v2);
              v1 = _mm256_permute4x64_epi64(v1, 0xD8);
              _mm256_stream_si256((__m256i *)(dp+32), v1);
            }
        }
      // Finish with generic processing loop based on streaming stores; this
      // will also takes care of completing the unrolled loops above.
      for (; width > 32; width-=32, dp+=32, sp+=32)
        { 
          v1=_mm256_loadu_si256((__m256i *)(sp+0));
          v2=_mm256_loadu_si256((__m256i *)(sp+16));
          v1 = _mm256_adds_epi16(v1, vec_off);
          v2 = _mm256_adds_epi16(v2, vec_off);
          v1 = _mm256_sra_epi16(v1, vec_shift);
          v2 = _mm256_sra_epi16(v2, vec_shift);
          v1 = _mm256_packus_epi16(v1, v2);
          v1 = _mm256_min_epu8(v1, vec_max);
          v1 = _mm256_permute4x64_epi64(v1, 0xD8);
          _mm256_stream_si256((__m256i *) dp, v1);
        }
    }
  else
    { // Regular (potentially) unaligned stores
      if (downshift == 0)
        { // Special (reversible) case to be accelerated further
          for (; width > 64; width-=64, dp+=64, sp+=64)
            { // Loop unrolled once to hide cost of counter/pointer updates
              v1=((__m256i *) sp)[0];  v2=((__m256i *) sp)[1];
              v1 = _mm256_adds_epi16(v1, vec_off);
              v2 = _mm256_adds_epi16(v2, vec_off);
              v1 = _mm256_packus_epi16(v1, v2);
              v1 = _mm256_min_epu8(v1, vec_max);
              v1 = _mm256_permute4x64_epi64(v1, 0xD8);
              _mm256_storeu_si256((__m256i *)(dp+0), v1);
              v1=((__m256i *) sp)[2];  v2=((__m256i *) sp)[3];
              v1 = _mm256_adds_epi16(v1, vec_off);
              v2 = _mm256_adds_epi16(v2, vec_off);
              v1 = _mm256_packus_epi16(v1, v2);
              v1 = _mm256_min_epu8(v1, vec_max);
              v1 = _mm256_permute4x64_epi64(v1, 0xD8);
              _mm256_storeu_si256((__m256i *)(dp+32), v1);
            }
        }
      else if ((downshift == 5) && (post_max_val == 255))
        { // Special (KDU_FIX_POINT to 8 bit) case to be accelerated further
          for (; width > 64; width-=64, dp+=64, sp+=64)
            { // Loop unrolled once to hide cost of counter/pointer updates
              v1=((__m256i *) sp)[0];  v2=((__m256i *) sp)[1];
              v1 = _mm256_adds_epi16(v1, vec_off);
              v2 = _mm256_adds_epi16(v2, vec_off);
              v1 = _mm256_srai_epi16(v1, 5);
              v2 = _mm256_srai_epi16(v2, 5);
              v1 = _mm256_packus_epi16(v1, v2);
              v1 = _mm256_permute4x64_epi64(v1, 0xD8);
              _mm256_storeu_si256((__m256i *)(dp+0), v1);
              v1=((__m256i *) sp)[2];  v2=((__m256i *) sp)[3];
              v1 = _mm256_adds_epi16(v1, vec_off);
              v2 = _mm256_adds_epi16(v2, vec_off);
              v1 = _mm256_srai_epi16(v1, 5);
              v2 = _mm256_srai_epi16(v2, 5);
              v1 = _mm256_packus_epi16(v1, v2);
              v1 = _mm256_permute4x64_epi64(v1, 0xD8);
              _mm256_storeu_si256((__m256i *)(dp+32), v1);
            }
        }
      // Finish with generic processing loop based on aligned reads and
      // (potentially) unaligned stores, which also takes care of completing
      // the unrolled loops above
      for (; width > 32; width-=32, dp+=32, sp+=32)
        { 
          v1=((__m256i *) sp)[0];  v2=((__m256i *) sp)[1];
          v1 = _mm256_adds_epi16(v1, vec_off);
          v2 = _mm256_adds_epi16(v2, vec_off);
          v1 = _mm256_sra_epi16(v1, vec_shift);
          v2 = _mm256_sra_epi16(v2, vec_shift);
          v1 = _mm256_packus_epi16(v1, v2);
          v1 = _mm256_min_epu8(v1, vec_max);
          v1 = _mm256_permute4x64_epi64(v1, 0xD8);
          _mm256_storeu_si256((__m256i *) dp, v1);
        }
    }
  
  // Backtrack 0 to 31 positions so we can use vector processing also for the
  // final output samples.  This generally involves unaligned loads and stores.
  int backtrack = ((-width) & 31);
  sp -= backtrack; dp -= backtrack;
  v1=_mm256_loadu_si256((__m256i *)(sp+0));
  v2=_mm256_loadu_si256((__m256i *)(sp+16));
  v1 = _mm256_adds_epi16(v1, vec_off);
  v2 = _mm256_adds_epi16(v2, vec_off);
  v1 = _mm256_sra_epi16(v1, vec_shift);
  v2 = _mm256_sra_epi16(v2, vec_shift);
  v1 = _mm256_packus_epi16(v1, v2);   v1 = _mm256_min_epu8(v1, vec_max);
  v1 = _mm256_permute4x64_epi64(v1, 0xD8);
  _mm256_storeu_si256((__m256i *) dp, v1);
}

/*****************************************************************************/
/* EXTERN               avx2_int16_to_uint8_rs_ilv3                          */
/*****************************************************************************/

void avx2_int16_to_uint8_rs_ilv3(kdu_byte *dst, kdu_int16 **src, int width,
                                 int precision, int orig_precision,
                                 bool is_absolute, bool dst_signed,
                                 int preferences)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(!dst_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    orig_precision = KDU_FIX_POINT;
  assert((orig_precision >= precision) && (orig_precision < 16));
    // NB: orig_precision cannot be 16 or the saturating shift below will
    // cause truncation of valid values.
  int downshift = orig_precision - precision;
  int offset_val = (1 << (orig_precision-1)) + ((1<<downshift) >> 1);
  int post_max_val = (1<<precision) - 1; // Max value after conversion

  __m256i vec_shuffle_556 = _mm256_loadu_si256((__m256i *) kd_shuffle_556);
  __m256i vec_shuffle_565 = _mm256_loadu_si256((__m256i *) kd_shuffle_565);
  __m256i vec_shuffle_655 = _mm256_loadu_si256((__m256i *) kd_shuffle_655);
  __m256i vec_off = _mm256_set1_epi16((kdu_int16)offset_val);
  __m128i vec_shift = _mm_cvtsi32_si128(downshift);
  __m256i vec_max = _mm256_set1_epi8((kdu_byte)post_max_val);
  __m256i v1, v2, v3, va, vb, vc;

  // Generate all but the last 1 to 32 output triplets
  kdu_byte *dp = dst;
  kdu_int16 *sp1=src[0], *sp2=src[1], *sp3=src[2];
  if (preferences & KDU_STRIPE_STORE_PREF_STREAMING)
    { // Want streaming aligned stores; may need to reposition pointers; see
      // the non-streaming loop below for explanatory comments, that have
      // been eliminated from the other similar copies.
      int rem (_addr_to_kdu_int32(dst) & 31);
      int advance = (11*rem) >> 5; // Fast divide by 3 for `rem' < 32
      rem -= 3*advance;  advance += 11*rem;  advance = 32-advance;
      v1=_mm256_loadu_si256((__m256i *)(sp1+0));
      va=_mm256_loadu_si256((__m256i *)(sp1+16));
      v1=_mm256_adds_epi16(v1,vec_off);  va=_mm256_adds_epi16(va,vec_off);
      v1=_mm256_sra_epi16(v1,vec_shift); va=_mm256_sra_epi16(va,vec_shift);
      v1 = _mm256_packus_epi16(v1,va);   v1 = _mm256_min_epu8(v1,vec_max);
      v1 = _mm256_permute4x64_epi64(v1,0xD8);
      v2=_mm256_loadu_si256((__m256i *)(sp2+0));
      vb=_mm256_loadu_si256((__m256i *)(sp2+16));
      v2=_mm256_adds_epi16(v2,vec_off);  vb=_mm256_adds_epi16(vb,vec_off);
      v2=_mm256_sra_epi16(v2,vec_shift); vb=_mm256_sra_epi16(vb,vec_shift);
      v2 = _mm256_packus_epi16(v2,vb);   v2 = _mm256_min_epu8(v2,vec_max);
      v2 = _mm256_permute4x64_epi64(v2,0xD8);
      v3=_mm256_loadu_si256((__m256i *)(sp3+0));
      vc=_mm256_loadu_si256((__m256i *)(sp3+16));
      v3=_mm256_adds_epi16(v3,vec_off);  vc=_mm256_adds_epi16(vc,vec_off);
      v3=_mm256_sra_epi16(v3,vec_shift); vc=_mm256_sra_epi16(vc,vec_shift);
      v3 = _mm256_packus_epi16(v3,vc);   v3 = _mm256_min_epu8(v3,vec_max);
      v3 = _mm256_permute4x64_epi64(v3,0xD8);
      va = _mm256_slli_si256(v1,10);
      va = _mm256_alignr_epi8(v2,va,5);  v2 = _mm256_srli_si256(v2, 5);
      va = _mm256_alignr_epi8(v3,va,5);  v3 = _mm256_srli_si256(v3, 5);
      va = _mm256_shuffle_epi8(va,vec_shuffle_556);
      vb = _mm256_slli_si256(v1,5);
      vb = _mm256_alignr_epi8(v2,vb,6);     v2 = _mm256_srli_si256(v2, 6);
      vb = _mm256_alignr_epi8(v3,vb,5);     v3 = _mm256_srli_si256(v3, 5);
      vb = _mm256_shuffle_epi8(vb,vec_shuffle_565);
      vc = _mm256_alignr_epi8(v2,v1,5);
      vc = _mm256_alignr_epi8(v3,vc,6);
      vc = _mm256_shuffle_epi8(vc,vec_shuffle_655);
      v1 = _mm256_permute2x128_si256(va,vb,0x20);
      v2 = _mm256_permute2x128_si256(vc,va,0x30);
      v3 = _mm256_permute2f128_si256(vb,vc,0x31);
      _mm256_storeu_si256((__m256i *) dp, v1);
      _mm256_storeu_si256((__m256i *)(dp+32), v2);
      _mm256_storeu_si256((__m256i *)(dp+64), v3);
      
      assert((advance >= 0) && (advance < 32));      
      width -= advance;  dp += 3*advance;
      sp1 += advance;  sp2 += advance;  sp3 += advance;
      assert(!(_addr_to_kdu_int32(dp) & 31));
      for (; width > 32; width-=32, sp1+=32, sp2+=32, sp3+=32, dp+=96)
        { 
          v1=_mm256_loadu_si256((__m256i *)(sp1+0));
          va=_mm256_loadu_si256((__m256i *)(sp1+16));
          v1=_mm256_adds_epi16(v1,vec_off);  va=_mm256_adds_epi16(va,vec_off);
          v1=_mm256_sra_epi16(v1,vec_shift); va=_mm256_sra_epi16(va,vec_shift);
          v1 = _mm256_packus_epi16(v1,va);   v1 = _mm256_min_epu8(v1,vec_max);
          v1 = _mm256_permute4x64_epi64(v1,0xD8);
          v2=_mm256_loadu_si256((__m256i *)(sp2+0));
          vb=_mm256_loadu_si256((__m256i *)(sp2+16));
          v2=_mm256_adds_epi16(v2,vec_off);  vb=_mm256_adds_epi16(vb,vec_off);
          v2=_mm256_sra_epi16(v2,vec_shift); vb=_mm256_sra_epi16(vb,vec_shift);
          v2 = _mm256_packus_epi16(v2,vb);   v2 = _mm256_min_epu8(v2,vec_max);
          v2 = _mm256_permute4x64_epi64(v2,0xD8);
          v3=_mm256_loadu_si256((__m256i *)(sp3+0));
          vc=_mm256_loadu_si256((__m256i *)(sp3+16));
          v3=_mm256_adds_epi16(v3,vec_off);  vc=_mm256_adds_epi16(vc,vec_off);
          v3=_mm256_sra_epi16(v3,vec_shift); vc=_mm256_sra_epi16(vc,vec_shift);
          v3 = _mm256_packus_epi16(v3,vc);   v3 = _mm256_min_epu8(v3,vec_max);
          v3 = _mm256_permute4x64_epi64(v3,0xD8);
          va = _mm256_slli_si256(v1,10);
          va = _mm256_alignr_epi8(v2,va,5);  v2 = _mm256_srli_si256(v2, 5);
          va = _mm256_alignr_epi8(v3,va,5);  v3 = _mm256_srli_si256(v3, 5);
          va = _mm256_shuffle_epi8(va,vec_shuffle_556);
          vb = _mm256_slli_si256(v1,5);
          vb = _mm256_alignr_epi8(v2,vb,6);     v2 = _mm256_srli_si256(v2, 6);
          vb = _mm256_alignr_epi8(v3,vb,5);     v3 = _mm256_srli_si256(v3, 5);
          vb = _mm256_shuffle_epi8(vb,vec_shuffle_565);
          vc = _mm256_alignr_epi8(v2,v1,5);
          vc = _mm256_alignr_epi8(v3,vc,6);
          vc = _mm256_shuffle_epi8(vc,vec_shuffle_655);
          v1 = _mm256_permute2x128_si256(va,vb,0x20);
          v2 = _mm256_permute2x128_si256(vc,va,0x30);
          v3 = _mm256_permute2f128_si256(vb,vc,0x31);
          _mm256_stream_si256((__m256i *) dp, v1);
          _mm256_stream_si256((__m256i *)(dp+32), v2);
          _mm256_stream_si256((__m256i *)(dp+64), v3);
        }
    }
  else
    for (; width > 32; width-=32, sp1+=32, sp2+=32, sp3+=32, dp+=96)
      { 
        // Prepare vectors with: 16xR, 16xG, 16xB
        v1=_mm256_loadu_si256((__m256i *)(sp1+0));
        va=_mm256_loadu_si256((__m256i *)(sp1+16));
        v1=_mm256_adds_epi16(v1, vec_off);  va=_mm256_adds_epi16(va, vec_off);
        v1=_mm256_sra_epi16(v1, vec_shift); va=_mm256_sra_epi16(va, vec_shift);
        v1 = _mm256_packus_epi16(v1, va);   v1 = _mm256_min_epu8(v1, vec_max);
        v1 = _mm256_permute4x64_epi64(v1, 0xD8); // Restores natural byte order
        v2=_mm256_loadu_si256((__m256i *)(sp2+0));
        vb=_mm256_loadu_si256((__m256i *)(sp2+16));
        v2=_mm256_adds_epi16(v2, vec_off);  vb=_mm256_adds_epi16(vb, vec_off);
        v2=_mm256_sra_epi16(v2, vec_shift); vb=_mm256_sra_epi16(vb, vec_shift);
        v2 = _mm256_packus_epi16(v2, vb);   v2 = _mm256_min_epu8(v2, vec_max);
        v2 = _mm256_permute4x64_epi64(v2, 0xD8); // Restores natural byte order
        v3=_mm256_loadu_si256((__m256i *)(sp3+0));
        vc=_mm256_loadu_si256((__m256i *)(sp3+16));
        v3=_mm256_adds_epi16(v3, vec_off);  vc=_mm256_adds_epi16(vc, vec_off);
        v3=_mm256_sra_epi16(v3, vec_shift); vc=_mm256_sra_epi16(vc, vec_shift);
        v3 = _mm256_packus_epi16(v3, vc);   v3 = _mm256_min_epu8(v3, vec_max);
        v3 = _mm256_permute4x64_epi64(v3, 0xD8); // Restores natural byte order
        
        // Perform lane-wise colour component interleaving
        va = _mm256_slli_si256(v1, 10); // Keep first 6 red bytes
        va = _mm256_alignr_epi8(v2, va, 5);  v2 = _mm256_srli_si256(v2, 5);
        va = _mm256_alignr_epi8(v3, va, 5);  v3 = _mm256_srli_si256(v3, 5);
        va = _mm256_shuffle_epi8(va, vec_shuffle_556); // [5B|5G|6R] per lane
        vb = _mm256_slli_si256(v1, 5); // Keep next 5 red bytes
        vb = _mm256_alignr_epi8(v2, vb, 6);     v2 = _mm256_srli_si256(v2, 6);
        vb = _mm256_alignr_epi8(v3, vb, 5);     v3 = _mm256_srli_si256(v3, 5);
        vb = _mm256_shuffle_epi8(vb, vec_shuffle_565); // [5B|6G|5R] per lane
        vc = _mm256_alignr_epi8(v2, v1, 5);
        vc = _mm256_alignr_epi8(v3, vc, 6);
        vc = _mm256_shuffle_epi8(vc, vec_shuffle_655); // [6B|5G|5R] per lane
        
        // At this point the low lanes of va,vb,vc together represent the first
        // 48 output bytes, while the high lanes represent the next 48 output
        // bytes.  Use 128-bit permutations to rearrange so that we can store
        // three 256-bit vectors consecutively.
        v1 = _mm256_permute2x128_si256(va, vb, 0x20);
        v2 = _mm256_permute2x128_si256(vc, va, 0x30);
        v3 = _mm256_permute2f128_si256(vb, vc, 0x31);
        _mm256_storeu_si256((__m256i *) dp, v1);
        _mm256_storeu_si256((__m256i *)(dp+32), v2);
        _mm256_storeu_si256((__m256i *)(dp+64), v3);
      }
  
  // Backtrack 0 to 31 src samples so we can use vector processing also for the
  // final output samples.  This generally involves unaligned loads and stores.
  int backtrack = ((-width) & 31);
  sp1 -= backtrack;  sp2 -= backtrack;  sp3 -= backtrack;  dp -= 3*backtrack;
  v1=_mm256_loadu_si256((__m256i *)(sp1+0));
  va=_mm256_loadu_si256((__m256i *)(sp1+16));
  v1=_mm256_adds_epi16(v1, vec_off);  va=_mm256_adds_epi16(va, vec_off);
  v1=_mm256_sra_epi16(v1, vec_shift); va=_mm256_sra_epi16(va, vec_shift);
  v1 = _mm256_packus_epi16(v1, va);   v1 = _mm256_min_epu8(v1, vec_max);
  v1 = _mm256_permute4x64_epi64(v1, 0xD8); // Restores natural byte order
  v2=_mm256_loadu_si256((__m256i *)(sp2+0));
  vb=_mm256_loadu_si256((__m256i *)(sp2+16));
  v2=_mm256_adds_epi16(v2, vec_off);  vb=_mm256_adds_epi16(vb, vec_off);
  v2=_mm256_sra_epi16(v2, vec_shift); vb=_mm256_sra_epi16(vb, vec_shift);
  v2 = _mm256_packus_epi16(v2, vb);   v2 = _mm256_min_epu8(v2, vec_max);
  v2 = _mm256_permute4x64_epi64(v2, 0xD8); // Restores natural byte order
  v3=_mm256_loadu_si256((__m256i *)(sp3+0));
  vc=_mm256_loadu_si256((__m256i *)(sp3+16));
  v3=_mm256_adds_epi16(v3, vec_off);  vc=_mm256_adds_epi16(vc, vec_off);
  v3=_mm256_sra_epi16(v3, vec_shift); vc=_mm256_sra_epi16(vc, vec_shift);
  v3 = _mm256_packus_epi16(v3, vc);   v3 = _mm256_min_epu8(v3, vec_max);
  v3 = _mm256_permute4x64_epi64(v3, 0xD8); // Restores natural byte order
  va = _mm256_slli_si256(v1, 10); // Keep first 6 red bytes
  va = _mm256_alignr_epi8(v2, va, 5);  v2 = _mm256_srli_si256(v2, 5);
  va = _mm256_alignr_epi8(v3, va, 5);  v3 = _mm256_srli_si256(v3, 5);
  va = _mm256_shuffle_epi8(va, vec_shuffle_556); // [5B | 5G | 6R] per lane
  vb = _mm256_slli_si256(v1, 5); // Keep next 5 red bytes
  vb = _mm256_alignr_epi8(v2, vb, 6);     v2 = _mm256_srli_si256(v2, 6);
  vb = _mm256_alignr_epi8(v3, vb, 5);     v3 = _mm256_srli_si256(v3, 5);
  vb = _mm256_shuffle_epi8(vb, vec_shuffle_565); // [5B | 6G | 5R] per lane
  vc = _mm256_alignr_epi8(v2, v1, 5);
  vc = _mm256_alignr_epi8(v3, vc, 6);
  vc = _mm256_shuffle_epi8(vc, vec_shuffle_655); // [6B | 5G | 5R] per lane
  v1 = _mm256_permute2x128_si256(va, vb, 0x20);
  v2 = _mm256_permute2x128_si256(vc, va, 0x30);
  v3 = _mm256_permute2f128_si256(vb, vc, 0x31);
  _mm256_storeu_si256((__m256i *) dp, v1);
  _mm256_storeu_si256((__m256i *)(dp+32), v2);
  _mm256_storeu_si256((__m256i *)(dp+64), v3);
}

/*****************************************************************************/
/* EXTERN               avx2_int16_to_uint8_rs_ilv4                          */
/*****************************************************************************/

void avx2_int16_to_uint8_rs_ilv4(kdu_byte *dst, kdu_int16 **src, int width,
                                 int precision, int orig_precision,
                                 bool is_absolute, bool dst_signed,
                                 int preferences)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(!dst_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    orig_precision = KDU_FIX_POINT;
  assert((orig_precision >= precision) && (orig_precision < 16));
    // NB: orig_precision cannot be 16 or the saturating shift below will
    // cause truncation of valid values.
  int downshift = orig_precision - precision;
  int offset_val = (1 << (orig_precision-1)) + ((1<<downshift) >> 1);
  int post_max_val = (1<<precision) - 1; // Max value after conversion

  __m256i byte_shuffle = _mm256_loadu_si256((__m256i *) kd_shuffle_r4444);
  __m256i dword_shuffle = _mm256_loadu_si256((__m256i *) kd_dilv_dwords);
  __m256i vec_off = _mm256_set1_epi16((kdu_int16)offset_val);
  __m128i vec_shift = _mm_cvtsi32_si128(downshift);
  __m256i vec_max = _mm256_set1_epi8((kdu_byte)post_max_val);
  __m256i v1, v2, v3;

  // Generate all but the last 1 to 16 output quartets
  kdu_int16 *sp1=src[0], *sp2=src[1], *sp3=src[2], *sp4=src[3];
  kdu_byte *dp = dst;
  if ((preferences & KDU_STRIPE_STORE_PREF_STREAMING) &&
      !(_addr_to_kdu_int32(dst) & 3))
    { // Want streaming aligned stores; may need to reposition pointers; see
      // the non-streaming loop below for explanatory comments, that have
      // been eliminated from the other similar copies.
      int advance = (64 - (_addr_to_kdu_int32(dst) & 31)) >> 2;
      v3 = _mm256_loadu_si256((__m256i *)(sp1+0));
      v3 = _mm256_adds_epi16(v3, vec_off);
      v3 = _mm256_sra_epi16(v3, vec_shift);
      v2 = _mm256_loadu_si256((__m256i *)(sp2+0));
      v2 = _mm256_adds_epi16(v2, vec_off);
      v2 = _mm256_sra_epi16(v2, vec_shift);
      v1 = _mm256_packus_epi16(v3, v2);  v1 = _mm256_min_epu8(v1, vec_max);          
      v3 = _mm256_loadu_si256((__m256i *)(sp3+0));
      v3 = _mm256_adds_epi16(v3, vec_off);
      v3 = _mm256_sra_epi16(v3, vec_shift);
      v2 = _mm256_loadu_si256((__m256i *)(sp4+0));
      v2 = _mm256_adds_epi16(v2, vec_off);
      v2 = _mm256_sra_epi16(v2, vec_shift);
      v2 = _mm256_packus_epi16(v3, v2);  v2 = _mm256_min_epu8(v2, vec_max);
      v3 = _mm256_permute2x128_si256(v1, v2, 0x20);
      v2 = _mm256_permute2x128_si256(v1, v2, 0x31);
      v1 = _mm256_permutevar8x32_epi32(v3, dword_shuffle);
      v2 = _mm256_permutevar8x32_epi32(v2, dword_shuffle);
      v1 = _mm256_shuffle_epi8(v1, byte_shuffle);
      v2 = _mm256_shuffle_epi8(v2, byte_shuffle);
      _mm256_storeu_si256((__m256i *) dp, v1);
      _mm256_storeu_si256((__m256i *)(dp+32), v2);
      
      width -= advance;  dp += 4*advance;
      sp1 += advance;  sp2 += advance;  sp3 += advance;  sp4 += advance;
      assert(!(_addr_to_kdu_int32(dp) & 31));
      for (; width > 16; width-=16, sp1+=16, sp2+=16, sp3+=16, sp4+=16, dp+=64)
        { 
          v3 = _mm256_loadu_si256((__m256i *)(sp1+0));
          v3 = _mm256_adds_epi16(v3, vec_off);
          v3 = _mm256_sra_epi16(v3, vec_shift);
          v2 = _mm256_loadu_si256((__m256i *)(sp2+0));
          v2 = _mm256_adds_epi16(v2, vec_off);
          v2 = _mm256_sra_epi16(v2, vec_shift);
          v1 = _mm256_packus_epi16(v3, v2);  v1 = _mm256_min_epu8(v1, vec_max);          
          v3 = _mm256_loadu_si256((__m256i *)(sp3+0));
          v3 = _mm256_adds_epi16(v3, vec_off);
          v3 = _mm256_sra_epi16(v3, vec_shift);
          v2 = _mm256_loadu_si256((__m256i *)(sp4+0));
          v2 = _mm256_adds_epi16(v2, vec_off);
          v2 = _mm256_sra_epi16(v2, vec_shift);
          v2 = _mm256_packus_epi16(v3, v2);  v2 = _mm256_min_epu8(v2, vec_max);
          v3 = _mm256_permute2x128_si256(v1, v2, 0x20);
          v2 = _mm256_permute2x128_si256(v1, v2, 0x31);
          v1 = _mm256_permutevar8x32_epi32(v3, dword_shuffle);
          v2 = _mm256_permutevar8x32_epi32(v2, dword_shuffle);
          v1 = _mm256_shuffle_epi8(v1, byte_shuffle);
          v2 = _mm256_shuffle_epi8(v2, byte_shuffle);
          _mm256_stream_si256((__m256i *) dp, v1);
          _mm256_stream_si256((__m256i *)(dp+32), v2);
        }
    }
  else
    for (; width > 16; width-=16, sp1+=16, sp2+=16, sp3+=16, sp4+=16, dp+=64)
      { 
        // Load one vector from each channel and pack in pairs to get
        // [8G | 8R | 8G | 8R] in v1 and [8A | 8B | 8A | 8B] in v2 (MSB to LSB)
        v3 = _mm256_loadu_si256((__m256i *)(sp1+0));
        v3 = _mm256_adds_epi16(v3, vec_off);
        v3 = _mm256_sra_epi16(v3, vec_shift);
        v2 = _mm256_loadu_si256((__m256i *)(sp2+0));
        v2 = _mm256_adds_epi16(v2, vec_off);
        v2 = _mm256_sra_epi16(v2, vec_shift);
        v1 = _mm256_packus_epi16(v3, v2);  v1 = _mm256_min_epu8(v1, vec_max);
        
        v3 = _mm256_loadu_si256((__m256i *)(sp3+0));
        v3 = _mm256_adds_epi16(v3, vec_off);
        v3 = _mm256_sra_epi16(v3, vec_shift);
        v2 = _mm256_loadu_si256((__m256i *)(sp4+0));
        v2 = _mm256_adds_epi16(v2, vec_off);
        v2 = _mm256_sra_epi16(v2, vec_shift);
        v2 = _mm256_packus_epi16(v3, v2);  v2 = _mm256_min_epu8(v2, vec_max);

        // Rearrange the 128-bit lanes of v1 and v2 so that v3 and v2 hold the
        // first and second sets of 32-bytes that need to be interleaved, each
        // organized initially as [8A | 8B | 8G | 8R] (MSB to LSB)
        v3 = _mm256_permute2x128_si256(v1, v2, 0x20);
        v2 = _mm256_permute2x128_si256(v1, v2, 0x31);
        
        // Rearrange dwords in each lane so that each lane of each vector has
        // the organization [4A | 4B | 4G | 4R] from MSB to LSB
        v1 = _mm256_permutevar8x32_epi32(v3, dword_shuffle);
        v2 = _mm256_permutevar8x32_epi32(v2, dword_shuffle);
        
        // Interleave the ABGR bytes within each lane and store the result
        v1 = _mm256_shuffle_epi8(v1, byte_shuffle);
        v2 = _mm256_shuffle_epi8(v2, byte_shuffle);
        _mm256_storeu_si256((__m256i *) dp, v1);
        _mm256_storeu_si256((__m256i *)(dp+32), v2);
      }
  
  // Backtrack 0 to 15 src samples so we can use vector processing also for the
  // final output samples.  This generally involves unaligned loads and stores.
  int backtrack = ((-width) & 15);
  sp1 -= backtrack;  sp2 -= backtrack;  sp3 -= backtrack;  sp4 -= backtrack;
  dp -= 4*backtrack;
  v3 = _mm256_loadu_si256((__m256i *)(sp1+0));
  v3 = _mm256_adds_epi16(v3, vec_off);
  v3 = _mm256_sra_epi16(v3, vec_shift);
  v2 = _mm256_loadu_si256((__m256i *)(sp2+0));
  v2 = _mm256_adds_epi16(v2, vec_off);
  v2 = _mm256_sra_epi16(v2, vec_shift);
  v1 = _mm256_packus_epi16(v3, v2);  v1 = _mm256_min_epu8(v1, vec_max);  
  v3 = _mm256_loadu_si256((__m256i *)(sp3+0));
  v3 = _mm256_adds_epi16(v3, vec_off);
  v3 = _mm256_sra_epi16(v3, vec_shift);
  v2 = _mm256_loadu_si256((__m256i *)(sp4+0));
  v2 = _mm256_adds_epi16(v2, vec_off);
  v2 = _mm256_sra_epi16(v2, vec_shift);
  v2 = _mm256_packus_epi16(v3, v2);  v2 = _mm256_min_epu8(v2, vec_max);
  v3 = _mm256_permute2x128_si256(v1, v2, 0x20);
  v2 = _mm256_permute2x128_si256(v1, v2, 0x31);
  v1 = _mm256_permutevar8x32_epi32(v3, dword_shuffle);
  v2 = _mm256_permutevar8x32_epi32(v2, dword_shuffle);  
  v1 = _mm256_shuffle_epi8(v1, byte_shuffle);
  v2 = _mm256_shuffle_epi8(v2, byte_shuffle);
  _mm256_storeu_si256((__m256i *) dp, v1);
  _mm256_storeu_si256((__m256i *)(dp+32), v2);
}

/*****************************************************************************/
/* EXTERN                avx2_floats_to_uint8_ilv1                           */
/*****************************************************************************/

void avx2_floats_to_uint8_ilv1(kdu_byte *dst, float **src, int width,
                               int precision, int orig_precision,
                               bool is_absolute, bool dst_signed,
                               int preferences)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(!dst_signed); // Function prototype generic; we don't do signed bytes
  float scale = (float)(1<<precision);
  int offset_val = (1 << (precision-1));
  int post_max_val = (1<<precision) - 1; // Max value after conversion

  int mxcsr_orig = _mm_getcsr();
  int mxcsr_cur = mxcsr_orig & ~(3<<13); // Reset rounding control bits
  _mm_setcsr(mxcsr_cur);

  __m256i permd_ctl = _mm256_loadu_si256((__m256i *) kd_ilv_dwords);
  __m256 vec_scale = _mm256_set1_ps(scale);
  __m256i vec_off = _mm256_set1_epi16((kdu_int16)offset_val);
  __m256i vec_max = _mm256_set1_epi8((kdu_byte)post_max_val);
  __m256 v1f, v2f, v3f, v4f;
  __m256i v1, v2, v3, v4;

  // Generate all but the last 1 to 32 outputs
  float *sp=src[0];
  kdu_byte *dp=dst;
  if (preferences & KDU_STRIPE_STORE_PREF_STREAMING)
    { // Want streaming aligned stores; may need to reposition pointers; see
      // the non-streaming loop below for explanatory comments, that have
      // been eliminated from the other similar copies.
      int advance = 32 - (_addr_to_kdu_int32(dst) & 31);
      v1f=_mm256_loadu_ps(sp+0);   v2f=_mm256_loadu_ps(sp+8);
      v3f=_mm256_loadu_ps(sp+16);  v4f=_mm256_loadu_ps(sp+24);
      v1f=_mm256_mul_ps(v1f,vec_scale);  v2f=_mm256_mul_ps(v2f,vec_scale);
      v3f=_mm256_mul_ps(v3f,vec_scale);  v4f=_mm256_mul_ps(v4f,vec_scale);
      v1=_mm256_cvtps_epi32(v1f);        v2=_mm256_cvtps_epi32(v2f);
      v3=_mm256_cvtps_epi32(v3f);        v4=_mm256_cvtps_epi32(v4f);
      v1=_mm256_packs_epi32(v1,v2);      v2=_mm256_packs_epi32(v3,v4);
      v1=_mm256_adds_epi16(v1,vec_off);  v2=_mm256_adds_epi16(v2,vec_off);
      v1=_mm256_packus_epi16(v1,v2);     v1=_mm256_min_epu8(v1,vec_max);
      v1=_mm256_permutevar8x32_epi32(v1, permd_ctl);
      _mm256_storeu_si256((__m256i *) dp, v1);
      
      width -= advance;  dp += advance;  sp += advance;
      assert(!(_addr_to_kdu_int32(dp) & 31));
      for (; width > 32; width-=32, sp+=32, dp+=32)
        { 
          v1f=_mm256_loadu_ps(sp+0);   v2f=_mm256_loadu_ps(sp+8);
          v3f=_mm256_loadu_ps(sp+16);  v4f=_mm256_loadu_ps(sp+24);
          v1f=_mm256_mul_ps(v1f,vec_scale);  v2f=_mm256_mul_ps(v2f,vec_scale);
          v3f=_mm256_mul_ps(v3f,vec_scale);  v4f=_mm256_mul_ps(v4f,vec_scale);
          v1=_mm256_cvtps_epi32(v1f);        v2=_mm256_cvtps_epi32(v2f);
          v3=_mm256_cvtps_epi32(v3f);        v4=_mm256_cvtps_epi32(v4f);
          v1=_mm256_packs_epi32(v1,v2);      v2=_mm256_packs_epi32(v3,v4);
          v1=_mm256_adds_epi16(v1,vec_off);  v2=_mm256_adds_epi16(v2,vec_off);
          v1=_mm256_packus_epi16(v1,v2);     v1=_mm256_min_epu8(v1,vec_max);
          v1=_mm256_permutevar8x32_epi32(v1, permd_ctl);
          _mm256_stream_si256((__m256i *) dp, v1);
        }
    }
  else
    for (; width > 32; width-=32, sp+=32, dp+=32)
      { 
        v1f=_mm256_loadu_ps(sp+0);   v2f=_mm256_loadu_ps(sp+8);
        v3f=_mm256_loadu_ps(sp+16);  v4f=_mm256_loadu_ps(sp+24);
        v1f=_mm256_mul_ps(v1f,vec_scale);  v2f=_mm256_mul_ps(v2f,vec_scale);
        v3f=_mm256_mul_ps(v3f,vec_scale);  v4f=_mm256_mul_ps(v4f,vec_scale);
        v1=_mm256_cvtps_epi32(v1f);        v2=_mm256_cvtps_epi32(v2f);
        v3=_mm256_cvtps_epi32(v3f);        v4=_mm256_cvtps_epi32(v4f);
        v1=_mm256_packs_epi32(v1,v2);      v2=_mm256_packs_epi32(v3,v4);
        
        // At this point, we have v1 and v2 with words whose ultimate positions
        // in the output byte stream will be as follows (MSB to LSB order):
        // v1 = [15:12   7:4 | 11:8    3:0]  v2 = [31:28 23:20 | 27:24 19:16]
        // Next, we apply the 16-bit offsets and pack down to bytes.
        v1=_mm256_adds_epi16(v1,vec_off);  v2=_mm256_adds_epi16(v2,vec_off);
        v1=_mm256_packus_epi16(v1,v2);     v1=_mm256_min_epu8(v1,vec_max);
        
        // v1 has the following organization (MSB to LSB):
        // [31:28 23:20 15:12 7:4  |  27:24 19:16 11:8 3:0]
        // We need to rearrange this into natural order using a dword permutation
        v1=_mm256_permutevar8x32_epi32(v1, permd_ctl);
        _mm256_storeu_si256((__m256i *) dp, v1);
      }
  
  // Backtrack 0 to 31 positions so we can use vector processing also for the
  // final output samples.  This generally involves unaligned loads and stores.
  int backtrack = ((-width) & 15);
  sp -= backtrack;  dp -= backtrack;
  v1f=_mm256_loadu_ps(sp+0);   v2f=_mm256_loadu_ps(sp+8);
  v3f=_mm256_loadu_ps(sp+16);  v4f=_mm256_loadu_ps(sp+24);
  v1f=_mm256_mul_ps(v1f,vec_scale);  v2f=_mm256_mul_ps(v2f,vec_scale);
  v3f=_mm256_mul_ps(v3f,vec_scale);  v4f=_mm256_mul_ps(v4f,vec_scale);
  v1=_mm256_cvtps_epi32(v1f);        v2=_mm256_cvtps_epi32(v2f);
  v3=_mm256_cvtps_epi32(v3f);        v4=_mm256_cvtps_epi32(v4f);
  v1=_mm256_packs_epi32(v1,v2);      v2=_mm256_packs_epi32(v3,v4);
  v1=_mm256_adds_epi16(v1,vec_off);  v2=_mm256_adds_epi16(v2,vec_off);
  v1=_mm256_packus_epi16(v1,v2);     v1=_mm256_min_epu8(v1,vec_max);
  v1=_mm256_permutevar8x32_epi32(v1, permd_ctl);
  _mm256_storeu_si256((__m256i *) dp, v1);
  
  _mm_setcsr(mxcsr_orig); // Restore rounding control bits
}

/*****************************************************************************/
/* EXTERN                avx2_floats_to_uint8_ilv3                           */
/*****************************************************************************/

void avx2_floats_to_uint8_ilv3(kdu_byte *dst, float **src, int width,
                               int precision, int orig_precision,
                               bool is_absolute, bool dst_signed,
                               int preferences)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(!dst_signed); // Function prototype generic; we don't do signed bytes
  float scale = (float)(1<<precision);
  int offset_val = (1 << (precision-1));
  int post_max_val = (1<<precision) - 1; // Max value after conversion

  int mxcsr_orig = _mm_getcsr();
  int mxcsr_cur = mxcsr_orig & ~(3<<13); // Reset rounding control bits
  _mm_setcsr(mxcsr_cur);

  __m256i permd_ctl = _mm256_loadu_si256((__m256i *) kd_ilv_dwords);
  __m256 vec_scale = _mm256_set1_ps(scale);
  __m256i vec_off = _mm256_set1_epi16((kdu_int16)offset_val);
  __m256i vec_max = _mm256_set1_epi8((kdu_byte)post_max_val);
  __m256i vec_shuffle_556 = _mm256_loadu_si256((__m256i *) kd_shuffle_556);
  __m256i vec_shuffle_565 = _mm256_loadu_si256((__m256i *) kd_shuffle_565);
  __m256i vec_shuffle_655 = _mm256_loadu_si256((__m256i *) kd_shuffle_655);
  __m256 v1f, v2f, v3f, v4f;
  __m256i v1, v2, v3, v4, v5, v6, vt;

  // Generate all but the last 1 to 32 output triplets
  kdu_byte *dp = dst;
  float *sp1=src[0], *sp2=src[1], *sp3=src[2];
  if (preferences & KDU_STRIPE_STORE_PREF_STREAMING)
    { // Want streaming aligned stores; may need to reposition pointers; see
      // the non-streaming loop below for explanatory comments, that have
      // been eliminated from the other similar copies.
      int rem (_addr_to_kdu_int32(dst) & 31);
      int advance = (11*rem) >> 5; // Fast divide by 3 for `rem' < 32
      rem -= 3*advance;  advance += 11*rem;  advance = 32-advance;
      v1f=_mm256_loadu_ps(sp1+0);       v2f=_mm256_loadu_ps(sp1+8);
      v1f=_mm256_mul_ps(v1f,vec_scale); v2f=_mm256_mul_ps(v2f,vec_scale);
      v1=_mm256_cvtps_epi32(v1f);       v2=_mm256_cvtps_epi32(v2f);
      v1=_mm256_packs_epi32(v1, v2);
      v3f=_mm256_loadu_ps(sp1+16);       v4f=_mm256_loadu_ps(sp1+24);
      v3f=_mm256_mul_ps(v3f, vec_scale); v4f=_mm256_mul_ps(v4f,vec_scale);
      v2=_mm256_cvtps_epi32(v3f);        v3=_mm256_cvtps_epi32(v4f);
      v2=_mm256_packs_epi32(v2, v3);
      v1f=_mm256_loadu_ps(sp2+0);        v2f=_mm256_loadu_ps(sp2+8);
      v1f=_mm256_mul_ps(v1f,vec_scale);  v2f=_mm256_mul_ps(v2f,vec_scale);
      v3=_mm256_cvtps_epi32(v1f);        v4=_mm256_cvtps_epi32(v2f);
      v3=_mm256_packs_epi32(v3, v4);
      v3f=_mm256_loadu_ps(sp2+16);       v4f=_mm256_loadu_ps(sp2+24);
      v3f=_mm256_mul_ps(v3f,vec_scale);  v4f=_mm256_mul_ps(v4f,vec_scale);
      v4=_mm256_cvtps_epi32(v3f);        v5=_mm256_cvtps_epi32(v4f);
      v4=_mm256_packs_epi32(v4, v5);
      v1f=_mm256_loadu_ps(sp3+0);        v2f=_mm256_loadu_ps(sp3+8);
      v1f=_mm256_mul_ps(v1f,vec_scale);  v2f=_mm256_mul_ps(v2f,vec_scale);
      v5=_mm256_cvtps_epi32(v1f);        v6=_mm256_cvtps_epi32(v2f);
      v5=_mm256_packs_epi32(v5, v6);
      v3f=_mm256_loadu_ps(sp3+16);       v4f=_mm256_loadu_ps(sp3+24);
      v3f=_mm256_mul_ps(v3f,vec_scale);  v4f=_mm256_mul_ps(v4f,vec_scale);
      v6=_mm256_cvtps_epi32(v3f);        vt=_mm256_cvtps_epi32(v4f);
      v6=_mm256_packs_epi32(v6, vt);
      v1=_mm256_adds_epi16(v1,vec_off);  v2=_mm256_adds_epi16(v2,vec_off);
      v1=_mm256_packus_epi16(v1,v2);     v1=_mm256_min_epu8(v1,vec_max);
      v1=_mm256_permutevar8x32_epi32(v1,permd_ctl);
      v3=_mm256_adds_epi16(v3,vec_off);  v4=_mm256_adds_epi16(v4,vec_off);
      v2=_mm256_packus_epi16(v3,v4);     v2=_mm256_min_epu8(v2,vec_max);
      v2=_mm256_permutevar8x32_epi32(v2,permd_ctl);
      v5=_mm256_adds_epi16(v5,vec_off);  v6=_mm256_adds_epi16(v6,vec_off);
      v3=_mm256_packus_epi16(v5,v6);     v3=_mm256_min_epu8(v3,vec_max);
      v3=_mm256_permutevar8x32_epi32(v3,permd_ctl);
      v4=_mm256_slli_si256(v1,10); // Keep first 6 red bytes
      v4=_mm256_alignr_epi8(v2,v4,5);    v2=_mm256_srli_si256(v2,5);
      v4=_mm256_alignr_epi8(v3,v4,5);    v3=_mm256_srli_si256(v3,5);
      v4=_mm256_shuffle_epi8(v4,vec_shuffle_556);
      v5=_mm256_slli_si256(v1,5); // Keep next 5 red bytes
      v5=_mm256_alignr_epi8(v2,v5,6);    v2=_mm256_srli_si256(v2,6);
      v5=_mm256_alignr_epi8(v3,v5,5);    v3=_mm256_srli_si256(v3,5);
      v5=_mm256_shuffle_epi8(v5,vec_shuffle_565);
      v6=_mm256_alignr_epi8(v2,v1,5);
      v6=_mm256_alignr_epi8(v3,v6,6);
      v6=_mm256_shuffle_epi8(v6,vec_shuffle_655);
      v1=_mm256_permute2x128_si256(v4,v5,0x20);
      v2=_mm256_permute2x128_si256(v6,v4,0x30);
      v3=_mm256_permute2f128_si256(v5,v6,0x31);
      _mm256_storeu_si256((__m256i *) dp,v1);
      _mm256_storeu_si256((__m256i *)(dp+32),v2);
      _mm256_storeu_si256((__m256i *)(dp+64),v3);
      
      assert((advance >= 0) && (advance < 32));      
      width -= advance;  dp += 3*advance;
      sp1 += advance;  sp2 += advance;  sp3 += advance;
      assert(!(_addr_to_kdu_int32(dp) & 31));
      for (; width > 32; width-=32, sp1+=32, sp2+=32, sp3+=32, dp+=96)
        { 
          v1f=_mm256_loadu_ps(sp1+0);       v2f=_mm256_loadu_ps(sp1+8);
          v1f=_mm256_mul_ps(v1f,vec_scale); v2f=_mm256_mul_ps(v2f,vec_scale);
          v1=_mm256_cvtps_epi32(v1f);       v2=_mm256_cvtps_epi32(v2f);
          v1=_mm256_packs_epi32(v1, v2);
          v3f=_mm256_loadu_ps(sp1+16);       v4f=_mm256_loadu_ps(sp1+24);
          v3f=_mm256_mul_ps(v3f, vec_scale); v4f=_mm256_mul_ps(v4f,vec_scale);
          v2=_mm256_cvtps_epi32(v3f);        v3=_mm256_cvtps_epi32(v4f);
          v2=_mm256_packs_epi32(v2, v3);
          v1f=_mm256_loadu_ps(sp2+0);        v2f=_mm256_loadu_ps(sp2+8);
          v1f=_mm256_mul_ps(v1f,vec_scale);  v2f=_mm256_mul_ps(v2f,vec_scale);
          v3=_mm256_cvtps_epi32(v1f);        v4=_mm256_cvtps_epi32(v2f);
          v3=_mm256_packs_epi32(v3, v4);
          v3f=_mm256_loadu_ps(sp2+16);       v4f=_mm256_loadu_ps(sp2+24);
          v3f=_mm256_mul_ps(v3f,vec_scale);  v4f=_mm256_mul_ps(v4f,vec_scale);
          v4=_mm256_cvtps_epi32(v3f);        v5=_mm256_cvtps_epi32(v4f);
          v4=_mm256_packs_epi32(v4, v5);
          v1f=_mm256_loadu_ps(sp3+0);        v2f=_mm256_loadu_ps(sp3+8);
          v1f=_mm256_mul_ps(v1f,vec_scale);  v2f=_mm256_mul_ps(v2f,vec_scale);
          v5=_mm256_cvtps_epi32(v1f);        v6=_mm256_cvtps_epi32(v2f);
          v5=_mm256_packs_epi32(v5, v6);
          v3f=_mm256_loadu_ps(sp3+16);       v4f=_mm256_loadu_ps(sp3+24);
          v3f=_mm256_mul_ps(v3f,vec_scale);  v4f=_mm256_mul_ps(v4f,vec_scale);
          v6=_mm256_cvtps_epi32(v3f);        vt=_mm256_cvtps_epi32(v4f);
          v6=_mm256_packs_epi32(v6, vt);
          v1=_mm256_adds_epi16(v1,vec_off);  v2=_mm256_adds_epi16(v2,vec_off);
          v1=_mm256_packus_epi16(v1,v2);     v1=_mm256_min_epu8(v1,vec_max);
          v1=_mm256_permutevar8x32_epi32(v1,permd_ctl);
          v3=_mm256_adds_epi16(v3,vec_off);  v4=_mm256_adds_epi16(v4,vec_off);
          v2=_mm256_packus_epi16(v3,v4);     v2=_mm256_min_epu8(v2,vec_max);
          v2=_mm256_permutevar8x32_epi32(v2,permd_ctl);
          v5=_mm256_adds_epi16(v5,vec_off);  v6=_mm256_adds_epi16(v6,vec_off);
          v3=_mm256_packus_epi16(v5,v6);     v3=_mm256_min_epu8(v3,vec_max);
          v3=_mm256_permutevar8x32_epi32(v3,permd_ctl);
          v4=_mm256_slli_si256(v1,10); // Keep first 6 red bytes
          v4=_mm256_alignr_epi8(v2,v4,5);    v2=_mm256_srli_si256(v2,5);
          v4=_mm256_alignr_epi8(v3,v4,5);    v3=_mm256_srli_si256(v3,5);
          v4=_mm256_shuffle_epi8(v4,vec_shuffle_556);
          v5=_mm256_slli_si256(v1,5); // Keep next 5 red bytes
          v5=_mm256_alignr_epi8(v2,v5,6);    v2=_mm256_srli_si256(v2,6);
          v5=_mm256_alignr_epi8(v3,v5,5);    v3=_mm256_srli_si256(v3,5);
          v5=_mm256_shuffle_epi8(v5,vec_shuffle_565);
          v6=_mm256_alignr_epi8(v2,v1,5);
          v6=_mm256_alignr_epi8(v3,v6,6);
          v6=_mm256_shuffle_epi8(v6,vec_shuffle_655);
          v1=_mm256_permute2x128_si256(v4,v5,0x20);
          v2=_mm256_permute2x128_si256(v6,v4,0x30);
          v3=_mm256_permute2f128_si256(v5,v6,0x31);
          _mm256_stream_si256((__m256i *) dp,v1);
          _mm256_stream_si256((__m256i *)(dp+32),v2);
          _mm256_stream_si256((__m256i *)(dp+64),v3);
        }
    }
  else  
    for (; width > 32; width-=32, sp1+=32, sp2+=32, sp3+=32, dp+=96)
      { 
        // Prepare three pairs of vectors, each with 16 packed 16-bit words
        // whose organization is the same as that described above in
        // conjunction with the `avx2_floats_to_uint8_ilv1' function.
        v1f=_mm256_loadu_ps(sp1+0);       v2f=_mm256_loadu_ps(sp1+8);
        v1f=_mm256_mul_ps(v1f,vec_scale); v2f=_mm256_mul_ps(v2f,vec_scale);
        v1=_mm256_cvtps_epi32(v1f);       v2=_mm256_cvtps_epi32(v2f);
        v1=_mm256_packs_epi32(v1, v2);
        v3f=_mm256_loadu_ps(sp1+16);       v4f=_mm256_loadu_ps(sp1+24);
        v3f=_mm256_mul_ps(v3f, vec_scale); v4f=_mm256_mul_ps(v4f,vec_scale);
        v2=_mm256_cvtps_epi32(v3f);        v3=_mm256_cvtps_epi32(v4f);
        v2=_mm256_packs_epi32(v2, v3);
        v1f=_mm256_loadu_ps(sp2+0);        v2f=_mm256_loadu_ps(sp2+8);
        v1f=_mm256_mul_ps(v1f,vec_scale);  v2f=_mm256_mul_ps(v2f,vec_scale);
        v3=_mm256_cvtps_epi32(v1f);        v4=_mm256_cvtps_epi32(v2f);
        v3=_mm256_packs_epi32(v3, v4);
        v3f=_mm256_loadu_ps(sp2+16);       v4f=_mm256_loadu_ps(sp2+24);
        v3f=_mm256_mul_ps(v3f,vec_scale);  v4f=_mm256_mul_ps(v4f,vec_scale);
        v4=_mm256_cvtps_epi32(v3f);        v5=_mm256_cvtps_epi32(v4f);
        v4=_mm256_packs_epi32(v4, v5);
        v1f=_mm256_loadu_ps(sp3+0);        v2f=_mm256_loadu_ps(sp3+8);
        v1f=_mm256_mul_ps(v1f,vec_scale);  v2f=_mm256_mul_ps(v2f,vec_scale);
        v5=_mm256_cvtps_epi32(v1f);        v6=_mm256_cvtps_epi32(v2f);
        v5=_mm256_packs_epi32(v5, v6);
        v3f=_mm256_loadu_ps(sp3+16);       v4f=_mm256_loadu_ps(sp3+24);
        v3f=_mm256_mul_ps(v3f,vec_scale);  v4f=_mm256_mul_ps(v4f,vec_scale);
        v6=_mm256_cvtps_epi32(v3f);        vt=_mm256_cvtps_epi32(v4f);
        v6=_mm256_packs_epi32(v6, vt);
        
        // Convert to 3 vectors with v1=16R, v2=16G and v3=16B
        v1=_mm256_adds_epi16(v1,vec_off);  v2=_mm256_adds_epi16(v2,vec_off);
        v1=_mm256_packus_epi16(v1,v2);     v1=_mm256_min_epu8(v1,vec_max);
        v1=_mm256_permutevar8x32_epi32(v1,permd_ctl);
        v3=_mm256_adds_epi16(v3,vec_off);  v4=_mm256_adds_epi16(v4,vec_off);
        v2=_mm256_packus_epi16(v3,v4);     v2=_mm256_min_epu8(v2,vec_max);
        v2=_mm256_permutevar8x32_epi32(v2,permd_ctl);
        v5=_mm256_adds_epi16(v5,vec_off);  v6=_mm256_adds_epi16(v6,vec_off);
        v3=_mm256_packus_epi16(v5,v6);     v3=_mm256_min_epu8(v3,vec_max);
        v3=_mm256_permutevar8x32_epi32(v3,permd_ctl);
        
        // Perform lane-wise colour component interleaving
        v4=_mm256_slli_si256(v1,10); // Keep first 6 red bytes
        v4=_mm256_alignr_epi8(v2,v4,5);    v2=_mm256_srli_si256(v2,5);
        v4=_mm256_alignr_epi8(v3,v4,5);    v3=_mm256_srli_si256(v3,5);
        v4=_mm256_shuffle_epi8(v4,vec_shuffle_556); // [5B | 5G | 6R] per lane
        v5=_mm256_slli_si256(v1,5); // Keep next 5 red bytes
        v5=_mm256_alignr_epi8(v2,v5,6);    v2=_mm256_srli_si256(v2,6);
        v5=_mm256_alignr_epi8(v3,v5,5);    v3=_mm256_srli_si256(v3,5);
        v5=_mm256_shuffle_epi8(v5,vec_shuffle_565); // [5B | 6G | 5R] per lane
        v6=_mm256_alignr_epi8(v2,v1,5);
        v6=_mm256_alignr_epi8(v3,v6,6);
        v6=_mm256_shuffle_epi8(v6,vec_shuffle_655); // [6B | 5G | 5R] per lane
        
        // At this point the low lanes of va,vb,vc together represent the first
        // 48 output bytes, while the high lanes represent the next 48 output
        // bytes.  Use 128-bit permutations to rearrange so that we can store
        // three 256-bit vectors consecutively.
        v1=_mm256_permute2x128_si256(v4,v5,0x20);
        v2=_mm256_permute2x128_si256(v6,v4,0x30);
        v3=_mm256_permute2f128_si256(v5,v6,0x31);
        _mm256_storeu_si256((__m256i *) dp,v1);
        _mm256_storeu_si256((__m256i *)(dp+32),v2);
        _mm256_storeu_si256((__m256i *)(dp+64),v3);
      }
  
  // Backtrack 0 to 31 src samples so we can use vector processing also for the
  // final output samples.  This generally involves unaligned loads and stores.
  int backtrack = ((-width) & 31);
  sp1 -= backtrack;  sp2 -= backtrack;  sp3 -= backtrack;  dp -= 3*backtrack;
  v1f=_mm256_loadu_ps(sp1+0);       v2f=_mm256_loadu_ps(sp1+8);
  v1f=_mm256_mul_ps(v1f,vec_scale); v2f=_mm256_mul_ps(v2f,vec_scale);
  v1=_mm256_cvtps_epi32(v1f);       v2=_mm256_cvtps_epi32(v2f);
  v1=_mm256_packs_epi32(v1, v2);
  v3f=_mm256_loadu_ps(sp1+16);       v4f=_mm256_loadu_ps(sp1+24);
  v3f=_mm256_mul_ps(v3f, vec_scale); v4f=_mm256_mul_ps(v4f,vec_scale);
  v2=_mm256_cvtps_epi32(v3f);        v3=_mm256_cvtps_epi32(v4f);
  v2=_mm256_packs_epi32(v2, v3);
  v1f=_mm256_loadu_ps(sp2+0);        v2f=_mm256_loadu_ps(sp2+8);
  v1f=_mm256_mul_ps(v1f,vec_scale);  v2f=_mm256_mul_ps(v2f,vec_scale);
  v3=_mm256_cvtps_epi32(v1f);        v4=_mm256_cvtps_epi32(v2f);
  v3=_mm256_packs_epi32(v3, v4);
  v3f=_mm256_loadu_ps(sp2+16);       v4f=_mm256_loadu_ps(sp2+24);
  v3f=_mm256_mul_ps(v3f,vec_scale);  v4f=_mm256_mul_ps(v4f,vec_scale);
  v4=_mm256_cvtps_epi32(v3f);        v5=_mm256_cvtps_epi32(v4f);
  v4=_mm256_packs_epi32(v4, v5);
  v1f=_mm256_loadu_ps(sp3+0);        v2f=_mm256_loadu_ps(sp3+8);
  v1f=_mm256_mul_ps(v1f,vec_scale);  v2f=_mm256_mul_ps(v2f,vec_scale);
  v5=_mm256_cvtps_epi32(v1f);        v6=_mm256_cvtps_epi32(v2f);
  v5=_mm256_packs_epi32(v5, v6);
  v3f=_mm256_loadu_ps(sp3+16);       v4f=_mm256_loadu_ps(sp3+24);
  v3f=_mm256_mul_ps(v3f,vec_scale);  v4f=_mm256_mul_ps(v4f,vec_scale);
  v6=_mm256_cvtps_epi32(v3f);        vt=_mm256_cvtps_epi32(v4f);
  v6=_mm256_packs_epi32(v6, vt);
  v1=_mm256_adds_epi16(v1,vec_off);  v2=_mm256_adds_epi16(v2,vec_off);
  v1=_mm256_packus_epi16(v1,v2);     v1=_mm256_min_epu8(v1,vec_max);
  v1=_mm256_permutevar8x32_epi32(v1,permd_ctl);
  v3=_mm256_adds_epi16(v3,vec_off);  v4=_mm256_adds_epi16(v4,vec_off);
  v2=_mm256_packus_epi16(v3,v4);     v2=_mm256_min_epu8(v2,vec_max);
  v2=_mm256_permutevar8x32_epi32(v2,permd_ctl);
  v5=_mm256_adds_epi16(v5,vec_off);  v6=_mm256_adds_epi16(v6,vec_off);
  v3=_mm256_packus_epi16(v5,v6);     v3=_mm256_min_epu8(v3,vec_max);
  v3=_mm256_permutevar8x32_epi32(v3,permd_ctl);
  v4=_mm256_slli_si256(v1,10); // Keep first 6 red bytes
  v4=_mm256_alignr_epi8(v2,v4,5);    v2=_mm256_srli_si256(v2,5);
  v4=_mm256_alignr_epi8(v3,v4,5);    v3=_mm256_srli_si256(v3,5);
  v4=_mm256_shuffle_epi8(v4,vec_shuffle_556);
  v5=_mm256_slli_si256(v1,5); // Keep next 5 red bytes
  v5=_mm256_alignr_epi8(v2,v5,6);    v2=_mm256_srli_si256(v2,6);
  v5=_mm256_alignr_epi8(v3,v5,5);    v3=_mm256_srli_si256(v3,5);
  v5=_mm256_shuffle_epi8(v5,vec_shuffle_565);
  v6=_mm256_alignr_epi8(v2,v1,5);
  v6=_mm256_alignr_epi8(v3,v6,6);
  v6=_mm256_shuffle_epi8(v6,vec_shuffle_655);
  v1=_mm256_permute2x128_si256(v4,v5,0x20);
  v2=_mm256_permute2x128_si256(v6,v4,0x30);
  v3=_mm256_permute2f128_si256(v5,v6,0x31);
  _mm256_storeu_si256((__m256i *) dp,v1);
  _mm256_storeu_si256((__m256i *)(dp+32),v2);
  _mm256_storeu_si256((__m256i *)(dp+64),v3);

  _mm_setcsr(mxcsr_orig); // Restore rounding control bits
}

/*****************************************************************************/
/* EXTERN                avx2_floats_to_uint8_ilv4                           */
/*****************************************************************************/

void avx2_floats_to_uint8_ilv4(kdu_byte *dst, float **src, int width,
                               int precision, int orig_precision,
                               bool is_absolute, bool dst_signed,
                               int preferences)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(!dst_signed); // Function prototype generic; we don't do signed bytes
  float scale = (float)(1<<precision);
  int offset_val = (1 << (precision-1));
  int post_max_val = (1<<precision) - 1; // Max value after conversion

  int mxcsr_orig = _mm_getcsr();
  int mxcsr_cur = mxcsr_orig & ~(3<<13); // Reset rounding control bits
  _mm_setcsr(mxcsr_cur);

  __m256i byte_shuffle = _mm256_loadu_si256((__m256i *) kd_shuffle_r4444);
  __m256 vec_scale = _mm256_set1_ps(scale);
  __m256i vec_off = _mm256_set1_epi16((kdu_int16)offset_val);
  __m256i vec_max = _mm256_set1_epi8((kdu_byte)post_max_val);
  __m256 v1f, v2f, v3f, v4f;
  __m256i v1, v2, v3, v4;

  // Generate all but the last 1 to 8 output quartets
  kdu_byte *dp = dst;
  float *sp1=src[0], *sp2=src[1], *sp3=src[2], *sp4=src[3];
  if ((preferences & KDU_STRIPE_STORE_PREF_STREAMING) &&
      !(_addr_to_kdu_int32(dst) & 3))
    { // Want streaming aligned stores; may need to reposition pointers; see
      // the non-streaming loop below for explanatory comments, that have
      // been eliminated from the other similar copies.
      int advance = (32 - (_addr_to_kdu_int32(dst) & 31)) >> 2;
      v1f=_mm256_loadu_ps(sp1);          v2f=_mm256_loadu_ps(sp2);
      v3f=_mm256_loadu_ps(sp3);          v4f=_mm256_loadu_ps(sp4);
      v1f=_mm256_mul_ps(v1f,vec_scale);  v2f=_mm256_mul_ps(v2f,vec_scale);
      v3f=_mm256_mul_ps(v3f,vec_scale);  v4f=_mm256_mul_ps(v4f,vec_scale);
      v1=_mm256_cvtps_epi32(v1f);        v2=_mm256_cvtps_epi32(v2f);
      v3=_mm256_cvtps_epi32(v3f);        v4=_mm256_cvtps_epi32(v4f);
      v1=_mm256_packs_epi32(v1,v2);      v2=_mm256_packs_epi32(v3,v4);
      v1=_mm256_adds_epi16(v1,vec_off);  v2=_mm256_adds_epi16(v2,vec_off);
      v1=_mm256_packus_epi16(v1,v2);     v1=_mm256_min_epu8(v1,vec_max);
      v1 = _mm256_shuffle_epi8(v1, byte_shuffle);
      _mm256_storeu_si256((__m256i *) dp, v1);

      width -= advance;  dp += advance;
      sp1 += advance;  sp2 += advance;  sp3 += advance;  sp4 += advance;
      for (; width > 8; width-=8, sp1+=8, sp2+=8, sp3+=8, sp4+=8, dp+=32)
        { 
          v1f=_mm256_loadu_ps(sp1);          v2f=_mm256_loadu_ps(sp2);
          v3f=_mm256_loadu_ps(sp3);          v4f=_mm256_loadu_ps(sp4);
          v1f=_mm256_mul_ps(v1f,vec_scale);  v2f=_mm256_mul_ps(v2f,vec_scale);
          v3f=_mm256_mul_ps(v3f,vec_scale);  v4f=_mm256_mul_ps(v4f,vec_scale);
          v1=_mm256_cvtps_epi32(v1f);        v2=_mm256_cvtps_epi32(v2f);
          v3=_mm256_cvtps_epi32(v3f);        v4=_mm256_cvtps_epi32(v4f);
          v1=_mm256_packs_epi32(v1,v2);      v2=_mm256_packs_epi32(v3,v4);
          v1=_mm256_adds_epi16(v1,vec_off);  v2=_mm256_adds_epi16(v2,vec_off);
          v1=_mm256_packus_epi16(v1,v2);     v1=_mm256_min_epu8(v1,vec_max);
          v1 = _mm256_shuffle_epi8(v1, byte_shuffle);
          _mm256_stream_si256((__m256i *) dp, v1);
        }
    }
  else
    for (; width > 8; width-=8, sp1+=8, sp2+=8, sp3+=8, sp4+=8, dp+=32)
      { 
        // Load R, G, B and A vectors, each with 8 samples, scaling and
        // converting to dwords and packing into two vectors of words
        v1f=_mm256_loadu_ps(sp1);          v2f=_mm256_loadu_ps(sp2);
        v3f=_mm256_loadu_ps(sp3);          v4f=_mm256_loadu_ps(sp4);
        v1f=_mm256_mul_ps(v1f,vec_scale);  v2f=_mm256_mul_ps(v2f,vec_scale);
        v3f=_mm256_mul_ps(v3f,vec_scale);  v4f=_mm256_mul_ps(v4f,vec_scale);
        v1=_mm256_cvtps_epi32(v1f);        v2=_mm256_cvtps_epi32(v2f);
        v3=_mm256_cvtps_epi32(v3f);        v4=_mm256_cvtps_epi32(v4f);
        v1=_mm256_packs_epi32(v1,v2);      v2=_mm256_packs_epi32(v3,v4);
        
        // We now have v1 = [4G 4R | 4G 4R]  and v2 = [4A 4B | 4A 4B] as words,
        // from MSB to LSB.  Perform 16 bit adjustments, pack to bytes and
        // limit the byte results.
        v1=_mm256_adds_epi16(v1,vec_off);  v2=_mm256_adds_epi16(v2,vec_off);
        v1=_mm256_packus_epi16(v1,v2);     v1=_mm256_min_epu8(v1,vec_max);
        
        // We now have v1 = [4A 4B 4G 4R | 4A 4B 4G 4R] as byes from MSB to LSB.
        // Interleave the ABGR bytes within each lane and store the result
        v1 = _mm256_shuffle_epi8(v1, byte_shuffle);
        _mm256_storeu_si256((__m256i *) dp, v1);
      }
  
  // Backtrack 0 to 7 src samples so we can use vector processing also for the
  // final output samples.  This generally involves unaligned loads and stores.
  int backtrack = ((-width) & 7);
  sp1 -= backtrack;  sp2 -= backtrack;  sp3 -= backtrack;  sp4 -= backtrack;
  dp -= 4*backtrack;
  v1f=_mm256_loadu_ps(sp1);          v2f=_mm256_loadu_ps(sp2);
  v3f=_mm256_loadu_ps(sp3);          v4f=_mm256_loadu_ps(sp4);
  v1f=_mm256_mul_ps(v1f,vec_scale);  v2f=_mm256_mul_ps(v2f,vec_scale);
  v3f=_mm256_mul_ps(v3f,vec_scale);  v4f=_mm256_mul_ps(v4f,vec_scale);
  v1=_mm256_cvtps_epi32(v1f);        v2=_mm256_cvtps_epi32(v2f);
  v3=_mm256_cvtps_epi32(v3f);        v4=_mm256_cvtps_epi32(v4f);
  v1=_mm256_packs_epi32(v1,v2);      v2=_mm256_packs_epi32(v3,v4);
  v1=_mm256_adds_epi16(v1,vec_off);  v2=_mm256_adds_epi16(v2,vec_off);
  v1=_mm256_packus_epi16(v1,v2);     v1=_mm256_min_epu8(v1,vec_max);
  v1 = _mm256_shuffle_epi8(v1, byte_shuffle);
  _mm256_storeu_si256((__m256i *) dp, v1);
  
  _mm_setcsr(mxcsr_orig); // Restore rounding control bits
}

/*****************************************************************************/
/* EXTERN                 avx2_int16_to_int16_ilv1                           */
/*****************************************************************************/

void avx2_int16_to_int16_ilv1(kdu_int16 *dst, kdu_int16 **src, int width,
                              int precision, int orig_precision,
                              bool is_absolute, bool dst_signed,
                              int preferences)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  if (!is_absolute)
    orig_precision = KDU_FIX_POINT;
  assert(orig_precision < 16);
    // NB: orig_precision cannot be 16 or the saturating downshift below may
    // cause truncation of valid values.
  int upshift=0, downshift=orig_precision-precision;
  int offset_val=0; // Offset is applied first, then downshift
  int pre_shift_min = -(1<<(orig_precision-1));
  int pre_shift_max = -(1+pre_shift_min);
  if (downshift <= 0)
    { 
      upshift = -downshift;
      downshift = 0;
    }
  else
    { 
      offset_val = 1 << (downshift-1); // Pre-shift rounding offset
      pre_shift_max -= offset_val;
    }
  if (!dst_signed)
    offset_val += 1 << (orig_precision-1); // Can saturate without overflow

  __m256i v_off = _mm256_set1_epi16((kdu_int16)offset_val);
  __m128i v_right = _mm_cvtsi32_si128(downshift);
  __m128i v_left = _mm_cvtsi32_si128(upshift);
  __m256i v_min = _mm256_set1_epi16((kdu_int16)pre_shift_min);
  __m256i v_max = _mm256_set1_epi16((kdu_int16)pre_shift_max);
  __m256i v1, v2;

  // Generate all but the last 1 to 32 outputs
  kdu_int16 *dp = dst;
  kdu_int16 *sp = src[0];
  if ((preferences & KDU_STRIPE_STORE_PREF_STREAMING) &&
      !(_addr_to_kdu_int32(dst) & 1))
    { // Process first vector and then backtrack to align streaming stores
      int advance = (64 - (_addr_to_kdu_int32(dst) & 31))>>1;
      v1=_mm256_loadu_si256((__m256i *)(sp+0));
      v2=_mm256_loadu_si256((__m256i *)(sp+16));
      v1=_mm256_max_epi16(v1,v_min);   v2=_mm256_max_epi16(v2,v_min);
      v1=_mm256_min_epi16(v1,v_max);   v2=_mm256_min_epi16(v2,v_max);
      v1=_mm256_adds_epi16(v1,v_off);  v2=_mm256_adds_epi16(v2,v_off);
      v1=_mm256_sra_epi16(v1,v_right); v2=_mm256_sra_epi16(v2,v_right);
      v1=_mm256_sll_epi16(v1,v_left);  v2=_mm256_sll_epi16(v2,v_left);
      _mm256_storeu_si256((__m256i *) dp,v1);
      _mm256_storeu_si256((__m256i *)(dp+16),v2);  
      
      width -= advance;  sp += advance;  dp += advance;
      assert(!(_addr_to_kdu_int32(dp) & 31));
      if (downshift == 0)
        { // Do upshift only 
          for (; width > 32; width-=32, sp+=32, dp+=32)
            { 
              v1=_mm256_loadu_si256((__m256i *)(sp+0));
              v2=_mm256_loadu_si256((__m256i *)(sp+16));
              v1=_mm256_max_epi16(v1,v_min);   v2=_mm256_max_epi16(v2,v_min);
              v1=_mm256_min_epi16(v1,v_max);   v2=_mm256_min_epi16(v2,v_max);
              v1=_mm256_adds_epi16(v1,v_off);  v2=_mm256_adds_epi16(v2,v_off);
              v1=_mm256_sll_epi16(v1,v_left);  v2=_mm256_sll_epi16(v2,v_left);
              _mm256_stream_si256((__m256i *) dp,v1);
              _mm256_stream_si256((__m256i *)(dp+16),v2);
            }          
        }
      else
        { // Do downshift only
          assert(upshift == 0);
          for (; width > 32; width-=32, sp+=32, dp+=32)
            { 
              v1=_mm256_loadu_si256((__m256i *)(sp+0));
              v2=_mm256_loadu_si256((__m256i *)(sp+16));
              v1=_mm256_max_epi16(v1,v_min);   v2=_mm256_max_epi16(v2,v_min);
              v1=_mm256_min_epi16(v1,v_max);   v2=_mm256_min_epi16(v2,v_max);
              v1=_mm256_adds_epi16(v1,v_off);  v2=_mm256_adds_epi16(v2,v_off);
              v1=_mm256_sra_epi16(v1,v_right); v2=_mm256_sra_epi16(v2,v_right);
              _mm256_stream_si256((__m256i *) dp,v1);
              _mm256_stream_si256((__m256i *)(dp+16),v2);
            }
        }
    }
  else
    { // Regular processing with (potentially) unaligned stores
      if (downshift == 0)
        { // Do upshift only 
          for (; width > 32; width-=32, sp+=32, dp+=32)
            { 
              v1=_mm256_loadu_si256((__m256i *)(sp+0));
              v2=_mm256_loadu_si256((__m256i *)(sp+16));
              v1=_mm256_max_epi16(v1,v_min);   v2=_mm256_max_epi16(v2,v_min);
              v1=_mm256_min_epi16(v1,v_max);   v2=_mm256_min_epi16(v2,v_max);
              v1=_mm256_adds_epi16(v1,v_off);  v2=_mm256_adds_epi16(v2,v_off);
              v1=_mm256_sll_epi16(v1,v_left);  v2=_mm256_sll_epi16(v2,v_left);
              _mm256_storeu_si256((__m256i *) dp,v1);
              _mm256_storeu_si256((__m256i *)(dp+16),v2);
            }
        }
      else
        { // Do downshift only
          assert(upshift == 0);
          for (; width > 32; width-=32, sp+=32, dp+=32)
            { 
              v1=_mm256_loadu_si256((__m256i *)(sp+0));
              v2=_mm256_loadu_si256((__m256i *)(sp+16));
              v1=_mm256_max_epi16(v1,v_min);   v2=_mm256_max_epi16(v2,v_min);
              v1=_mm256_min_epi16(v1,v_max);   v2=_mm256_min_epi16(v2,v_max);
              v1=_mm256_adds_epi16(v1,v_off);  v2=_mm256_adds_epi16(v2,v_off);
              v1=_mm256_sra_epi16(v1,v_right); v2=_mm256_sra_epi16(v2,v_right);
              _mm256_storeu_si256((__m256i *) dp,v1);
              _mm256_storeu_si256((__m256i *)(dp+16),v2);
            }
        }
    }
  
  // Backtrack 0 to 31 positions so we can use vector processing also for the
  // final output samples.  This generally involves unaligned loads and stores.
  int backtrack = ((-width) & 31);
  sp -= backtrack; dp -= backtrack;
  v1=_mm256_loadu_si256((__m256i *)(sp+0));
  v2=_mm256_loadu_si256((__m256i *)(sp+16));
  v1=_mm256_max_epi16(v1,v_min);   v2=_mm256_max_epi16(v2,v_min);
  v1=_mm256_min_epi16(v1,v_max);   v2=_mm256_min_epi16(v2,v_max);
  v1=_mm256_adds_epi16(v1,v_off);  v2=_mm256_adds_epi16(v2,v_off);
  v1=_mm256_sra_epi16(v1,v_right); v2=_mm256_sra_epi16(v2,v_right);
  v1=_mm256_sll_epi16(v1,v_left);  v2=_mm256_sll_epi16(v2,v_left);
  _mm256_storeu_si256((__m256i *) dp,v1);
  _mm256_storeu_si256((__m256i *)(dp+16),v2);  
}

/*****************************************************************************/
/* EXTERN               avx2_int32_to_int16_rs_ilv1                          */
/*****************************************************************************/

void avx2_int32_to_int16_rs_ilv1(kdu_int16 *dst, kdu_int32 **src, int width,
                                 int precision, int orig_precision,
                                 bool is_absolute, bool dst_signed,
                                 int preferences)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(is_absolute);
  assert(orig_precision >= precision);
  int downshift = orig_precision - precision;
  int pre_offset_val = (1 << downshift) >> 1;
  int min_val = -(1 << (precision-1));
  int max_val = -(1+min_val);
  int post_offset_val = (dst_signed)?0:(1<<(precision-1));

  __m256i vec_preoff = _mm256_set1_epi32(pre_offset_val);
  __m128i vec_shift = _mm_cvtsi32_si128(downshift);
  __m256i vec_postoff = _mm256_set1_epi16((kdu_int16)post_offset_val);
  __m256i vec_min = _mm256_set1_epi16((kdu_int16)min_val);
  __m256i vec_max = _mm256_set1_epi16((kdu_int16)max_val);
  __m256i v1, v2, v3, v4;

  // Generate all but the last 1 to 32 outputs
  kdu_int16 *dp = dst;
  kdu_int32 *sp = src[0];
  if ((preferences & KDU_STRIPE_STORE_PREF_STREAMING) &&
      !(_addr_to_kdu_int32(dst) & 1))
    { // Want streaming aligned stores; may need to reposition pointers; see
      // the non-streaming loop below for explanatory comments, that have
      // been eliminated from the other similar copies.
      int advance = (64 - (_addr_to_kdu_int32(dst) & 31))>>1;
      v1=_mm256_loadu_si256((__m256i *)(sp+0));
      v2=_mm256_loadu_si256((__m256i *)(sp+8));
      v3=_mm256_loadu_si256((__m256i *)(sp+16));
      v4=_mm256_loadu_si256((__m256i *)(sp+24));
      v1=_mm256_add_epi32(v1,vec_preoff);
      v2=_mm256_add_epi32(v2,vec_preoff);
      v3=_mm256_add_epi32(v3,vec_preoff);
      v4=_mm256_add_epi32(v4,vec_preoff);
      v1=_mm256_sra_epi32(v1,vec_shift);
      v2=_mm256_sra_epi32(v2,vec_shift);
      v3=_mm256_sra_epi32(v3,vec_shift);
      v4=_mm256_sra_epi32(v4,vec_shift);
      v1=_mm256_packs_epi32(v1,v2);        v2=_mm256_packs_epi32(v3,v4);
      v1=_mm256_max_epi16(v1,vec_min);     v2=_mm256_max_epi16(v2,vec_min);
      v1=_mm256_min_epi16(v1,vec_max);     v2=_mm256_min_epi16(v2,vec_max);
      v1=_mm256_add_epi16(v1,vec_postoff);
      v2=_mm256_add_epi16(v2,vec_postoff);
      v1=_mm256_permute4x64_epi64(v1,0xD8);
      v2=_mm256_permute4x64_epi64(v2,0xD8);
      _mm256_storeu_si256((__m256i *)(dp+0),v1);
      _mm256_storeu_si256((__m256i *)(dp+16),v2);
      
      width -= advance;  sp += advance;  dp += advance;
      assert(!(_addr_to_kdu_int32(dp) & 31));
      for (; width > 32; width-=32, sp+=32, dp+=32)
        { 
          v1=_mm256_loadu_si256((__m256i *)(sp+0));
          v2=_mm256_loadu_si256((__m256i *)(sp+8));
          v3=_mm256_loadu_si256((__m256i *)(sp+16));
          v4=_mm256_loadu_si256((__m256i *)(sp+24));
          v1=_mm256_add_epi32(v1,vec_preoff);
          v2=_mm256_add_epi32(v2,vec_preoff);
          v3=_mm256_add_epi32(v3,vec_preoff);
          v4=_mm256_add_epi32(v4,vec_preoff);
          v1=_mm256_sra_epi32(v1,vec_shift);
          v2=_mm256_sra_epi32(v2,vec_shift);
          v3=_mm256_sra_epi32(v3,vec_shift);
          v4=_mm256_sra_epi32(v4,vec_shift);
          v1=_mm256_packs_epi32(v1,v2);        v2=_mm256_packs_epi32(v3,v4);
          v1=_mm256_max_epi16(v1,vec_min);     v2=_mm256_max_epi16(v2,vec_min);
          v1=_mm256_min_epi16(v1,vec_max);     v2=_mm256_min_epi16(v2,vec_max);
          v1=_mm256_add_epi16(v1,vec_postoff);
          v2=_mm256_add_epi16(v2,vec_postoff);
          v1=_mm256_permute4x64_epi64(v1,0xD8);
          v2=_mm256_permute4x64_epi64(v2,0xD8);
          _mm256_stream_si256((__m256i *)(dp+0),v1);
          _mm256_stream_si256((__m256i *)(dp+16),v2);
        }
    }
  else
    for (; width > 32; width-=32, sp+=32, dp+=32)
      { 
        v1=_mm256_loadu_si256((__m256i *)(sp+0));
        v2=_mm256_loadu_si256((__m256i *)(sp+8));
        v3=_mm256_loadu_si256((__m256i *)(sp+16));
        v4=_mm256_loadu_si256((__m256i *)(sp+24));
        v1=_mm256_add_epi32(v1,vec_preoff);
        v2=_mm256_add_epi32(v2,vec_preoff);
        v3=_mm256_add_epi32(v3,vec_preoff);
        v4=_mm256_add_epi32(v4,vec_preoff);
        v1=_mm256_sra_epi32(v1,vec_shift);
        v2=_mm256_sra_epi32(v2,vec_shift);
        v3=_mm256_sra_epi32(v3,vec_shift);
        v4=_mm256_sra_epi32(v4,vec_shift);
        v1=_mm256_packs_epi32(v1,v2);        v2=_mm256_packs_epi32(v3,v4);
        v1=_mm256_max_epi16(v1,vec_min);     v2=_mm256_max_epi16(v2,vec_min);
        v1=_mm256_min_epi16(v1,vec_max);     v2=_mm256_min_epi16(v2,vec_max);
        v1=_mm256_add_epi16(v1,vec_postoff);
        v2=_mm256_add_epi16(v2,vec_postoff);
        
        // We now have v1=[ 15 14 13 12  7 6 5 4 | 11 10 9 8  3 2 1 0] as words
        // with v2 having the same organization with an offset of 16 in the
        // represented words.  Need to rearrange quadwords within each of v1
        // and v2 before storing
        v1=_mm256_permute4x64_epi64(v1,0xD8);
        v2=_mm256_permute4x64_epi64(v2,0xD8);
        _mm256_storeu_si256((__m256i *)(dp+0),v1);
        _mm256_storeu_si256((__m256i *)(dp+16),v2);
      }

  // Backtrack 0 to 31 positions so we can use vector processing also for the
  // final output samples.  This generally involves unaligned loads and stores.
  int backtrack = ((-width) & 31);
  sp -= backtrack; dp -= backtrack;
  v1=_mm256_loadu_si256((__m256i *)(sp+0));
  v2=_mm256_loadu_si256((__m256i *)(sp+8));
  v3=_mm256_loadu_si256((__m256i *)(sp+16));
  v4=_mm256_loadu_si256((__m256i *)(sp+24));
  v1=_mm256_add_epi32(v1,vec_preoff);
  v2=_mm256_add_epi32(v2,vec_preoff);
  v3=_mm256_add_epi32(v3,vec_preoff);
  v4=_mm256_add_epi32(v4,vec_preoff);
  v1=_mm256_sra_epi32(v1,vec_shift);
  v2=_mm256_sra_epi32(v2,vec_shift);
  v3=_mm256_sra_epi32(v3,vec_shift);
  v4=_mm256_sra_epi32(v4,vec_shift);
  v1=_mm256_packs_epi32(v1,v2);        v2=_mm256_packs_epi32(v3,v4);
  v1=_mm256_max_epi16(v1,vec_min);     v2=_mm256_max_epi16(v2,vec_min);
  v1=_mm256_min_epi16(v1,vec_max);     v2=_mm256_min_epi16(v2,vec_max);
  v1=_mm256_add_epi16(v1,vec_postoff);
  v2=_mm256_add_epi16(v2,vec_postoff);
  v1=_mm256_permute4x64_epi64(v1,0xD8);
  v2=_mm256_permute4x64_epi64(v2,0xD8);
  _mm256_storeu_si256((__m256i *)(dp+0),v1);
  _mm256_storeu_si256((__m256i *)(dp+16),v2);
}

/*****************************************************************************/
/* EXTERN                 avx2_floats_to_int16_ilv1                          */
/*****************************************************************************/

void avx2_floats_to_int16_ilv1(kdu_int16 *dst, float **src, int width,
                               int precision, int orig_precision,
                               bool is_absolute, bool dst_signed,
                               int preferences)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(!is_absolute);
  float scale = (float)(1<<precision);
  int min_val = -(1 << (precision-1));
  int max_val = -(1+min_val);
  int post_offset_val = (dst_signed)?0:(1<<(precision-1));

  int mxcsr_orig = _mm_getcsr();
  int mxcsr_cur = mxcsr_orig & ~(3<<13); // Reset rounding control bits
  _mm_setcsr(mxcsr_cur);

  __m256 vec_scale = _mm256_set1_ps(scale);
  __m256i vec_postoff = _mm256_set1_epi16((kdu_int16)post_offset_val);;
  __m256i vec_min = _mm256_set1_epi16((kdu_int16)min_val);
  __m256i vec_max = _mm256_set1_epi16((kdu_int16)max_val);
  __m256 v1f, v2f, v3f, v4f;
  __m256i v1, v2, v3, v4;

  // Generate all but the last 1 to 32 outputs
  kdu_int16 *dp = dst;
  float *sp = src[0];
  if ((preferences & KDU_STRIPE_STORE_PREF_STREAMING) &&
      !(_addr_to_kdu_int32(dst) & 1))
    { // Want streaming aligned stores; may need to reposition pointers; see
      // the non-streaming loop below for explanatory comments, that have
      // been eliminated from the other similar copies.
      int advance = (64 - (_addr_to_kdu_int32(dst) & 31))>>1;
      v1f=_mm256_loadu_ps(sp+0);          v2f=_mm256_loadu_ps(sp+8);
      v3f=_mm256_loadu_ps(sp+16);         v4f=_mm256_loadu_ps(sp+24);
      v1f=_mm256_mul_ps(v1f,vec_scale);   v2f=_mm256_mul_ps(v2f,vec_scale);
      v3f=_mm256_mul_ps(v3f,vec_scale);   v4f=_mm256_mul_ps(v4f,vec_scale);
      v1=_mm256_cvtps_epi32(v1f);         v2=_mm256_cvtps_epi32(v2f);
      v3=_mm256_cvtps_epi32(v3f);         v4=_mm256_cvtps_epi32(v4f);
      v1=_mm256_packs_epi32(v1,v2);       v2=_mm256_packs_epi32(v3,v4);
      v1=_mm256_max_epi16(v1,vec_min);    v2=_mm256_max_epi16(v2,vec_min);
      v1=_mm256_min_epi16(v1,vec_max);    v2=_mm256_min_epi16(v2,vec_max);
      v1=_mm256_add_epi16(v1,vec_postoff);
      v2=_mm256_add_epi16(v2,vec_postoff);
      v1=_mm256_permute4x64_epi64(v1,0xD8);
      v2=_mm256_permute4x64_epi64(v2,0xD8);
      _mm256_storeu_si256((__m256i *)(dp+0),v1);
      _mm256_storeu_si256((__m256i *)(dp+16),v2);      
      
      width -= advance;  sp += advance;  dp += advance;
      assert(!(_addr_to_kdu_int32(dp) & 31));
      for (; width > 32; width-=32, sp+=32, dp+=32)
        { 
          v1f=_mm256_loadu_ps(sp+0);          v2f=_mm256_loadu_ps(sp+8);
          v3f=_mm256_loadu_ps(sp+16);         v4f=_mm256_loadu_ps(sp+24);
          v1f=_mm256_mul_ps(v1f,vec_scale);   v2f=_mm256_mul_ps(v2f,vec_scale);
          v3f=_mm256_mul_ps(v3f,vec_scale);   v4f=_mm256_mul_ps(v4f,vec_scale);
          v1=_mm256_cvtps_epi32(v1f);         v2=_mm256_cvtps_epi32(v2f);
          v3=_mm256_cvtps_epi32(v3f);         v4=_mm256_cvtps_epi32(v4f);
          v1=_mm256_packs_epi32(v1,v2);       v2=_mm256_packs_epi32(v3,v4);
          v1=_mm256_max_epi16(v1,vec_min);    v2=_mm256_max_epi16(v2,vec_min);
          v1=_mm256_min_epi16(v1,vec_max);    v2=_mm256_min_epi16(v2,vec_max);
          v1=_mm256_add_epi16(v1,vec_postoff);
          v2=_mm256_add_epi16(v2,vec_postoff);
          v1=_mm256_permute4x64_epi64(v1,0xD8);
          v2=_mm256_permute4x64_epi64(v2,0xD8);
          _mm256_stream_si256((__m256i *)(dp+0),v1);
          _mm256_stream_si256((__m256i *)(dp+16),v2);
        }      
    }
  else
    for (; width > 32; width-=32, sp+=32, dp+=32)
      { 
        v1f=_mm256_loadu_ps(sp+0);          v2f=_mm256_loadu_ps(sp+8);
        v3f=_mm256_loadu_ps(sp+16);         v4f=_mm256_loadu_ps(sp+24);
        v1f=_mm256_mul_ps(v1f,vec_scale);   v2f=_mm256_mul_ps(v2f,vec_scale);
        v3f=_mm256_mul_ps(v3f,vec_scale);   v4f=_mm256_mul_ps(v4f,vec_scale);
        v1=_mm256_cvtps_epi32(v1f);         v2=_mm256_cvtps_epi32(v2f);
        v3=_mm256_cvtps_epi32(v3f);         v4=_mm256_cvtps_epi32(v4f);
        v1=_mm256_packs_epi32(v1,v2);       v2=_mm256_packs_epi32(v3,v4);
        v1=_mm256_max_epi16(v1,vec_min);    v2=_mm256_max_epi16(v2,vec_min);
        v1=_mm256_min_epi16(v1,vec_max);    v2=_mm256_min_epi16(v2,vec_max);
        v1=_mm256_add_epi16(v1,vec_postoff);
        v2=_mm256_add_epi16(v2,vec_postoff);
        
        // We now have v1=[ 15 14 13 12  7 6 5 4 | 11 10 9 8  3 2 1 0] as words
        // with v2 having the same organization with an offset of 16 in the
        // represented words.  Need to rearrange quadwords within each of v1
        // and v2 before storing
        v1=_mm256_permute4x64_epi64(v1,0xD8);
        v2=_mm256_permute4x64_epi64(v2,0xD8);
        _mm256_storeu_si256((__m256i *)(dp+0),v1);
        _mm256_storeu_si256((__m256i *)(dp+16),v2);
      }

  // Backtrack 0 to 31 positions so we can use vector processing also for the
  // final output samples.  This generally involves unaligned loads and stores.
  int backtrack = ((-width) & 31);
  sp -= backtrack; dp -= backtrack;
  v1f=_mm256_loadu_ps(sp+0);          v2f=_mm256_loadu_ps(sp+8);
  v3f=_mm256_loadu_ps(sp+16);         v4f=_mm256_loadu_ps(sp+24);
  v1f=_mm256_mul_ps(v1f,vec_scale);   v2f=_mm256_mul_ps(v2f,vec_scale);
  v3f=_mm256_mul_ps(v3f,vec_scale);   v4f=_mm256_mul_ps(v4f,vec_scale);
  v1=_mm256_cvtps_epi32(v1f);         v2=_mm256_cvtps_epi32(v2f);
  v3=_mm256_cvtps_epi32(v3f);         v4=_mm256_cvtps_epi32(v4f);
  v1=_mm256_packs_epi32(v1,v2);       v2=_mm256_packs_epi32(v3,v4);
  v1=_mm256_max_epi16(v1,vec_min);    v2=_mm256_max_epi16(v2,vec_min);
  v1=_mm256_min_epi16(v1,vec_max);    v2=_mm256_min_epi16(v2,vec_max);
  v1=_mm256_add_epi16(v1,vec_postoff);
  v2=_mm256_add_epi16(v2,vec_postoff);
  v1=_mm256_permute4x64_epi64(v1,0xD8);
  v2=_mm256_permute4x64_epi64(v2,0xD8);
  _mm256_storeu_si256((__m256i *)(dp+0),v1);
  _mm256_storeu_si256((__m256i *)(dp+16),v2);      
  
  _mm_setcsr(mxcsr_orig); // Restore rounding control bits
}

/*****************************************************************************/
/* EXTERN                 avx2_floats_to_floats_ilv1                         */
/*****************************************************************************/

void avx2_floats_to_floats_ilv1(float *dst, float **src, int width,
                                int precision, int orig_precision,
                                bool is_absolute, bool dst_signed,
                                int preferences)
{
  assert(width > 32); // Guaranteed by macros in "x86_stripe_transfer_local.h"
  assert(!is_absolute);
  float scale = 1.0F; // Amount to scale from unit range to dst
  while (precision < 0)
    { precision += 16; scale *= 1.0F/(float)(1<<16); }
  while (precision > 16)
    { precision -= 16; scale *= (float)(1<<16); }
  scale *= (float)(1<<precision);
  float offset = (dst_signed)?0.0F:(0.5F*scale);

  __m256 vec_scale = _mm256_set1_ps(scale);
  __m256 vec_off = _mm256_set1_ps(offset);
  __m256 v1f, v2f, v3f, v4f;
  
  // Generate all but the last 1 to 32 outputs
  float *dp = dst;
  float *sp = src[0];
  if ((preferences & KDU_STRIPE_STORE_PREF_STREAMING) &&
      !(_addr_to_kdu_int32(dst) & 3))
    { // Process first vector and then backtrack to align streaming stores
      int advance = (128 - (_addr_to_kdu_int32(dst) & 31))>>2;
      v1f=_mm256_loadu_ps(sp+0);          v2f=_mm256_loadu_ps(sp+8);
      v3f=_mm256_loadu_ps(sp+16);         v4f=_mm256_loadu_ps(sp+24);
      v1f=_mm256_fmadd_ps(v1f,vec_scale,vec_off);
      v2f=_mm256_fmadd_ps(v2f,vec_scale,vec_off);
      v3f=_mm256_fmadd_ps(v3f,vec_scale,vec_off);
      v4f=_mm256_fmadd_ps(v4f,vec_scale,vec_off);
      _mm256_storeu_ps((dp+0),v1f);      _mm256_storeu_ps((dp+8),v2f);
      _mm256_storeu_ps((dp+16),v3f);     _mm256_storeu_ps((dp+24),v4f);
  
      width -= advance;  sp += advance;  dp += advance;
      assert(!(_addr_to_kdu_int32(dp) & 31));
      for (; width > 32; width-=32, sp+=32, dp+=32)
        { 
          v1f=_mm256_loadu_ps(sp+0);          v2f=_mm256_loadu_ps(sp+8);
          v3f=_mm256_loadu_ps(sp+16);         v4f=_mm256_loadu_ps(sp+24);
          v1f=_mm256_fmadd_ps(v1f,vec_scale,vec_off);
          v2f=_mm256_fmadd_ps(v2f,vec_scale,vec_off);
          v3f=_mm256_fmadd_ps(v3f,vec_scale,vec_off);
          v4f=_mm256_fmadd_ps(v4f,vec_scale,vec_off);
          _mm256_stream_ps((dp+0),v1f);      _mm256_stream_ps((dp+8),v2f);
          _mm256_stream_ps((dp+16),v3f);     _mm256_stream_ps((dp+24),v4f);
        }
    }
  else
    for (; width > 32; width-=32, sp+=32, dp+=32)
      { 
        v1f=_mm256_loadu_ps(sp+0);          v2f=_mm256_loadu_ps(sp+8);
        v3f=_mm256_loadu_ps(sp+16);         v4f=_mm256_loadu_ps(sp+24);
        v1f=_mm256_fmadd_ps(v1f,vec_scale,vec_off);
        v2f=_mm256_fmadd_ps(v2f,vec_scale,vec_off);
        v3f=_mm256_fmadd_ps(v3f,vec_scale,vec_off);
        v4f=_mm256_fmadd_ps(v4f,vec_scale,vec_off);
        _mm256_storeu_ps((dp+0),v1f);      _mm256_storeu_ps((dp+8),v2f);
        _mm256_storeu_ps((dp+16),v3f);     _mm256_storeu_ps((dp+24),v4f);
      }

  // Backtrack 0 to 31 positions so we can use vector processing also for the
  // final output samples.  This generally involves unaligned loads and stores.
  int backtrack = ((-width) & 31);
  sp -= backtrack; dp -= backtrack;
  v1f=_mm256_loadu_ps(sp+0);          v2f=_mm256_loadu_ps(sp+8);
  v3f=_mm256_loadu_ps(sp+16);         v4f=_mm256_loadu_ps(sp+24);
  v1f=_mm256_fmadd_ps(v1f,vec_scale,vec_off);
  v2f=_mm256_fmadd_ps(v2f,vec_scale,vec_off);
  v3f=_mm256_fmadd_ps(v3f,vec_scale,vec_off);
  v4f=_mm256_fmadd_ps(v4f,vec_scale,vec_off);
  _mm256_storeu_ps((dp+0),v1f);      _mm256_storeu_ps((dp+8),v2f);
  _mm256_storeu_ps((dp+16),v3f);     _mm256_storeu_ps((dp+24),v4f);
}
  
} // namespace kd_supp_simd

#endif // !KDU_NO_AVX2

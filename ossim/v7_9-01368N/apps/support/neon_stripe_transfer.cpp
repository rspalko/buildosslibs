/*****************************************************************************/
// File: neon_stripe_transfer.cpp [scope = APPS/SUPPORT]
// Version: Kakadu, V7.9
// Author: David Taubman
// Last Revised: 8 January, 2017
/*****************************************************************************/
// Copyright 2001, David Taubman, The University of New South Wales (UNSW)
// The copyright owner is Unisearch Ltd, Australia (commercial arm of UNSW)
// Neither this copyright statement, nor the licensing details below
// may be removed from this file or dissociated from its contents.
/*****************************************************************************/
// Licensee: Open Systems Integration; Inc
// License number: 01368
// The licensee has been granted a NON-COMMERCIAL license to the contents of
// this source file.  A brief summary of this license appears below.  This
// summary is not to be relied upon in preference to the full text of the
// license agreement, accepted at purchase of the license.
// 1. The Licensee has the right to install and use the Kakadu software and
//    to develop Applications for the Licensee's own use.
// 2. The Licensee has the right to Deploy Applications built using the
//    Kakadu software to Third Parties, so long as such Deployment does not
//    result in any direct or indirect financial return to the Licensee or
//    any other Third Party, which further supplies or otherwise uses such
//    Applications.
// 3. The Licensee has the right to distribute Reusable Code (including
//    source code and dynamically or statically linked libraries) to a Third
//    Party, provided the Third Party possesses a license to use the Kakadu
//    software, and provided such distribution does not result in any direct
//    or indirect financial return to the Licensee.
/******************************************************************************
Description:
   Provides SIMD implementations to accelerate the conversion and transfer of
data between the line buffers generated by `kdu_multi_synthesis' or
`kdu_multi_analysis' and the (possibly interleaved) application-supplied
sample buffers supplied to `kdu_stripe_decompressor::pull_stripe' or
`kdu_stripe_compressor::push_stripe'.  This file provides conversion
functions that are based on the ARM NEON instruction set.
******************************************************************************/
#include "kdu_arch.h"

#if ((!defined KDU_NO_NEON) && (defined KDU_NEON_INTRINSICS))

#include <arm_neon.h>
#include <assert.h>

using namespace kdu_core;

namespace kd_supp_simd {
  using namespace kdu_core;
  
#define KDU_STRIPE_STORE_PREF_STREAMING ((int) 1)
  /* Direct copy of the definition found in "kdu_stripe_decompressor.h". */


/* ========================================================================= */
/*               SIMD functions used by `kdu_stripe_compressor'              */
/* ========================================================================= */

/*****************************************************************************/
/* EXTERN               neoni_int16_from_uint8_ilv1                          */
/*****************************************************************************/

void
  neoni_int16_from_uint8_ilv1(kdu_int16 **dst, kdu_byte *src, int width,
                              int src_precision, int tgt_precision,
                              bool is_absolute, bool src_signed)
{
  assert(width > 32); // Guaranteed by macros in "neon_stripe_transfer_local.h"
  assert(!src_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    tgt_precision = KDU_FIX_POINT;
  KD_ARM_PREFETCH(src);     KD_ARM_PREFETCH(src+32);
  KD_ARM_PREFETCH(src+64);  KD_ARM_PREFETCH(src+96);
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int pre_upshift = 8 - src_precision; // Discards unwanted MSBs
  int post_downshift = 16 - tgt_precision;
  int8x16_t vec_off = vdupq_n_s8((int8_t) offset);
  int8x16_t vec_left = vdupq_n_s8((int8_t) pre_upshift);
  int16x8_t vec_right = vdupq_n_s16((kdu_int16) -post_downshift);
  int8x16_t vb1, vb2;
  
  int16x8_t v1, v2, v3, v4;
  kdu_int16 *dp = dst[0];
  int8_t *sp = (int8_t *) src;
  
  // Process all but the last 1-32 input bytes using aligned stores
  for (; width > 32; width-=32)
    { 
      KD_ARM_PREFETCH(sp+128);
      
      // Load, offset, upshift and convert 32 input bytes to shorts
      vb1 = vld1q_s8(sp); sp += 16;  vb2 = vld1q_s8(sp); sp += 16;
      vb1 = vsubq_s8(vb1,vec_off);   vb2 = vsubq_s8(vb2,vec_off);
      vb1 = vshlq_s8(vb1,vec_left);  vb2 = vshlq_s8(vb2,vec_left);
      v1 = vshll_n_s8(vget_low_s8(vb1),8);
      v2 = vshll_n_s8(vget_high_s8(vb1),8);
      v3 = vshll_n_s8(vget_low_s8(vb2),8);
      v4 = vshll_n_s8(vget_high_s8(vb2),8);
      
      // Convert and store 4 output vectors
      v1 = vshlq_s16(v1,vec_right);  v2 = vshlq_s16(v2,vec_right);
      v3 = vshlq_s16(v3,vec_right);  v4 = vshlq_s16(v4,vec_right);
      vst1q_s16(dp,v1);  dp += 8;    vst1q_s16(dp,v2);  dp += 8;
      vst1q_s16(dp,v3);  dp += 8;    vst1q_s16(dp,v4);  dp += 8;
    }
  
  // Finish with 4 unaligned vector stores, after first shifting the source
  // and destination buffers back sufficiently to ensure that we neither
  // overwrite the output buffer nor overread the input buffer.
  int backtrack = ((-width) & 31);
  sp -= backtrack;  dp -= backtrack;
  vb1 = vld1q_s8(sp); sp += 16;  vb2 = vld1q_s8(sp); sp += 16;
  vb1 = vsubq_s8(vb1,vec_off);   vb2 = vsubq_s8(vb2,vec_off);
  vb1 = vshlq_s8(vb1,vec_left);  vb2 = vshlq_s8(vb2,vec_left);
  v1 = vshll_n_s8(vget_low_s8(vb1),8);
  v2 = vshll_n_s8(vget_high_s8(vb1),8);
  v3 = vshll_n_s8(vget_low_s8(vb2),8);
  v4 = vshll_n_s8(vget_high_s8(vb2),8);
  v1 = vshlq_s16(v1,vec_right);  v2 = vshlq_s16(v2,vec_right);
  v3 = vshlq_s16(v3,vec_right);  v4 = vshlq_s16(v4,vec_right);
  vst1q_s16(dp,v1);  dp += 8;    vst1q_s16(dp,v2);  dp += 8;
  vst1q_s16(dp,v3);  dp += 8;    vst1q_s16(dp,v4);  dp += 8;  
}

/*****************************************************************************/
/* EXTERN              neoni_int16_from_uint8_ilv3                           */
/*****************************************************************************/

void
  neoni_int16_from_uint8_ilv3(kdu_int16 **dst, kdu_byte *src, int width,
                              int src_precision, int tgt_precision,
                              bool is_absolute, bool src_signed)
{
  assert(width > 16); // Guaranteed by macros in "neon_stripe_transfer_local.h"
  assert(!src_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    tgt_precision = KDU_FIX_POINT;
  KD_ARM_PREFETCH(src);    KD_ARM_PREFETCH(src+32);  KD_ARM_PREFETCH(src+64);
  KD_ARM_PREFETCH(src+96); KD_ARM_PREFETCH(src+128); KD_ARM_PREFETCH(src+160);
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int pre_upshift = 8 - src_precision; // Discards unwanted MSBs
  int post_downshift = 16 - tgt_precision;
  int8x16_t vec_off = vdupq_n_s8((int8_t) offset);
  int8x16_t vec_left = vdupq_n_s8((int8_t) pre_upshift);
  int16x8_t vec_right = vdupq_n_s16((kdu_int16) -post_downshift);
  int8x16x3_t ivec;
  int8x16_t vb1, vb2, vb3;
  int16x8_t v1, v2, v3, v4, v5, v6;
  kdu_int16 *dp1=dst[0], *dp2=dst[1], *dp3=dst[2];
  int8_t *sp = (int8_t *) src;
  
  // Process all but the last 1-16 input triplets using aligned stores
  for (; width > 16; width-=16)
    { 
      KD_ARM_PREFETCH(sp+192);  KD_ARM_PREFETCH(sp+224);
      
      // Load, offset, upshift and convert 48 input bytes to shorts
      ivec = vld3q_s8(sp);  sp += 48;
      vb1 = vsubq_s8(ivec.val[0],vec_off);
      vb2 = vsubq_s8(ivec.val[1],vec_off);
      vb3 = vsubq_s8(ivec.val[2],vec_off);
      vb1 = vshlq_s8(vb1,vec_left);
      vb2 = vshlq_s8(vb2,vec_left);
      vb3 = vshlq_s8(vb3,vec_left);
      v1 = vshll_n_s8(vget_low_s8(vb1),8);
      v2 = vshll_n_s8(vget_low_s8(vb2),8);
      v3 = vshll_n_s8(vget_low_s8(vb3),8);
      v4 = vshll_n_s8(vget_high_s8(vb1),8);
      v5 = vshll_n_s8(vget_high_s8(vb2),8);
      v6 = vshll_n_s8(vget_high_s8(vb3),8);
      
      // Convert and store 6 output vectors
      v1 = vshlq_s16(v1,vec_right);  v2 = vshlq_s16(v2,vec_right);
      v3 = vshlq_s16(v3,vec_right);  v4 = vshlq_s16(v4,vec_right);
      v5 = vshlq_s16(v5,vec_right);  v6 = vshlq_s16(v6,vec_right);
      vst1q_s16(dp1,v1);  dp1 += 8;  vst1q_s16(dp2,v2);  dp2 += 8;
      vst1q_s16(dp3,v3);  dp3 += 8;
      vst1q_s16(dp1,v4);  dp1 += 8;  vst1q_s16(dp2,v5);  dp2 += 8;
      vst1q_s16(dp3,v6);  dp3 += 8;
    }
  
  // Finish with 6 unaligned vector stores, after first shifting the source
  // and destination buffers back sufficiently to ensure that we neither
  // overwrite the output buffer nor overread the input buffer.
  int backtrack = ((-width) & 15);
  sp -= 3*backtrack;  dp1 -= backtrack;  dp2 -= backtrack;  dp3 -= backtrack;
  ivec = vld3q_s8(sp);  sp += 48;
  vb1 = vsubq_s8(ivec.val[0],vec_off);
  vb2 = vsubq_s8(ivec.val[1],vec_off);
  vb3 = vsubq_s8(ivec.val[2],vec_off);
  vb1 = vshlq_s8(vb1,vec_left);
  vb2 = vshlq_s8(vb2,vec_left);
  vb3 = vshlq_s8(vb3,vec_left);
  v1 = vshll_n_s8(vget_low_s8(vb1),8);
  v2 = vshll_n_s8(vget_low_s8(vb2),8);
  v3 = vshll_n_s8(vget_low_s8(vb3),8);
  v4 = vshll_n_s8(vget_high_s8(vb1),8);
  v5 = vshll_n_s8(vget_high_s8(vb2),8);
  v6 = vshll_n_s8(vget_high_s8(vb3),8);
  v1 = vshlq_s16(v1,vec_right);  v2 = vshlq_s16(v2,vec_right);
  v3 = vshlq_s16(v3,vec_right);  v4 = vshlq_s16(v4,vec_right);
  v5 = vshlq_s16(v5,vec_right);  v6 = vshlq_s16(v6,vec_right);
  vst1q_s16(dp1,v1);  dp1 += 8;  vst1q_s16(dp2,v2);  dp2 += 8;
  vst1q_s16(dp3,v3);  dp3 += 8;
  vst1q_s16(dp1,v4);  dp1 += 8;  vst1q_s16(dp2,v5);  dp2 += 8;
  vst1q_s16(dp3,v6);  dp3 += 8;
}

/*****************************************************************************/
/* EXTERN               neoni_int16_from_uint8_ilv4                          */
/*****************************************************************************/

void
  neoni_int16_from_uint8_ilv4(kdu_int16 **dst, kdu_byte *src, int width,
                              int src_precision, int tgt_precision,
                              bool is_absolute, bool src_signed)
{
  assert(width > 16); // Guaranteed by macros in "neon_stripe_transfer_local.h"
  assert(!src_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    tgt_precision = KDU_FIX_POINT;
  KD_ARM_PREFETCH(src);     KD_ARM_PREFETCH(src+32);
  KD_ARM_PREFETCH(src+64);  KD_ARM_PREFETCH(src+96);
  KD_ARM_PREFETCH(src+128); KD_ARM_PREFETCH(src+160);
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int pre_upshift = 8 - src_precision; // Discards unwanted MSBs
  int post_downshift = 16 - tgt_precision;
  int8x16_t vec_off = vdupq_n_s8((int8_t) offset);
  int8x16_t vec_left = vdupq_n_s8((int8_t) pre_upshift);
  int16x8_t vec_right = vdupq_n_s16((kdu_int16) -post_downshift);
  int8x16x4_t ivec;
  int8x16_t vb1, vb2, vb3, vb4;
  int16x8_t v1, v2, v3, v4, v5, v6, v7, v8;
  kdu_int16 *dp1=dst[0], *dp2=dst[1], *dp3=dst[2], *dp4=dst[3];
  int8_t *sp = (int8_t *) src;
  
  // Process all but the last 1-16 input triplets using aligned stores
  for (; width > 16; width-=16)
    { 
      KD_ARM_PREFETCH(sp+192);  KD_ARM_PREFETCH(sp+224);
      
      // Load, offset, upshift and convert 64 input bytes to shorts
      ivec = vld4q_s8(sp);  sp += 64;
      vb1 = vsubq_s8(ivec.val[0],vec_off); vb2 = vsubq_s8(ivec.val[1],vec_off);
      vb3 = vsubq_s8(ivec.val[2],vec_off); vb4 = vsubq_s8(ivec.val[3],vec_off);
      vb1 = vshlq_s8(vb1,vec_left);        vb2 = vshlq_s8(vb2,vec_left);
      vb3 = vshlq_s8(vb3,vec_left);        vb4 = vshlq_s8(vb4,vec_left);
      v1 = vshll_n_s8(vget_low_s8(vb1),8);
      v2 = vshll_n_s8(vget_high_s8(vb1),8);
      v3 = vshll_n_s8(vget_low_s8(vb2),8);
      v4 = vshll_n_s8(vget_high_s8(vb2),8);
      v5 = vshll_n_s8(vget_low_s8(vb3),8);
      v6 = vshll_n_s8(vget_high_s8(vb3),8);
      v7 = vshll_n_s8(vget_low_s8(vb4),8);
      v8 = vshll_n_s8(vget_high_s8(vb4),8);
      
      // Convert and store 8 output vectors
      v1 = vshlq_s16(v1,vec_right);  v2 = vshlq_s16(v2,vec_right);
      v3 = vshlq_s16(v3,vec_right);  v4 = vshlq_s16(v4,vec_right);
      v5 = vshlq_s16(v5,vec_right);  v6 = vshlq_s16(v6,vec_right);
      v7 = vshlq_s16(v7,vec_right);  v8 = vshlq_s16(v8,vec_right);
      vst1q_s16(dp1,v1);  dp1 += 8;  vst1q_s16(dp1,v2);  dp1 += 8;
      vst1q_s16(dp2,v3);  dp2 += 8;  vst1q_s16(dp2,v4);  dp2 += 8;
      vst1q_s16(dp3,v5);  dp3 += 8;  vst1q_s16(dp3,v6);  dp3 += 8;
      vst1q_s16(dp4,v7);  dp4 += 8;  vst1q_s16(dp4,v8);  dp4 += 8;
    }
  
  // Finish with 8 unaligned vector stores, after first shifting the source
  // and destination buffers back sufficiently to ensure that we neither
  // overwrite the output buffer nor overread the input buffer.
  int backtrack = ((-width) & 15);
  sp -= 4*backtrack;
  dp1 -= backtrack;  dp2 -= backtrack;  dp3 -= backtrack;  dp4 -= backtrack;
  ivec = vld4q_s8(sp);  sp += 64;
  vb1 = vsubq_s8(ivec.val[0],vec_off); vb2 = vsubq_s8(ivec.val[1],vec_off);
  vb3 = vsubq_s8(ivec.val[2],vec_off); vb4 = vsubq_s8(ivec.val[3],vec_off);
  vb1 = vshlq_s8(vb1,vec_left);        vb2 = vshlq_s8(vb2,vec_left);
  vb3 = vshlq_s8(vb3,vec_left);        vb4 = vshlq_s8(vb4,vec_left);
  v1 = vshll_n_s8(vget_low_s8(vb1),8);
  v2 = vshll_n_s8(vget_high_s8(vb1),8);
  v3 = vshll_n_s8(vget_low_s8(vb2),8);
  v4 = vshll_n_s8(vget_high_s8(vb2),8);
  v5 = vshll_n_s8(vget_low_s8(vb3),8);
  v6 = vshll_n_s8(vget_high_s8(vb3),8);
  v7 = vshll_n_s8(vget_low_s8(vb4),8);
  v8 = vshll_n_s8(vget_high_s8(vb4),8);
  v1 = vshlq_s16(v1,vec_right);  v2 = vshlq_s16(v2,vec_right);
  v3 = vshlq_s16(v3,vec_right);  v4 = vshlq_s16(v4,vec_right);
  v5 = vshlq_s16(v5,vec_right);  v6 = vshlq_s16(v6,vec_right);
  v7 = vshlq_s16(v7,vec_right);  v8 = vshlq_s16(v8,vec_right);
  vst1q_s16(dp1,v1);  dp1 += 8;  vst1q_s16(dp1,v2);  dp1 += 8;
  vst1q_s16(dp2,v3);  dp2 += 8;  vst1q_s16(dp2,v4);  dp2 += 8;
  vst1q_s16(dp3,v5);  dp3 += 8;  vst1q_s16(dp3,v6);  dp3 += 8;
  vst1q_s16(dp4,v7);  dp4 += 8;  vst1q_s16(dp4,v8);  dp4 += 8;
}

/*****************************************************************************/
/* EXTERN               neoni_int16_from_int16_ilv1                          */
/*****************************************************************************/

void
  neoni_int16_from_int16_ilv1(kdu_int16 **dst, kdu_int16 *src, int width,
                              int src_precision, int tgt_precision,
                              bool is_absolute, bool src_signed)
{
  assert(width > 32); // Guaranteed by macros in "neon_stripe_transfer_local.h"
  if (!is_absolute)
    tgt_precision = KDU_FIX_POINT;
  KD_ARM_PREFETCH(src);     KD_ARM_PREFETCH(src+16);
  KD_ARM_PREFETCH(src+32);  KD_ARM_PREFETCH(src+48);
  KD_ARM_PREFETCH(src+64);  KD_ARM_PREFETCH(src+80);
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int pre_upshift = 16 - src_precision; // Deliberately discards unwanted MSBs
  int post_downshift = 16 - tgt_precision;
  int16x8_t vec_off = vdupq_n_s16((kdu_int16) offset);
  int16x8_t vec_left = vdupq_n_s16((kdu_int16) pre_upshift);
  int16x8_t vec_right = vdupq_n_s16((kdu_int16) -post_downshift);
  int16x8_t v1, v2, v3, v4;
  kdu_int16 *dp = dst[0];
  kdu_int16 *sp = src;
  
  // Process all but the last 1-32 input bytes using aligned stores
  for (; width > 32; width-=32)
    { 
      KD_ARM_PREFETCH(sp+96);  KD_ARM_PREFETCH(sp+112);
      
      // Load 32 shorts in 4 vectors
      v1 = vld1q_s16(sp);  sp += 8;  v2 = vld1q_s16(sp);  sp += 8;
      v3 = vld1q_s16(sp);  sp += 8;  v4 = vld1q_s16(sp);  sp += 8;
      
      // Convert and store 4 output vectors
      v1 = vsubq_s16(v1,vec_off);    v2 = vsubq_s16(v2,vec_off);
      v3 = vsubq_s16(v3,vec_off);    v4 = vsubq_s16(v4,vec_off);
      v1 = vshlq_s16(v1,vec_left);   v2 = vshlq_s16(v2,vec_left);
      v3 = vshlq_s16(v3,vec_left);   v4 = vshlq_s16(v4,vec_left);
      v1 = vshlq_s16(v1,vec_right);  v2 = vshlq_s16(v2,vec_right);
      v3 = vshlq_s16(v3,vec_right);  v4 = vshlq_s16(v4,vec_right);
      vst1q_s16(dp,v1);  dp += 8;    vst1q_s16(dp,v2);  dp += 8;
      vst1q_s16(dp,v3);  dp += 8;    vst1q_s16(dp,v4);  dp += 8;
    }
  
  // Finish with 4 unaligned vector stores, after first shifting the source
  // and destination buffers back sufficiently to ensure that we neither
  // overwrite the output buffer nor overread the input buffer.
  int backtrack = ((-width) & 31);
  sp -= backtrack;  dp -= backtrack;
  v1 = vld1q_s16(sp);  sp += 8;  v2 = vld1q_s16(sp);  sp += 8;
  v3 = vld1q_s16(sp);  sp += 8;  v4 = vld1q_s16(sp);  sp += 8;
  v1 = vsubq_s16(v1,vec_off);    v2 = vsubq_s16(v2,vec_off);
  v3 = vsubq_s16(v3,vec_off);    v4 = vsubq_s16(v4,vec_off);
  v1 = vshlq_s16(v1,vec_left);   v2 = vshlq_s16(v2,vec_left);
  v3 = vshlq_s16(v3,vec_left);   v4 = vshlq_s16(v4,vec_left);
  v1 = vshlq_s16(v1,vec_right);  v2 = vshlq_s16(v2,vec_right);
  v3 = vshlq_s16(v3,vec_right);  v4 = vshlq_s16(v4,vec_right);
  vst1q_s16(dp,v1);  dp += 8;    vst1q_s16(dp,v2);  dp += 8;
  vst1q_s16(dp,v3);  dp += 8;    vst1q_s16(dp,v4);  dp += 8;  
}

/*****************************************************************************/
/* EXTERN               neoni_floats_from_int16_ilv1                         */
/*****************************************************************************/

void
  neoni_floats_from_int16_ilv1(float **dst, kdu_int16 *src, int width,
                               int src_precision, int tgt_precision,
                               bool is_absolute, bool src_signed)
{
  assert(width > 32); // Guaranteed by macros in "neon_stripe_transfer_local.h"
  assert(!is_absolute);
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int pre_upshift = 16 - src_precision;
  float scale = 1.0F  / ((float)(1<<16));
  KD_ARM_PREFETCH(src);     KD_ARM_PREFETCH(src+16);
  KD_ARM_PREFETCH(src+32);  KD_ARM_PREFETCH(src+48);
  KD_ARM_PREFETCH(src+64);  KD_ARM_PREFETCH(src+80);
  int16x8_t vec_off = vdupq_n_s16((kdu_int16) offset);
  int16x8_t vec_left = vdupq_n_s16((kdu_int16) pre_upshift);
  int16x8_t v1, v2, v3, v4;
  float32x4_t vf1, vf2, vf3, vf4;
  float *dp = dst[0];
  kdu_int16 *sp = src;
  
  // Process all but the last 1-32 input bytes using aligned stores
  for (; width > 32; width-=32)
    { 
      KD_ARM_PREFETCH(sp+96);  KD_ARM_PREFETCH(sp+112);
      
      // Load 32 shorts in 4 vectors
      v1 = vld1q_s16(sp);  sp += 8;  v2 = vld1q_s16(sp);  sp += 8;
      v3 = vld1q_s16(sp);  sp += 8;  v4 = vld1q_s16(sp);  sp += 8;
      
      // Level adjust and upshift to discard unused MSB's
      v1 = vsubq_s16(v1,vec_off);    v2 = vsubq_s16(v2,vec_off);
      v3 = vsubq_s16(v3,vec_off);    v4 = vsubq_s16(v4,vec_off);
      v1 = vshlq_s16(v1,vec_left);   v2 = vshlq_s16(v2,vec_left);
      v3 = vshlq_s16(v3,vec_left);   v4 = vshlq_s16(v4,vec_left);

      // Sign extend to integers, convert, scale and save
      vf1 = vcvtq_f32_s32(vmovl_s16(vget_low_s16(v1)));
      vf2 = vcvtq_f32_s32(vmovl_s16(vget_high_s16(v1)));
      vf3 = vcvtq_f32_s32(vmovl_s16(vget_low_s16(v2)));
      vf4 = vcvtq_f32_s32(vmovl_s16(vget_high_s16(v2)));
      vf1 = vmulq_n_f32(vf1,scale);  vf2 = vmulq_n_f32(vf2,scale);
      vf3 = vmulq_n_f32(vf3,scale);  vf4 = vmulq_n_f32(vf4,scale);
      vst1q_f32(dp,vf1);  dp += 4;   vst1q_f32(dp,vf2);  dp += 4;
      vst1q_f32(dp,vf3);  dp += 4;   vst1q_f32(dp,vf4);  dp += 4;

      vf1 = vcvtq_f32_s32(vmovl_s16(vget_low_s16(v3)));
      vf2 = vcvtq_f32_s32(vmovl_s16(vget_high_s16(v3)));
      vf3 = vcvtq_f32_s32(vmovl_s16(vget_low_s16(v4)));
      vf4 = vcvtq_f32_s32(vmovl_s16(vget_high_s16(v4)));
      vf1 = vmulq_n_f32(vf1,scale);  vf2 = vmulq_n_f32(vf2,scale);
      vf3 = vmulq_n_f32(vf3,scale);  vf4 = vmulq_n_f32(vf4,scale);
      vst1q_f32(dp,vf1);  dp += 4;   vst1q_f32(dp,vf2);  dp += 4;
      vst1q_f32(dp,vf3);  dp += 4;   vst1q_f32(dp,vf4);  dp += 4;
    }
}


/* ========================================================================= */
/*             SIMD functions used by `kdu_stripe_decompressor'              */
/* ========================================================================= */

/*****************************************************************************/
/* EXTERN                neoni_int16_to_uint8_ilv1                           */
/*****************************************************************************/

void neoni_int16_to_uint8_ilv1(kdu_byte *dst, kdu_int16 **src, int width,
                               int precision, int orig_precision,
                               bool is_absolute, bool dst_signed,
                               int preferences)
{
  assert(width > 32); // Guaranteed by macros in "neon_stripe_transfer_local.h"
  assert(!dst_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    orig_precision = KDU_FIX_POINT;
  kdu_int16 *sp = src[0];
  KD_ARM_PREFETCH(sp);     KD_ARM_PREFETCH(sp+16);
  KD_ARM_PREFETCH(sp+32);  KD_ARM_PREFETCH(sp+48);
  int downshift = orig_precision - precision; // Usually +ve but -ve allowed
  int min_val = -(1<<(precision-1)); // Prior to level offset
  int max_val = (1<<(precision-1))-1; // Prior to level offset
  int16x8_t vec_shift = vdupq_n_s16((kdu_int16) -downshift);
  int8x16_t vec_min = vdupq_n_s8((int8_t) min_val);
  int8x16_t vec_max = vdupq_n_s8((int8_t) max_val);
  int16x8_t v1, v2, v3, v4;
  int8x16_t vb1, vb2;
  int8_t *dp = (int8_t *) dst;
  
  // Generate all but the last 1 to 32 outputs
  for (; width > 32; width-=32)
    { 
      KD_ARM_PREFETCH(sp+64);  KD_ARM_PREFETCH(sp+80);
      
      // Load 32 shorts, shift (with rounding) and pack to signed 8-bit samples
      v1 = vld1q_s16(sp);  sp += 8;    v2 = vld1q_s16(sp);  sp += 8;
      v3 = vld1q_s16(sp);  sp += 8;    v4 = vld1q_s16(sp);  sp += 8;
      v1 = vrshlq_s16(v1,vec_shift);   v2 = vrshlq_s16(v2,vec_shift);
      v3 = vrshlq_s16(v3,vec_shift);   v4 = vrshlq_s16(v4,vec_shift);
      vb1 = vcombine_s8(vqmovn_s16(v1),vqmovn_s16(v2));
      vb2 = vcombine_s8(vqmovn_s16(v3),vqmovn_s16(v4));
      
      // Apply limits, level adjust and store results
      vb1 = vminq_s8(vb1,vec_max);     vb2 = vminq_s8(vb2,vec_max);
      vb1 = vmaxq_s8(vb1,vec_min);     vb2 = vmaxq_s8(vb2,vec_min);
      vb1 = vsubq_s8(vb1,vec_min);     vb2 = vsubq_s8(vb2,vec_min);
      vst1q_s8(dp,vb1);  dp += 16;     vst1q_s8(dp,vb2);  dp += 16;
    }

  // Backtrack 0 to 31 positions so we can use vector processing also for the
  // final output samples.  This generally involves unaligned loads and stores.
  int backtrack = ((-width) & 31);
  sp -= backtrack; dp -= backtrack;
  v1 = vld1q_s16(sp);  sp += 8;    v2 = vld1q_s16(sp);  sp += 8;
  v3 = vld1q_s16(sp);  sp += 8;    v4 = vld1q_s16(sp);  sp += 8;
  v1 = vrshlq_s16(v1,vec_shift);   v2 = vrshlq_s16(v2,vec_shift);
  v3 = vrshlq_s16(v3,vec_shift);   v4 = vrshlq_s16(v4,vec_shift);
  vb1 = vcombine_s8(vqmovn_s16(v1),vqmovn_s16(v2));
  vb2 = vcombine_s8(vqmovn_s16(v3),vqmovn_s16(v4));
  vb1 = vminq_s8(vb1,vec_max);     vb2 = vminq_s8(vb2,vec_max);
  vb1 = vmaxq_s8(vb1,vec_min);     vb2 = vmaxq_s8(vb2,vec_min);
  vb1 = vsubq_s8(vb1,vec_min);     vb2 = vsubq_s8(vb2,vec_min);
  vst1q_s8(dp,vb1);  dp += 16;     vst1q_s8(dp,vb2);  dp += 16;
}

/*****************************************************************************/
/* EXTERN                neoni_int16_to_uint8_ilv3                           */
/*****************************************************************************/

void neoni_int16_to_uint8_ilv3(kdu_byte *dst, kdu_int16 **src, int width,
                               int precision, int orig_precision,
                               bool is_absolute, bool dst_signed,
                               int preferences)
{
  assert(width > 16); // Guaranteed by macros in "neon_stripe_transfer_local.h"
  assert(!dst_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    orig_precision = KDU_FIX_POINT;
  kdu_int16 *sp1=src[0], *sp2=src[1], *sp3=src[2];
  KD_ARM_PREFETCH(sp1);     KD_ARM_PREFETCH(sp1+16);
  KD_ARM_PREFETCH(sp2);     KD_ARM_PREFETCH(sp2+16);
  KD_ARM_PREFETCH(sp3);     KD_ARM_PREFETCH(sp3+16);
  int downshift = orig_precision - precision; // Usually +ve but -ve allowed
  int min_val = -(1<<(precision-1)); // Prior to level offset
  int max_val = (1<<(precision-1))-1; // Prior to level offset
  int16x8_t vec_shift = vdupq_n_s16((kdu_int16) -downshift);
  int8x16_t vec_min = vdupq_n_s8((int8_t) min_val);
  int8x16_t vec_max = vdupq_n_s8((int8_t) max_val);
  int16x8_t v1, v2, v3, v4, v5, v6;
  int8x16x3_t ivec;
  int8_t *dp = (int8_t *) dst;
  
  // Process all but the last 1 to 16 triplets
  for (; width > 16; width-=16)
    { 
      KD_ARM_PREFETCH(sp1+32);  KD_ARM_PREFETCH(sp2+32);
      KD_ARM_PREFETCH(sp3+32);
      
      // Load 48 shorts, shift (with rounding) and pack to signed 8-bit samples
      v1 = vld1q_s16(sp1);  sp1 += 8;  v2 = vld1q_s16(sp1);  sp1 += 8;
      v3 = vld1q_s16(sp2);  sp2 += 8;  v4 = vld1q_s16(sp2);  sp2 += 8;
      v5 = vld1q_s16(sp3);  sp3 += 8;  v6 = vld1q_s16(sp3);  sp3 += 8;
      v1 = vrshlq_s16(v1,vec_shift);   v2 = vrshlq_s16(v2,vec_shift);
      v3 = vrshlq_s16(v3,vec_shift);   v4 = vrshlq_s16(v4,vec_shift);
      v5 = vrshlq_s16(v5,vec_shift);   v6 = vrshlq_s16(v6,vec_shift);
      ivec.val[0] = vcombine_s8(vqmovn_s16(v1),vqmovn_s16(v2));
      ivec.val[1] = vcombine_s8(vqmovn_s16(v3),vqmovn_s16(v4));
      ivec.val[2] = vcombine_s8(vqmovn_s16(v5),vqmovn_s16(v6));
      
      // Apply limits and level adjust and store results
      ivec.val[0] = vminq_s8(ivec.val[0],vec_max);
      ivec.val[1] = vminq_s8(ivec.val[1],vec_max);
      ivec.val[2] = vminq_s8(ivec.val[2],vec_max);
      ivec.val[0] = vmaxq_s8(ivec.val[0],vec_min);
      ivec.val[1] = vmaxq_s8(ivec.val[1],vec_min);
      ivec.val[2] = vmaxq_s8(ivec.val[2],vec_min);
      ivec.val[0] = vsubq_s8(ivec.val[0],vec_min);
      ivec.val[1] = vsubq_s8(ivec.val[1],vec_min);
      ivec.val[2] = vsubq_s8(ivec.val[2],vec_min);
      vst3q_s8(dp,ivec); dp += 48;
    }

  // Backtrack 0 to 15 positions so we can use vector processing also for the
  // final output samples.  This generally involves unaligned loads and stores.
  int backtrack = ((-width) & 15);
  dp -= 3*backtrack;  sp1 -= backtrack;  sp2 -= backtrack;  sp3 -= backtrack;
  v1 = vld1q_s16(sp1);  sp1 += 8;  v2 = vld1q_s16(sp1);  sp1 += 8;
  v3 = vld1q_s16(sp2);  sp2 += 8;  v4 = vld1q_s16(sp2);  sp2 += 8;
  v5 = vld1q_s16(sp3);  sp3 += 8;  v6 = vld1q_s16(sp3);  sp3 += 8;
  v1 = vrshlq_s16(v1,vec_shift);   v2 = vrshlq_s16(v2,vec_shift);
  v3 = vrshlq_s16(v3,vec_shift);   v4 = vrshlq_s16(v4,vec_shift);
  v5 = vrshlq_s16(v5,vec_shift);   v6 = vrshlq_s16(v6,vec_shift);
  ivec.val[0] = vcombine_s8(vqmovn_s16(v1),vqmovn_s16(v2));
  ivec.val[1] = vcombine_s8(vqmovn_s16(v3),vqmovn_s16(v4));
  ivec.val[2] = vcombine_s8(vqmovn_s16(v5),vqmovn_s16(v6));
  ivec.val[0] = vminq_s8(ivec.val[0],vec_max);
  ivec.val[1] = vminq_s8(ivec.val[1],vec_max);
  ivec.val[2] = vminq_s8(ivec.val[2],vec_max);
  ivec.val[0] = vmaxq_s8(ivec.val[0],vec_min);
  ivec.val[1] = vmaxq_s8(ivec.val[1],vec_min);
  ivec.val[2] = vmaxq_s8(ivec.val[2],vec_min);
  ivec.val[0] = vsubq_s8(ivec.val[0],vec_min);
  ivec.val[1] = vsubq_s8(ivec.val[1],vec_min);
  ivec.val[2] = vsubq_s8(ivec.val[2],vec_min);
  vst3q_s8(dp,ivec); dp += 48;
}

/*****************************************************************************/
/* EXTERN                neoni_int16_to_uint8_ilv4                           */
/*****************************************************************************/

void neoni_int16_to_uint8_ilv4(kdu_byte *dst, kdu_int16 **src, int width,
                               int precision, int orig_precision,
                               bool is_absolute, bool dst_signed,
                               int preferences)
{
  assert(width > 16); // Guaranteed by macros in "neon_stripe_transfer_local.h"
  assert(!dst_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    orig_precision = KDU_FIX_POINT;
  kdu_int16 *sp1=src[0], *sp2=src[1], *sp3=src[2], *sp4=src[3];
  KD_ARM_PREFETCH(sp1);     KD_ARM_PREFETCH(sp1+16);
  KD_ARM_PREFETCH(sp2);     KD_ARM_PREFETCH(sp2+16);
  KD_ARM_PREFETCH(sp3);     KD_ARM_PREFETCH(sp3+16);
  KD_ARM_PREFETCH(sp4);     KD_ARM_PREFETCH(sp4+16);

  int downshift = orig_precision - precision; // Usually +ve but -ve allowed
  int min_val = -(1<<(precision-1)); // Prior to level offset
  int max_val = (1<<(precision-1))-1; // Prior to level offset
  int16x8_t vec_shift = vdupq_n_s16((kdu_int16) -downshift);
  int8x16_t vec_min = vdupq_n_s8((int8_t) min_val);
  int8x16_t vec_max = vdupq_n_s8((int8_t) max_val);
  int16x8_t v1, v2, v3, v4, v5, v6, v7, v8;
  int8x16x4_t ivec;
  int8_t *dp = (int8_t *) dst;

  // Process all but the last 1 to 16 quartets
  for (; width > 16; width-=16)
    { 
      KD_ARM_PREFETCH(sp1+32);  KD_ARM_PREFETCH(sp2+32);
      KD_ARM_PREFETCH(sp3+32);  KD_ARM_PREFETCH(sp4+32);

      // Load 64 shorts, shift (with rounding) and pack to signed 8-bit samples
      v1 = vld1q_s16(sp1);  sp1 += 8;  v2 = vld1q_s16(sp1);  sp1 += 8;
      v3 = vld1q_s16(sp2);  sp2 += 8;  v4 = vld1q_s16(sp2);  sp2 += 8;
      v5 = vld1q_s16(sp3);  sp3 += 8;  v6 = vld1q_s16(sp3);  sp3 += 8;
      v7 = vld1q_s16(sp4);  sp4 += 8;  v8 = vld1q_s16(sp4);  sp4 += 8;
      v1 = vrshlq_s16(v1,vec_shift);   v2 = vrshlq_s16(v2,vec_shift);
      v3 = vrshlq_s16(v3,vec_shift);   v4 = vrshlq_s16(v4,vec_shift);
      v5 = vrshlq_s16(v5,vec_shift);   v6 = vrshlq_s16(v6,vec_shift);
      v7 = vrshlq_s16(v7,vec_shift);   v8 = vrshlq_s16(v8,vec_shift);
      ivec.val[0] = vcombine_s8(vqmovn_s16(v1),vqmovn_s16(v2));
      ivec.val[1] = vcombine_s8(vqmovn_s16(v3),vqmovn_s16(v4));
      ivec.val[2] = vcombine_s8(vqmovn_s16(v5),vqmovn_s16(v6));
      ivec.val[3] = vcombine_s8(vqmovn_s16(v7),vqmovn_s16(v8));

      // Apply limits and level adjust and store results
      ivec.val[0] = vminq_s8(ivec.val[0],vec_max);
      ivec.val[1] = vminq_s8(ivec.val[1],vec_max);
      ivec.val[2] = vminq_s8(ivec.val[2],vec_max);
      ivec.val[3] = vminq_s8(ivec.val[3],vec_max);
      ivec.val[0] = vmaxq_s8(ivec.val[0],vec_min);
      ivec.val[1] = vmaxq_s8(ivec.val[1],vec_min);
      ivec.val[2] = vmaxq_s8(ivec.val[2],vec_min);
      ivec.val[3] = vmaxq_s8(ivec.val[3],vec_min);
      ivec.val[0] = vsubq_s8(ivec.val[0],vec_min);
      ivec.val[1] = vsubq_s8(ivec.val[1],vec_min);
      ivec.val[2] = vsubq_s8(ivec.val[2],vec_min);
      ivec.val[3] = vsubq_s8(ivec.val[3],vec_min);
      vst4q_s8(dp,ivec); dp += 64;
    }

  // Backtrack 0 to 15 positions so we can use vector processing also for the
  // final output samples.  This generally involves unaligned loads and stores.
  int backtrack = ((-width) & 15);
  dp -= 4*backtrack;
  sp1 -= backtrack;  sp2 -= backtrack;  sp3 -= backtrack;  sp4 -= backtrack;
  v1 = vld1q_s16(sp1);  sp1 += 8;  v2 = vld1q_s16(sp1);  sp1 += 8;
  v3 = vld1q_s16(sp2);  sp2 += 8;  v4 = vld1q_s16(sp2);  sp2 += 8;
  v5 = vld1q_s16(sp3);  sp3 += 8;  v6 = vld1q_s16(sp3);  sp3 += 8;
  v7 = vld1q_s16(sp4);  sp4 += 8;  v8 = vld1q_s16(sp4);  sp4 += 8;
  v1 = vrshlq_s16(v1,vec_shift);   v2 = vrshlq_s16(v2,vec_shift);
  v3 = vrshlq_s16(v3,vec_shift);   v4 = vrshlq_s16(v4,vec_shift);
  v5 = vrshlq_s16(v5,vec_shift);   v6 = vrshlq_s16(v6,vec_shift);
  v7 = vrshlq_s16(v7,vec_shift);   v8 = vrshlq_s16(v8,vec_shift);
  ivec.val[0] = vcombine_s8(vqmovn_s16(v1),vqmovn_s16(v2));
  ivec.val[1] = vcombine_s8(vqmovn_s16(v3),vqmovn_s16(v4));
  ivec.val[2] = vcombine_s8(vqmovn_s16(v5),vqmovn_s16(v6));
  ivec.val[3] = vcombine_s8(vqmovn_s16(v7),vqmovn_s16(v8));
  ivec.val[0] = vminq_s8(ivec.val[0],vec_max);
  ivec.val[1] = vminq_s8(ivec.val[1],vec_max);
  ivec.val[2] = vminq_s8(ivec.val[2],vec_max);
  ivec.val[3] = vminq_s8(ivec.val[3],vec_max);
  ivec.val[0] = vmaxq_s8(ivec.val[0],vec_min);
  ivec.val[1] = vmaxq_s8(ivec.val[1],vec_min);
  ivec.val[2] = vmaxq_s8(ivec.val[2],vec_min);
  ivec.val[3] = vmaxq_s8(ivec.val[3],vec_min);
  ivec.val[0] = vsubq_s8(ivec.val[0],vec_min);
  ivec.val[1] = vsubq_s8(ivec.val[1],vec_min);
  ivec.val[2] = vsubq_s8(ivec.val[2],vec_min);
  ivec.val[3] = vsubq_s8(ivec.val[3],vec_min);
  vst4q_s8(dp,ivec); dp += 64;
}

/*****************************************************************************/
/* EXTERN                neoni_int16_to_int16_ilv1                           */
/*****************************************************************************/

void neoni_int16_to_int16_ilv1(kdu_int16 *dst, kdu_int16 **src, int width,
                               int precision, int orig_precision,
                               bool is_absolute, bool dst_signed,
                               int preferences)
{
  assert(width > 32); // Guaranteed by macros in "neon_stripe_transfer_local.h"
  if (!is_absolute)
    orig_precision = KDU_FIX_POINT;
  kdu_int16 *sp = src[0];
  KD_ARM_PREFETCH(sp);     KD_ARM_PREFETCH(sp+16);
  KD_ARM_PREFETCH(sp+32);  KD_ARM_PREFETCH(sp+48);
  int downshift = orig_precision - precision; // Usually +ve but -ve allowed
  int min_val = -(1<<(precision-1)); // Prior to level offset
  int max_val = (1<<(precision-1))-1; // Prior to level offset
  int16x8_t vec_shift = vdupq_n_s16((kdu_int16) -downshift);
  int16x8_t vec_min = vdupq_n_s16((kdu_int16) min_val);
  int16x8_t vec_max = vdupq_n_s16((kdu_int16) max_val);
  int16x8_t v1, v2, v3, v4;
  kdu_int16 *dp = dst;

  // Generate all but the last 1 to 32 outputs
  for (; width > 32; width-=32)
    { 
      KD_ARM_PREFETCH(sp+64);  KD_ARM_PREFETCH(sp+80);      
      v1 = vld1q_s16(sp);  sp += 8;    v2 = vld1q_s16(sp);  sp += 8;
      v3 = vld1q_s16(sp);  sp += 8;    v4 = vld1q_s16(sp);  sp += 8;
      v1 = vrshlq_s16(v1,vec_shift);   v2 = vrshlq_s16(v2,vec_shift);
      v3 = vrshlq_s16(v3,vec_shift);   v4 = vrshlq_s16(v4,vec_shift);
      v1 = vminq_s16(v1,vec_max);      v2 = vminq_s16(v2,vec_max);
      v3 = vminq_s16(v3,vec_max);      v4 = vminq_s16(v4,vec_max);
      v1 = vmaxq_s16(v1,vec_min);      v2 = vmaxq_s16(v2,vec_min);
      v3 = vmaxq_s16(v3,vec_min);      v4 = vmaxq_s16(v4,vec_min);
      v1 = vsubq_s16(v1,vec_min);      v2 = vsubq_s16(v2,vec_min);
      v3 = vsubq_s16(v3,vec_min);      v4 = vsubq_s16(v4,vec_min);
      vst1q_s16(dp,v1);  dp += 8;      vst1q_s16(dp,v2);  dp += 8;
      vst1q_s16(dp,v3);  dp += 8;      vst1q_s16(dp,v4);  dp += 8;
    }
  
  // Backtrack 0 to 31 positions so we can use vector processing also for the
  // final output samples.  This generally involves unaligned loads and stores.
  int backtrack = ((-width) & 31);
  sp -= backtrack; dp -= backtrack;
  v1 = vld1q_s16(sp);  sp += 8;    v2 = vld1q_s16(sp);  sp += 8;
  v3 = vld1q_s16(sp);  sp += 8;    v4 = vld1q_s16(sp);  sp += 8;
  v1 = vrshlq_s16(v1,vec_shift);   v2 = vrshlq_s16(v2,vec_shift);
  v3 = vrshlq_s16(v3,vec_shift);   v4 = vrshlq_s16(v4,vec_shift);
  v1 = vminq_s16(v1,vec_max);      v2 = vminq_s16(v2,vec_max);
  v3 = vminq_s16(v3,vec_max);      v4 = vminq_s16(v4,vec_max);
  v1 = vmaxq_s16(v1,vec_min);      v2 = vmaxq_s16(v2,vec_min);
  v3 = vmaxq_s16(v3,vec_min);      v4 = vmaxq_s16(v4,vec_min);
  v1 = vsubq_s16(v1,vec_min);      v2 = vsubq_s16(v2,vec_min);
  v3 = vsubq_s16(v3,vec_min);      v4 = vsubq_s16(v4,vec_min);
  vst1q_s16(dp,v1);  dp += 8;      vst1q_s16(dp,v2);  dp += 8;
  vst1q_s16(dp,v3);  dp += 8;      vst1q_s16(dp,v4);  dp += 8;
}

/*****************************************************************************/
/* EXTERN                neoni_floats_to_int16_ilv1                          */
/*****************************************************************************/

void neoni_floats_to_int16_ilv1(kdu_int16 *dst, float **src, int width,
                                int precision, int orig_precision,
                                bool is_absolute, bool dst_signed,
                                int preferences)
{
  assert(width > 16); // Guaranteed by macros in "neon_stripe_transfer_local.h"
  assert(!is_absolute);
  float *sp = src[0];
  KD_ARM_PREFETCH(sp);     KD_ARM_PREFETCH(sp+8);
  KD_ARM_PREFETCH(sp+16);  KD_ARM_PREFETCH(sp+24);
  float scale = (float)(1<<precision);
  float offset = 0.5F + (float)(1<<(precision-1)); // Includes rounding offset
  int max_val = (1<<precision)-1; // After conversion to unsigned int  

  float32x4_t voff = vdupq_n_f32(offset);
  float32x4_t v1f, v2f, v3f, v4f;
  uint32x4_t vec_max = vdupq_n_u32((kdu_uint32) max_val);
  uint32x4_t v1, v2, v3, v4;
  uint16x8_t vs1, vs2;;
  kdu_uint16 *dp = (kdu_uint16 *) dst;
  
  // Generate all but the last 1 to 32 outputs
  for (; width > 16; width-=16)
    { 
      KD_ARM_PREFETCH(sp+32);
      v1f = vld1q_f32(sp);  sp += 4;      v2f = vld1q_f32(sp);  sp += 4;
      v3f = vld1q_f32(sp);  sp += 4;      v4f = vld1q_f32(sp);  sp += 4;
      v1f = vmlaq_n_f32(voff,v1f,scale);  v2f = vmlaq_n_f32(voff,v2f,scale);
      v3f = vmlaq_n_f32(voff,v3f,scale);  v4f = vmlaq_n_f32(voff,v4f,scale);
      v1 = vcvtq_u32_f32(v1f);            v2 = vcvtq_u32_f32(v2f);
      v3 = vcvtq_u32_f32(v3f);            v4 = vcvtq_u32_f32(v4f);
      v1 = vminq_u32(v1,vec_max);         v2 = vminq_u32(v2,vec_max);
      v3 = vminq_u32(v3,vec_max);         v4 = vminq_u32(v4,vec_max);
      vs1 = vcombine_u16(vmovn_u32(v1), vmovn_u32(v2));
      vs2 = vcombine_u16(vmovn_u32(v3), vmovn_u32(v4));
      vst1q_u16(dp,vs1);  dp += 8;        vst1q_u16(dp,vs2);  dp += 8;
    }
  
  // Backtrack 0 to 15 positions so we can use vector processing also for the
  // final output samples.  This generally involves unaligned loads and stores.
  int backtrack = ((-width) & 15);
  dp -= backtrack; sp -= backtrack;
  v1f = vld1q_f32(sp);  sp += 4;      v2f = vld1q_f32(sp);  sp += 4;
  v3f = vld1q_f32(sp);  sp += 4;      v4f = vld1q_f32(sp);  sp += 4;
  v1f = vmlaq_n_f32(voff,v1f,scale);  v2f = vmlaq_n_f32(voff,v2f,scale);
  v3f = vmlaq_n_f32(voff,v3f,scale);  v4f = vmlaq_n_f32(voff,v4f,scale);
  v1 = vcvtq_u32_f32(v1f);            v2 = vcvtq_u32_f32(v2f);
  v3 = vcvtq_u32_f32(v3f);            v4 = vcvtq_u32_f32(v4f);
  v1 = vminq_u32(v1,vec_max);         v2 = vminq_u32(v2,vec_max);
  v3 = vminq_u32(v3,vec_max);         v4 = vminq_u32(v4,vec_max);
  vs1 = vcombine_u16(vmovn_u32(v1), vmovn_u32(v2));
  vs2 = vcombine_u16(vmovn_u32(v3), vmovn_u32(v4));
  vst1q_u16(dp,vs1);  dp += 8;        vst1q_u16(dp,vs2);  dp += 8;
}
  
} // namespace kd_supp_simd

#endif // !KDU_NO_NEON

Changes from KDU-7.8 to KDU-7.9
-------------------------------
1. Introduced CBR (constant-bit-rate) codestream flushing capabilities to
   support low latency applications.  While Kakadu has supported incremental
   codestream flushing for a very long time, this has always been subject to
   rate or quality objectives that are defined relative to the entire image
   or video frame.  This means that many more coded bits might be allocated
   to the top part of a frame than the bottom, or vice-versa.  This meant
   that it was not possible to explicitly constain end-to-end delay in
   the context of a constant (or limited) bit-rate communication channel.
   This version addresses these weaknesses as follows:
   a. A new `Scbr' codestream parameter attribute is provided, as part of
      the `siz_params' complex.  This option allows a constant bit-rate
      communication channel to be defined and communication delay to be
      constrained via a classic leaky-bucket model.  The attribute effectively
      defines a flush interval, so that the PCRD-opt algorithm is run
      once every flush interval, adjusting the distortion-length slope
      as smoothly as possible while ensuring that the leaky bucket
      communication model (models a limited size bit-buffer between encoder
      and decoder) neither overflows nor underflows.
   b. All code-stream flushing operations are modified in the presence of
      the `Scbr' option.  Separate versions of the pcrd-opt rate control
      process, packet construction and codestream generation functions are
      provided for the CBR case, so that the regular versions remain as
      efficient as before, while the CBR versions are also optimized for
      efficiency.
   c. The CBR variants of the JPEG2000 packet construction functions
      have the ability to insert "padding" bytes, which will be disregarded
      by any compliant JPEG2000 decoder.  The padding bytes are an important
      capability for CBR codestream flushing, since otherwise there is no
      way to guarantee that a fixed size bit-buffer will not underflow
      when serviced by a constant data rate communication channel.
   d. CBR flushing is orthogonal to incremental codestream flushing, which
      has existed for a long time.  If both are used, the CBR flushing
      constraints are respected while the content is emitted progressively,
      as it is generated.  This allows actual low latency operation to be
      realized.  If CBR flushing is specified via `Scbr' but incremental
      flushing is not configured, the CBR constraint applies to the
      generated codestream, as if it had been emitted incrementally, but
      the actual flushing of content to the output file (or other data
      sink) is performed at the end of each image/frame.
   e. A new `kdu_flush_stats' interface is introduced as part of the
      core codestream management machinery, which is used to exchange
      information between consecutive codestreams in a video sequence
      so as to improve the behaviour of CBR-constrained codestream
      flushing.  This interface is of interest to video applications such as
      "kdu_v_compress" and "kdu_vcom_fast".  The `kdu_flush_stats' interface
      is ready to be extended in future releases to assist high performance
      applications involving modified high throughput, versions of the block
      coding algorithm.
   f. CBR flushing can be performed via "kdu_compress", "kdu_v_compress" and
      other Kakadu demo appls, but the "kdu_v_compress" application has
      specifically been extended to demonstrate CBR flushing capabilities.
      The `kdu_flush_stats' interface is used to exchange flushing
      information between consecutive codestreams that might be processed
      asynchronously.  Moreover, the MJC format (see below) can be targeted
      to write files in which each video frame's codestream occupies exactly
      the same number of bytes.
   g. The `kdu_simple_video_source' and `kdu_simple_video_target' classes
      defined within "kdu_video_io.h" have been extended, along with the
      associated simple compressed video file format (known as MJC within
      Kakadu), so as to support CBR video compression.  Specifically, the
      file header can include a new flag `KDU_SIMPLE_VIDEO_CBR', meaning
      that each codestream occupies a fixed size within the file and the
      coded data within each codestream appears at a constant rate
      (bits per image line) subject to the leaky bucket model (bit-buffer)
      meantioned earlier.  MJC files with this CBR flag support seeking
      to frames and other functionalities normally available only in
      richer formats like MJ2.  MJC files are intended mainly to demonstrate,
      using files, what would happen in a real-time communication system
      where coded content is communicated to a receiver immediately, without
      intervening files.
   h. The "kdu_merge" demo app has been augmented to support injest of
      one or more MJC files, so that they can be merged into MJ2 or JPX
      files.  The "kdu_merge" tool plays an important role in unifying the
      different types of compressed outputs that Kakadu can produce.  Using
      it in conjunction with kdu_v_compress", "kdu_compress", "kdu_expand"
      and "kdu_v_expand" allows media to be moved from just about any
      format to any other format, entirely via the command-line (or scripts).

2. Introduced new parameter attributes to help configure Part-2
   DWT kernels and quantization step sizes with particular properties of
   interest.  The new attributes are: Kkernels; Qfix16; and Qweights.
      -- Kkernels provides a dead easy way to set up any of a number of
         now-predefined Part2 wavelet kernels, including reversible and
         irreversible versions of the Haar (or S-transform), and the
         irreversible 5/3 transform.  The latter is particularly
         interesting for ultra low latency compression, since its compression
         performance is substantially better than the reversible 5/3,
         especially at higher bit-rates, unless truly lossless compression
         is required.  Prior to Kkernels, configuration of Part 2
         wavelet transform kernels required a number of attributes to
         be configured, using some knowlege of the associated signal
         processing definitions.
      -- The Qfix16 option adjusts quantization step sizes automatically,
         so as to minimize numerical noise that occurs if encoding and/or
         decoding is performed using fixed-point arithmetic in place of
         floating-point.  Fixed-point processing is generally faster and
         presents a lower memory footprint, so reducing all sources of
         extra distortion when using fixed-point processing is of interest.
         While the effect of Qfix16 could be achieved before, by careful
         selection of Qabs_steps parameters, this could only be done by
         an expert, especially with wavelet transforms having non power-of-2
         coefficients.
      -- The Qweights attribute helps reduce imbalance in the number of
         discarded coding passes across subbands when visual weighting is
         employed during rate control.  This allows for more efficient
         compression, especially if Qstep is used in conjunction with the
         default PCRD-based rate control, to avoid unnecessary computation
         while providing a guaranteed compressed data rate.  Again, the
         Qweights behaviour could have been obtained before, by carefully
         choosing Qabs_steps parameters, but this was far from simple,
         especially with complex transform structures.

3. SIMD accelerators for quantization and dequantization operations have
   been improved in two ways.  First, the accelerations are now available
   for code-blocks whose actual dimensions are not exactly divisible by
   the vector length, or which are being cropped on the fly to accommodate
   region-of-interest rendering.  This is not all that important for
   square code-blocks, but it is more important for short-wide code-blocks
   (e.g., 4 high by 1024 wide) which are of interest in low latency or
   low memory applications.  Second, highly efficient accelerated
   implementations are provided for AVX2-capable platforms.  Irreversible
   dequantization may be able to reach throughputs of around 0.25 clocks
   per sample, while irreversible quantization might take slightly longer.
   These very high throughputs are unlikely to have a significant impact
   on most applications at present, but their effect will be more
   noticeable when we introduce a much faster block coding algorithm as
   part of an upcoming release.

4. Introduced some minor improvements to the core system to support
   block coder optimizations (and new block coding algorithms that
   may appear in future versions).  These are as follows:
   a. The sample buffer and coded byte buffer resources offered via
      `kdu_block' now have desirable vector alignment properties and
      extra readable/writable regions, sized to the natural vector
      dimensions that are defined in "kdu_arch.h".
   b. The `kdu_mmx_level' value of 5 (for x86 processors), which previously
      advertised support for SSE4.1 only, now means that SSE4.2 and POPCNT
      are also supported -- all higher MMX levels also include POPCNT.
      To do this, the run-time architecture tests have been modified, so
      some CPU's which previously allowed the MMX level to reach 5 might
      no longer allow levels beyond 4, but this will not happen on any
      modern CPU.
   c. The scalar type definitions in "kdu_ubiquitous.h" have been augmented
      to include `kdu_uint64'.

5. Miscellaneous improvements: 
   a. Augmented the `kdu_region_animator' to provide recommendations about
      the number of quality layers to decompress in order to optimize
      a quality-vs-throughput objective when redering high frame rate
      animations.  Kakadu is fast enough that this should only be
      important for low power mobile devices with high resolution
      displays.
   b. Slightly modified `kdu_region_compositor' to determine not only the
      maximum number of resolution levels that can be discarded from
      a codestream, but also the maximum number of resolution levels that
      are non-empty (non-zero area) so that it does not make the mistake
      of accessing resolution levels that formally exist but have no
      sample data.  This can happen in certain pathalogically constructed
      codestreams.  Previous implementations handled such situations by
      generating formal errors, but we now detect and avoid accessing
      such resolution levels.
   c. Modified "args.cpp" so as to remove line length limitations for
      switch ("-s") files provided to the various demo-apps.  This is
      especially helpful for hyperspectral image compression applications
      where parameters describing multi-component transforms can be
      very extensive.
   d. Modified `kdu_fseek' and `kdu_ftell' implementations on 64-bit Windows
      platforms to use `_fseeki64' and `_ftelli64' rather `fsetpos' and
      `fgetpos'.
   e. Addressed a host of compiler warnings generated by the more rigorous
      tests that are performed by recent C++ compilers.

6. Bug Fixes (some very important!):
a. Small changes made while upgrading the XT version of Kakadu to version
   7.8, during which various typos and minor inconsistencies were noticed.
b. Fixed a small bug in `jp2_source', whose constructor failed to initialize
   some of the member variables tested by `jp2_source::read_header'.  This
   only becomes a problem if a valid `jp2_source::open' call is not issued
   first.
c. Corrected a minor oversight in `kdu_region_compositor::configure_overlays',
   where a surface buffer's completeness status might be reset when the
   overlay rendering conditions change even if the conditions do not
   affect any of the actual rendered content for the buffer so that the
   processing is noted as still being complete.  This may lead some
   applications that monitor completeness status within the buffers
   themselves to believe the buffer's content should not be rendered to
   a display, even though it is fully processed.  A one line fix has
   been introduced to avoid this possibility.
d. Fixed minor errors in the experimental makefiles that build for ARM
   under Linux, with NEON accelerations.  These have now been tested
   at least on a Cortex A7 platform.
e. Added an Arm-for-Linux branch to the architecture specific
   initialization logic in "kdu_arch.cpp".
f. Fixed minor errors in the accelerator functions in "neon_dwt_local.cpp"
   for 4tap Part-2 vertical analysis and synthesis filters at 32-bit
   precision, where s16 vector initializers were used in place of s32.
   Some compilers would have complained about this, while others silently
   let it go.
g. The "kdu_show" apps now check for exceptions in calls to their
   `kd.._region_compositor::get_composition_buffer_ex' functions whenever
   they temporarily lock a mutex, to make sure that deadlock cannot
   occur if an exception-throwing error is detected inside
   `kdu_region_compositor::get_composition_buffer' -- can happen if
   something is seriously wrong with a codestream, for example.
h. Corrected a tiny, yet very significant bug introduced into the
   handling of opacity channel descriptions in version 7.8.  The
   correction is in function `jp2_channels::set_opacity_mapping', where
   the opacity channel's component index accidentally overwrote the
   colour channel's component index.
i. Corrected a problem that affects transfers from `kdu_region_decompressor'
   to 16-bit output buffers, the problem having been accidentally introduced
   in version 7.8 along with the extensive changes related to true-max
   and true-zero colour intensity scaling policies.
j. Fixed a bug that has been present in each of the "kdu_show" applications
   (kdu_macshow, kdu_winshow and the mobile version kdu_padshow), where
   `kdxx_region_compositor::get_composition_buffer_ex' mistakenly returned
   the head of the composition buffer queue to the rendering agent instead
   of the incomplete buffer to which content should be written.  This error
   affects only animations and only becomes noticeable in certain cases,
   depending on the scheduling of display and rendering threads.
k. Fixed a bug in `kdu_region_animator::advance_animation', that could
   under rare conditions result in NULL pointer dereferencing during the
   trimming of out-of-date frame requests -- affects only JPIP video/animation
   applications.
l. Fixed a bug in "multi_transform.cpp" that was introduced with vector
   accelerations for matrix multi-component transforms.  The bug was not
   in the accelerations themselves, but rather their invocation in the
   case where the number of input and output components to a matrix block
   are different.  The bug affected multi-component compression (not
   decompression) in cases where Scomponents and Mcomponents are different.
m. Fixed a bug in the parsing and digestion of channel bindings for JPX
   compositing layers that are formed from multiple codestreams; the bug
   is associated with an error message that was introduced into version 7.8
   to help catch and explain illegal file format constructions, but
   the code failed to wait until all codestreams associated with a
   compositing layer had been considered before reporting an
   error related to incomplete channel binding information.
n. Fixed a bug in the data format information returned by
   `jp2_channels::get_opacity_mapping' and the similar `get_premult_mapping'
   function.  The data format information was introduced with version 7.8
   to support alternate channel formats (especially true float formats).
   The bug is excited where a single codestream component "broadcasts"
   its opacity to all colours and there is no explicit format information
   provided.  In these cases, the code failed to broadcast the data format
   as well as the channel binding information.  This bug could cause a problem
   during compositing of such content within `kdu_region_compositor'.
o. Fixed a subtle bug in the interaction between NLT parameter parsing in
   kdu_expand (for consistency verification) and NLT parameter dimensioning
   in the core codestream machinery, which occurred because the dimensioning
   of the NLT parameter cluster was done in a lazy way for codestreams that
   do not have the NLT feature.  The problem can only manifest under a complex
   combination of conditions.  While the core system does not exactly have
   a bug here, we chose to fix the problem within the core system so that
   applications do not need to understand the intricacies of the parsing
   problem created by NLT/MCT interaction.
p. Fixed a small bug in the implementation of `kdu_simple_video_source::seek'
   and `kdu_simple_video_source::get_pos'.
q. Fixed a race-like error (multi-threading dependent, but problem occurs
   due to the interaction of statements protected by the same critical
   section) that could occur when decoding codestreams without random access
   information, if the decoding process is fast enough to complete all work
   as soon as a background codestream parsing process completes.  The problem
   arose due to a neglected condition inside `kd_tile::finished_reading',
   which was intended to release any empty precincts (very rare), but may have
   released fully consumed precincts that were separately sitting on an
   asynchronously updated pending-release queue.
r. Modified "kdu_compress" to generate an error message if horizontal
   tiling is present in conjunction with the "-rgb_to_420" option, which
   it is not designed to handle.
s. Fixed a bug in `kdu_region_decompressor' which caused faulty behaviour
   when rendering content at resolutions more than a factor of 3 below
   the lowest native resolution available by discarding resolution levels
   from the compressed content.  The bug was accidentally introduced with
   the rationalization of rendering and rendering acceleration logic in
   version 7.6.  Specifically, the bug fix involved changing `KDRD_FLOAT_TYPE'
   to `KDRD_FIX16_TYPE' in the line-buffer width adjustment tests within
   `kdu_region_decompressor::make_tile_bank_current', where internal
   line buffers are dimensioned and allocated.

Changes from KDU-7.7.1 to KDU-7.8
---------------------------------
This release is focussed on features that support Part-2 non-linear point
transforms and the Part-2 Pixel Format (pxfm) box (introduced in Ammendment 3),
which are of particular interest for High Dynamic Range (HDR) compression
and rendering applications.  Additional support is provided for careful
mapping of intensity ranges in content rendering applications with a
diversity of sample precision values, for high dynamic range colour conversion,
and for the preservation of colour intensities that lie beyond the nominal
maximum values (i.e., superluminous values).  Additional SIMD accelerators
are provided for these applications, and for general Part-2 multi-component
transforms.  The specific updates are listed below.
1. Added support for the Part-2 Non-Linear Point Transforms (NLT) feature to
   the core coding system.
   a. New parameter attributes `NLType', `NLTgamma', `NLTlut', `NLTdata' and
      `NLTmake' have all been added, where the last one provides a very
      convenient tool to construct non-linear point transforms as a cascade
      of forward or reverse gamma and custom log-like transforms.
      Together, you can use these to build non-linear point transforms that map
      gamma corrected data back to the linear domain and then forward again to
      a logarithmic domain, or vice-versa, which can be very helpful.
   b. The sign-magnitude to 2's complement conversion non-linear point
      transforms added with IS15444-2/AMD3 are also supported; these are
      designed to work with true floating point data compression, where
      a wide range of floating point representations can be re-interpreted
      as integers and compressed efficiently with the aid of this type of
      NLT.
   c. Added coding parameter attributes `Ncomponents', `Nprecision' and
      `Nsigned', which mirror the roles played by `Mcomponents', `Mprecision'
      and `Msigned' as well as those of `Scomponents', `Sprecision' and
      `Ssigned'.  The S... parameters describe so-called codestream image
      components that are the subject of spatial transformation, quantization
      and coding processes.  The M... parameters describe so-called
      output image components that are produced (during decompression) by
      applying any Part-2 inverse multi-component transform.  The N...
      parameters describe the final output components that are produced
      (during decompression) by applying any inverse non-linear point
      transforms to the components described by the M... parameters.  The
      M... attributes exist only if there are Part-2 multi-component
      transforms, but the N... and S... attributes exist always.  The
      value of Ncomponents is always the final number of output components
      that can be generated by a decompressor, which will either be equal to
      Mcomponents (if it exists and is non-zero) or Scomponents.  For
      Part-1 codestreams, all the N... attributes are guaranteed to be
      identical to the S... attributes.
   d. While the introduction of Nprecision and Nsigned attributes may seem
      to complicate things, many of the parameter attributes are
      automatically instantiated if not set.  The one thing you need to bear
      in mind is that compressors are best advised now to configure the
      Nprecision and Nsigned attributes explicitly based on original image
      characteristics, rather than configuring Sprecision and Ssigned or
      Mprecision and Msigned, as was the case before.  The upshot is that
      things are now usually simpler, because a compression application
      does not need to determine whether the M... or S... attributes
      correspond to the original image precision and signed/unsigned
      attributes -- it just configures the N... attributes and lets the
      rest happen automatically.
   e. Forward and reverse non-linear point transformation is handled
      automatically by the powerful `kdu_multi_analysis' and
      `kdu_multi_synthesis' objects that lie at the heart of almost all
      compression or decompression applications.  These objects also
      take care of all Part-1 and Part-2 multi-component transforms, so
      virtually no changes are required to a compression or decompression
      application in order to take advantage of the new Non-Linear Point
      Transform features.
   f. It is worth noting that you can define an inverse multi-component
      transform that produces more decompressed output components than
      there were input components.  A major use for this is the definition
      of output components that are formed by applying various transformations
      to the original compressed content, so as to provide a variety of
      different presentations of the content.  These presentations can
      include various linear and affine combinations of the original
      components, but they can also now include any number of different
      non-linear point transformations of the content.  So, for example,
      you could provide a variety of linear, gamma-corrected or log-mapped
      representations of different linear combinations of the original
      content as output components from the codestream.  The JPX file
      format features can then be used to describe compositing layers,
      animated presentation tracks and so forth, that use these components
      in interesting ways. 
2. Added support for the JPX Pixel Format (pxfm) box that was introduced with
   Ammendment 3 to IS15444-2.
   a. The most significant feature of the pixel format box is that it allows
      the file format to indicate that output image components from the
      codestream produce sample values whose integer bit-patterns are to be
      re-interpreted as floating point numbers.  This, together with the
      sign-magnitude to 2's complement non-linear point transform (NLT)
      feature offered by Part-2 codestreams, allows you to efficiently
      compress floating point data directly.  Kakadu supports lossless
      compression of floating point data with up to 28 bits of precision,
      where the division of these bits into exponent and mantissa parts
      is arbitrary.  For example, you can compress 16-bit half floats, or
      you can compress true 32-bit IEEE floats with a reduced exponent
      range or a reduced mantissa precision, without any loss.  Irreversible
      compression of floating point data in an effectively logarithmic
      representation (created by treating the foating point bit-patterns as
      sign-magnitude integer bit-patterns) is also supported of course.  For
      irreversible true floating point compression using the pixel format
      box and sign-magnitude NLT, we recommend that you use as few exponent
      bits as you can afford, so as to maximize the numerical precision with
      which the mantissa is compressed.  Denormalized numbers and NaN's are
      correctly handled with any number of exponent bits, so in practice
      4 or 5 exponent bits is usually enough to address most applications.
   b. A second feature of the Pixel Format box is that it allows the
      integer bit-patterns associated with decompressed output image
      components to be re-interpreted as fixed-point numbers with a
      custom number of integer bits (or equivalently fraction bits).
   c. A third option with the Pixel Format box is to split the representation
      of colour intensity values across two compressed components, one
      representing a basic intensity value, while the other represents
      an exponential scaling factor (usually common to multiple components).
      This option primarily targets RGBE representations of high dynamic
      range content, but may have broader application.  Currently, this
      option is supported only at the file format level -- none of the
      data processing tools provided by Kakadu can current handle this
      so-called "split-exponent" format, but the file format interfaces
      allow it to be specified and identified.
   d. The float and fixpoint pixel formats are suitable for the
      compression of HDR content.  In particular, both offer the ability
      to represent super-luminous regions correctly, without loss of
      gamut due to saturation.  In both cases, the nominal maximum
      intensity is understood to have a value of 1.0, but this is not
      the largest value that can be represented.
   e. A the application level, there is no explicit interface to anything
      resembling the Pixel Format box.  Instead, the creation, parsing
      and interpretation of this box is handled automatically via
      extensions to the existing `jp2_channels' member functions.  In
      particular, `jp2_channels::set_colour_mapping',
      `jp2_channels::get_colour_mapping', `jp2_channels::set_opacity_mapping',
      `jp2_channels::get_opacity_mapping' and so forth, all now provide
      additional arguments.  These arguments almost all have defaults so
      as to maintain backward compatibility with existing applications.
      However, this is not quite true of the `jp2_channels::get_...'
      functions, for which an extra non-default by-reference argument
      (`format') is now required.  This design is deliberate so as to
      alert application developers of the fact that the colour or
      opacity channels produced during rendering might have a non-default
      interpretation (or format).  Handling these new formats is quite
      easy, though, and common interfaces like `kdu_region_decompressor'
      and `kdu_region_compositor' handle them transparently.
3. The float and fixpoint re-interpretations of codestream output
    components that are offered via the Pixel Format box are both of
    interest for compression of HDR (High Dynamic Range) content and
    Kakadu now provides considerable support for such applications, as
    follows:
    a. The kdu_compress demo-app can compress floating point data directly,
       reading it from a raw of floating-point TIFF file, or converting
       other image file formats to floating point during the compression
       process.  These capabilities are supported via extensions to the
       "-fprec" command-line argument, whose role has always been that
       of forcing a particular numerical representation to be used during
       compression ("fprec" stands for "forced precision").  You can
       explicitly identify the number of exponent bits to be used in a
       floating point representation, along with the overall precision
       (and hence indirectly the number of mantissa bits).  Original
       source content, be it floating point or integer-valued can be
       converted into any floating-point representation with up to
       32 bits of precision (irreversible) or up to 28 bits of precision
       (reversible -- 29 can actually be achieved with care in parameter
       selection).  The "kdu_compress" application automatically introduces
       non-linear point transforms of the sign-magnitude variety, as well
       as a Pixel Format box, when writing a JP2-family file (as opposed to
       a raw codestream), so that the compressed data will be correctly
       interpreted by suitably equipped renderers, such as "kdu_show"
       or "kdu_render".
    b. The kdu_expand demo-app can re-interpret decompressed image components
       as floating point bit-patterns, writing the appropriate results to
       output image file(s).  This is all done automatically if the
       compressed source is embedded in a JPX file with a suitable
       Pixel Format box.  If a raw codestream is being decompressed, the
       user must explicitly supply the number of exponent bits in a
       float-reinterpreted representation, because this information is
       not identifiable from the codestream itself.  However, warning
       messages are produced if a sign-magnitude non-linear point
       transform is found in the codestream without any identification of
       a floating-point format by the user or via a Pixel Format box, since
       the use of the sign-magnitude point transform is almost certainly
       limited to the compression of true floating point data.
    c. The `kdu_region_decompressor' and associated `kdu_channel_mapping'
       objects have been augmented to support the float and fixpoint
       pixel formats offered by the underlying pixel format box.  By and
       large all these formats are handled transparently, so long as
       you are using the standard `kdu_channel_mapping::configure' functions.
       All the `kdu_region_decompressor::process' functions return values
       that have a consistent interpretation, regardless of the underlying
       pixel format, but the floating point `process' variants are able
       to preserve the additional head-room offered by underlying
       float-formatted or fixpoint-formatted data without clipping.  This
       is achieved via an extra argument `always_clip_outputs' that
       defaults to true for the benefit of applications that do not offer
       specific support for HDR content.
    d. Other tools like "kdu_merge" support the preservation of Pixel Format
       box information when merging JPX compositing layers from one or more
       source file(s) into an output JPX file.
4. Additional SIMD acceleration paths have been added for both x86 and
   ARM/NEON platforms, as follows:
   a. Multi-component irreversible matrix block transforms are now
      accelerated at both floating-point and 16-bit fixed-point precisions.
      Note that most multi-component wavelet transforms of interest already
      had accelerated implementations.  The addition of matrix transforms
      means that colour transforms, plus any of the block transforms
      commonly used with hyperspectral data are now all implemented via
      SIMD accelerators on all platforms of interest.
   b. Wherever offsets are added to image components within a multi-component
      transform, this is now done with SIMD vectorized accelerators; this
      is a common operation in many applications of the JPEG2000 Part-2
      multi-component transform.
   c. The Part-2 non-linear point transforms associated with transforms
      that convert between 2's complement and sign-magnitude
      representations (used for true-float compression) are also
      accelerated when used at 32-bit precision (lower precision paths
      are supported, but are not expected to be used much for true-float
      compression applications).
   d. Data paths in `kdu_region_decompressor' that reinterpret integer
      bit-patterns as custom floating point values based on the new Part-2
      Pixel Format (pxfm) box are also processed via SIMD accelerators on all
      significant platforms.  This ensures high processing throughput when
      rendering content that has been compressed as true-floats, as
      explained above.
5. The `jp2_palette' interface has been augmented in order to support
   the re-interpretation of palette lookup table entries as floating-point
   or fixed=point bit-patterns based on the information that might be
   found in a JPX Pixel Format box.  In particular, `jp2_palette::get_lut'
   now comes in two forms (16-bit fixed-point and 32-bit floating-point
   precision) and offers optional auxiliary arguments that exactly match
   the extra pixel-format information that is available from the augmented
   `jp2_channels::get_colour_mapping' and related functions.  By passing
   the pixel format through to `jp2_palette::get_lut', there is no need
   for a rendering application to worry about the impact of special
   pixel formats on palettes.
6. The `jp2_palette' interface has also been augmented with new
   `get_abs_lut' functions that can be used to return absolute integer
   representations of the palette entries, without normalization or
   float-/fixpoint- reinterpretation.  These can be useful for applications
   that need to recover the exact palette entry bit-patterns.  In particular,
   the "kdu_expand" demo-app now uses these `get_abs_lut' functions instead
   of the normalized `get_lut' functions in order to do palette-based
   mapping of decoded image components.  This is useful because it allows
   the image file writing modules to handle all re-interpretation of
   bit patterns coming from either a palette or an unmapped codestream
   output image component, in a uniform way.  Basically, you now have all
   possible interesting options available for recovering and using
   palette lookup tables that may exist in a source file.
7. The `jp2_colour_converter' object's colour conversion functions have
   all been augmented to support both 16-bit fixed-point and 32-bit
   floating point precision colour conversions, where previously only
   the lower precision conversions were offered.  When a colour converter
   is instantiated in the wide-gamut mode, the floating point precision
   conversions provide much extended-gamut colour support, allowing
   channel intensities up to 8 times the nominal maximum to be correctly
   processed through forward and reverse tone curves.  These higher
   precision colour conversions are used automatically by the
   `kdu_region_decompressor' object if one of the new non-default
   pixel formats is used, having the capability to represent HDR content.
8. The `kdu_region_decompressor' and `kdu_channel_mapping' and
   `kdu_region_compositor' objects have also been augmented to provide
   applications with more control over the way in which intensities are
   mapped across different numerical representations.
   a. In simple cases, original colour spaces defined with respect to
      8-bit sample values are compressed with 8-bit component samples.
      However, JPEG2000 formats allow the sample values to be compressed
      at much higher, or potentially lower precision, and they also allow
      signed representations to be used to compress intensity values that
      are nominally unsigned, or chominance values that are formally
      defined with respect to unsigned integers.  This means that the
      rendering operations performed by the workhorse
      `kdu_region_decompressor' object need to be able to appropriately
      map content across different bit-depths and sometimes between
      signed and unsigned representations, without losing the
      correct colorimetric interpretation of the content.
   b. Prior to this release, Kakadu managed such transformations only
      approximately (although still quite accurately), preferring to
      use power-of-2 scaling factors and level adjustments where the
      most accurate values might differ slightly from the nearest power
      of 2 and making up for the deficiencies of this approach by
      providing the `kdu_region_decompressor::set_white_stretch' function
      that is called automatically from within `kdu_region_compositor'.
      Also, prior to this release, signed content was mapped to unsigned
      values by mapping 0 to a mid-level intensity, rather than 0.  These are
      still the default procedures, and indeed there are good reasons
      why one may prefer them.
   c. For colorimetric correctness, however, `kdu_region_decompressor' now
      provides a `set_true_scaling' function which is mirrored by
      `kdu_region_compositor::configure_intensity_scaling',
      whereby you can independently adjust the way the zero and maximum
      levels are mapped across different representations.
   d. To support colorimetrically correct mapping of component samples,
      it is important to take into account the properties of the
      associated colour space.  For example, black is not always
      represented by 0, and the achromatic point for a chrominance
      component is not always (in fact it is almost never exactly) half
      way between the component's minimum and maximum values.  In view
      of this, the `jp2_colour' interface has been augmented with the
      function `jp2_colour::get_natural_unsigned_zero_point', that is
      used by `kdu_channel_mapping' and hence by higher level objects like
      `kdu_region_compositor' in order to correctly configure intensity
      mapping operations for the new true-scaling modes.
9. The "kdu_render" demo application now takes two extra command-line
   options "-true_zero" and "-true_max" that can be used to explore
   the impact of the true intensity scaling options mentioned above
   on rendered content.  These options work for all of the rendering
   approach demonstrations that are implemented by "kdu_render", so you
   can see how best to apply the new options in each case.  As mentioned
   above, these new options should have no impact at all in simple
   (and common) situations where no precision or signed/unsigned
   conversions are required.
10. It should be noted that at this stage, the `kdu_stripe_compressor' and
   `kdu_stripe_decompressor' interfaces do not provide explicit support
   for re-interpreting compressed bit-patterns as floating point or
   fixed point values, unlike `kdu_region_decompressor' which does this
   transparently.  This will not be difficult to add to a future release,
   but for those who wish to compress and render HDR or scientific content
   based on these new pixel formats, the tools are available as mentioned
   above, assuming your compression approach follows that of "kdu_compress"
   and your rendering approach is based on `kdu_region_decompressor' or
   `kdu_region_compositor', as is the case for the "kdu_render" and
   "kdu_macshow"/"kdu_winshow" demo apps, as well as our IOS application
   "kdu_padshow".
11. The powerful `kdu_region_compositor' that underpins many interactive
   image rendering applications has been modified slightly to watch for
   attempts to allocate composition buffers that are ridiculously large.
   Exactly what is meant by "ridiculously large" depends on two macros
   defined at the start of "region_compositor_local.h".  Currently,
   the maximum compositor buffer is limited to 16 Giga-pixels and the
   maximum buffer width is limited to 256 Mega-pixels.  Exceeding these
   dimensions results in a meaningful error through the usual `kdu_error'
   mechanism, that allows error conditions to be caught by the application.
   These dimension bounds are generally helpful and should never prove a
   hindrance to a reasonable application.  This is because the
   `kdu_region_compositor' object is intended to be used to render only
   a portion (region of interest) of a huge image to memory.  You can
   always render the entire image piecewise in this way if desired, but
   the usual intent is to support interactive rendering and viewing, for
   which a 16 Giga-pixel surface buffer is already on the ridiculous side.
12. The public classes found in "kdu_compress/roi_sources.h", that are
   compression applications involving a region of interest, are now
   assigned non-native language bindings (Java, C# and VB) and are
   exported via the standard managed auxiliary library "libkdu_a...".
   This is in part to facilitate better build environments for the
   popular GDAL toolkit.
13. Fixed all security issues reported to-date by Google's Security team --
   special thanks to Haris Andrianakis for his recent security report.
   It is not clear if any of these security issues could have been
   exploitable.  They are all generated by source files that are either
   deliberately corrupt or else push well beyond the boundaries of
   what can be considered reasonable.  The most important fixes are, as
   follows -- none of these should have any impact on existing applications,
   except to make them more robust.
   a) Fixed a weaknesses in the second form of the overloaded
      `kdu_codestream::apply_input_restrictions' function, to both check
      for negative component indices and also eliminate the possibility
      that illegal or repeated components might lead to an invalid
      number of apparent output image components being recorded internally;
      this had the potential to cause other interface functions to succeed
      where they should fail under error conditions.
   b) Fixed an exception handling error in the core multi-threaded
      sub-system functions `kdu_thread_context::acquire_lock' and
      `kdu_thread_context::try_lock', where the testing for exceptions
      generated by other threads was erroneously placed before lock
      acquisition; it now occurs after lock acquisition, which was
      always intended.  This fixes a problem where applications might
      crash, rather than correctly throwing and catching exceptions,
      where errors are generated asynchronously in different threads within
      the core codestream machinery.
   c) Replaced a number of assert() statements in the core codestream
      machinery with additional error generating calls that will properly
      terminate an application, or cause exceptions to be thrown and robustly
      caught, when unexpected illegal conditions are detected.
   d) Introduced additional consistency checks into the powerful
      `kdu_region_decompressor' and `kdu_region_compositor' applications
      that rely upon interaction between the codestream and file-format
      information in JP2/JPX/MJ2 files.  These additional checks look for
      pathalogical situations such as codestream image components that are
      referenced by the file format, but do not actually exist, and
      codestream components that are required for a rendering operation
      but are entirely empty -- it turns out that the JPEG2000 standard
      does actually allow image components to appear which have no samples
      whatsoever, but this requires sub-sampling factors that are at least
      the span of the image on the high resolution canvas, coupled with
      non-trivial image offsets.
   e) Fixed a bug in the sample data conversion operations invoked by
      `kdu_region_decompressor' which can be excited if a codestream
      image component has a sub-sampling factor that is larger than the
      tile width and either a massive number of tiles or non-trivial
      image or tile offsets are present.
   f) Improved the way in which the JPEG2000 packet header parsing machinery
      handles ridiculous bit patterns (legal, but almost certainly the
      result of corruption).  Additional conditions now result in the
      throwing of exceptions that will generate meaningful termination
      errors, which in turn either terminate an application or throw
      `kdu_exception' exceptions to the application, depending on how
      the application has configured error handling.

Changes from KDU-7.7 to KDU-7.7.1
---------------------------------
1. Introduced a "-palette" argument to "kdu_merge" that allows it to
   install a custom colour palette described on the command-line, as
   a convenience for synthesizing JPX files with specific properties of
   interest.
2. Fixed a weakness with "kdu_server" that particularly affects the handling
   of one-shot JPIP requests over slow links (or when "-max_rate" is
   configured to be small).  Previously, when the server became aware that
   the channel being used would not longer be needed after completing a
   request, it installed a (usually short) timeout, waiting for the holding
   queue to empty or the timeout to occur before closing the channel.  It
   could easily happen, however, that this timeout was too short.
   Lengthening the timeout is a solution (it is configurable via the
   command line) but does not adequately address the situation in which
   a lot of data may be requested and the link may be slow -- configuring
   very long timeouts would cause the server to waste its resources if
   the connection becomes non-responsive for some reason, which is the
   primary reason for timeouts.  The solution now implemented, involves
   replenishing timeouts, that extend the timeout each time progress is
   made in the delivery of content from the holding queue.
3. Recent changes in the channel state estimation and flow control
   machinery within "kdu_server" were well adapted to the HTTP-UDP
   transport, but less suitable for use HTTP-TCP.  For this reason,
   we have re-instated the older channel estimation and flow control
   algorithm as the default for HTTP-TCP, with the newer (low delay)
   algorithm used only with HTTP-UDP by default.  However, you can
   override this from the command line via the "-low_delay_tcp"
   switch, which causes the low delay algorithm used for HTTP-UDP to be
   employed also with HTTP-TCP.

4. Bug fixes:
a. Fixed a tiny bug in the handling of reversible MATRIX
   multi-component transforms during compression.  The bug was
   accidentally introduced while fixing a separate problem, in
   the transition from version 7.4 to 7.5 or 7.5 to 7.6 (to be
   precisely confirmed) into the `kd_mct_stage::create_stages'
   function.  In particular, if (tnum < 0), that function used
   to skip the conditional overwriting of the Mxform_MATRIX transform
   identifier with Mxform_MAT, subject to no Cmct flags being found
   (a step to preserve backward compatibility with early non-conformant
   codestreams).  The (tnum < 0) condition was later commented out
   to allow proper discovery of the number of output image components
   produced by all multi-component transforms, along with their
   dependencies; however, this resulted in overwriting of MATRIX
   with MAT if the `create_stages' function was called during
   codestream generation before `kdu_params::finalize_all' had been
   called by the application -- that is where the `Cmct' flags are
   automatically configured.  This problem means that attempting
   to compress imagery with certain multi-component transform
   configurations would result in an error message, complaining that
   the MAT option was being used in place of MATRIX.  The solution
   that we have adopted is to add an extra argument to
   `kd_mct_stage::create_stages' that identifies whether or not a
   new codestream is being generated, so that the overwriting of
   MATRIX to MAT is restricted to the case of existing codestreams
   without the Cmct flags, thereby meeting the original objective
   of providing legacy support for non-compliant existing codestreams.
b. Fixed a bug in the implementations of `jx_metanode::load_recursive'
   and `jpx_meta_manager::load_matches'.  Both functions were careful to
   watch for situations in which recursive parsing of boxes might resolve
   the target of a link node, causing its `parse_state' to be removed
   and unlinking it from the list of incomplete boxes.  However, both
   functions failed to check the possibility that the already fetched
   `next' incomplete box might be unlinked from the incomplete list in
   this way.  Errors resulting from this bug would have raised assertion
   failures in debug mode, but would have crashed release mode deployments.
   These errors could only occur in certain out-of-order parsing scenarios
   created by the progressive parsing of metadata as it arrives from a
   JPIP server.
c. Fixed an incorrect use of `memset' to initialize the little-used
   `jb_source' structure in "jpb.cpp".
d. Corrected a minor oversight in the "kdu_v_compress" and "kdu_vcom_fast"
   demo apps which prevented the "Cycc" coding parameter attribute from
   defaulting to "NO" when the source file arrives with a Luma-chroma
   (rather than RGB) format.  This "bug" meant that the default compression
   options for luma-chroma (e.g., YUV/YCbCr) content in 4:4:4 format
   would have been significantly less efficient than the natural option
   (can always be forced) of "Cycc=no".
e. Corrected an error in the rendering of many of the enmerated JPX opponent
   colour spaces via the `jp2_colour_converter' service.  In practice, this
   error does not often appear to come up, but the main issue is that those
   YCbCr/YPbPr-type spaces that cannot be well approximated by the usual
   high-performance YCbCr transform (the one defined by JPEG2000 Part-1)
   had their green and blue channels accidentally interchanged.
f. Corrected a bug in the way SIMD data conversions were being invoked
   within "kdu_v_compress" when ingesting LSB_aligned multi-byte sample
   values from a VIX of high precision YUV file.  The bug has no impact
   when reading VIX files with the default MSB-aligned sample packing
   convention, or when not using SIMD accelerations.  Low-precision (e.g.
   8-bit/sample) VIX/YUV files were also not affected.
g. Corrected a significant error in the multi-threaded memory allocation
   sub-system, created in going from version 7.6 to 7.7, where a line of
   code was accidentally deleted.  The error can result in a NULL pointer
   de-reference under extremely rare circumstances, in systems running
   many threads.
h. Fixed a minor bug in `qcd_params::copy_with_xforms' that caused QCD/QCC
   marker segments to be copied (e.g. during transcoding) wrongly if the
   Qderived flag is set -- in practice, this is a rarely used option.  The
   problem primarily manifested in a non-fatal error occuring in the
   "kdu_server" application when serving content compressed irreversibly
   under the little-used Qderived=yes condition.

Changes from KDU-7.6 to KDU-7.7
---------------------------------
1. Introduce a neighbourhood contrast masking mechanism into the block
   coding machinery that is largely identical to the mechanism originally
   described in Taubman's EBCOT paper (IEEE Transactions on Image Processing)
   which is also the "Cvis" option that appeared within the JPEG2000
   verification model VM8 during development of the original standard.
   The main difference from these earlier works is tha the new option
   (accessed via the "Civs" coding parameter attribute) uses an overlapping
   moving window of size 8x8 within each subband to compute masking levels,
   whereas the original approach used disjoint 8x8 blocks to compute a masking
   level for the whole block, as a matter of convenience.

2. Introduced "-grey_weights" and "-chroma_weights" options to the
   "kdu_compress" demo app that can be used to impart the same type of
   visual weighting scheme that is automatically applied to RGB images
   in the case where an image is monochrome ("-grey_weights") or in a
   luma-chroma space ("-chroma_weights"), possibly with sub-sampling of
   the chrominance components.  The mechanism behind "-chroma_weights" is
   identical to that which has existed in the "kdu_v_compress" and
   "kdu_vcom_fast" applications for quite a long time, where a video
   file's header is used to deduce the colour space.

3. Introduced an option to "kdu_compress" for converting input imagery
   from an RGB space to YCbCr with 4:2:0 chrominance sub-sampling, all
   on the fly, so as to facilitate the evaluation of JPEG2000 compression
   performance in 4:2:0 mode, which is the most common modality adopted
   by other less flexible codecs.  This option is activated via the
   "-rgb_to_420" switch, which automatically configures appropriate visual
   weights (unless `-no_weights' is specified) and CRGoffset parameters,
   while also automatically selecting the sYCC colour space descrition if
   there is a JP2/JPX file wrapper.

4. Increased the robustness of the codestream parsing machinery to
   errors by arranging for the `kd_tile::read_tile_part_header' function
   to hunt sequentially for SOT (Start-Of-Tilepart) marker segments if
   the information supplied by an earlier SOT marker segment's tile-part
   length field turns out to be invalid.  This helps in the parsing of
   corrupted codestreams that contain many tiles or tile-parts.

5. Modified the core networking machinery embodied by `kdcs_channel_monitor'
   to allow for true interruption of internal waits for network events,
   thereby allowing the `kdcs_channel_monitor::run_once' function to be
   used effectively even with large timeouts in both client and server
   components (leads to better power efficiency).  At the same time,
   introduced a `KDCS_CONDITION_IMMEDIATE' service condition that can be
   scheduled by channel servicers to arrange for the transmission of
   multiple packets, chunks, etc. to be interleaved with data transmission
   from other channels without introducing delays.  This allows for more
   accurate channel state estimation within the Kakadu server wherever
   multiple channels (could be associated with different clients) share a
   common bottleneck in the network.

6. Modified the network state estimation logic in "kdu_server" to make
   less aggressive estimates of channel capacity, so as to more fairly
   interleave data with other services (including other JPIP channels)
   that might share a common bottleneck link in the network.  This
   affects only the HTTP-TCP and (especially) HTTP-UDP transport
   protocols.  The congestion control logic is now substantially similar
   to the delay-sensitive TCP-Vegas algorithm.

7. Corrected a problem with "kdu_server" in that when serving long
   animations or video, the server's resources continued to grow quite
   rapidly.  The server now relinquishes older resources associated with
   codestream transcoding and structure appraisal, but it does hold onto
   the limited resources required to model a client's cache (in session-based
   communications only) indefinitely.  This will be changed in the future
   with the introduction of the "mset" JPIP request field and associated
   response headers for cache-model handshaking.

8. Modified the logic that limits the maximum data chunk size used by
   "kdu_server" with the UDP transport so that UDP datagrams should
   never need to be fragmented into multiple MTU's, assuming the common
   1500 byte MTU limit associated with ethernet traffic.  These bounds
   could always have been achieved before by supplying a suitable
   "-chunk_size" argument to the server upon launch, but that argument
   affects all transport types, whereas UDP transport is the one most
   sensitive to fragmentation.

9. Added a "-daemon" argument to "kdu_server" (Unix-like operating
   systems only) that causes the server to launch a separate
   daemon child process.  This is the preferable way to start up
   a JPIP server during system initialization.

10. Modified the way `kdu_client' behaves when disconnecting JPIP channels.
    Previously, the client waited until the final disconnection request's
    complete response arrived, but this may not happen if the server
    terminates the channel before sending all response data, forcing the
    client to wait upon a timeout, which can unnecessarily delay closure
    of browsing applications that attempt to terminate gracefully.  Now
    the client is content to recognize a server's reply to its closure
    message as indication that the channel is finished, rather than waiting
    for data on an auxiliary channel.

11. Modified the `kdu_event' synchronization interface, replacing the `set'
    member function with two alternatives, `protected_set' and
    `unprotected_set'.  While `kdu_event' does not play a major role in
    Kakadu (none at all in the core system), it is used in a number of
    the demo apps and quite extensively within "kdu_server".  If your own
    application uses `kdu_event', you will find that the compiler now
    complains that the `set' function does not exist.  This is intentional,
    since it is important that you consider which of the two alternative
    `set' functions is appropriate.  In almost all cases, `set_protected'
    is the right replacement, but you should read the API documentation.
    In particular, if you have been invoking `kdu_event::set' from a context
    in which the mutex that is passed to `kdu_event::wait' is not locked,
    you should be using `set_unprotected' now -- in practice, this is not
    very likely and not a common design strategy, but one that occasionally
    becomes necessary. 

12. Completed incorporation of the changes introduced by ammendment
    IS15444-1/AMD8 (Interoperable Master Format) into Kakadu.  The IMF
    profiles and associated tests were included with version 7.6, while
    this version completes the upgrades to the metadata associated with
    the elementary streaming media format described in that ammendment.

13. Bug fixes
   -- Fixed a problem with the handling of nested rubber-length JP2 boxes.
      Previously, when a rubber length box was closed, its parent was also
      closed, which had the desired effect of preventing further access to
      any of the box's ancestors but had the drawback that higher level
      users of the `jp2_input_box' API might not take the possibility of
      a parent box's incidental closure into account.  In particular, this
      already caused problems for some of the other higher level Kakadu
      API's when rubber boxes were nested (not such a common scenario).  The
      solution adopted was to avoid closing the parent box but mark the
      location within that box, beyond which no further sub-boxes can
      be opened on account of the discovery of the rubber length box.  This
      avoids possible programming errors in either Kakadu code or other
      application level code that might have arisen from failing to
      recognize the possibility of silent closure of the parent of a box
      being closed.
   -- Fixed two minor bugs in the implementations of functions
      `local_convert_and_copy_...' and `local_convert_and_add...' within
      "kdu_region_decompressor.cpp" that affect multi-tile processing
      where the SIMD accelerated sample conversion functions are not
      available for a particular architecture or data precision.
   -- Fixed a minor bug in the in-line implementation of
      `kd_precinct::release' where the `KD_PFLAG_LOADED_LOCKED' flag was
      not explicitly being reset; in certain cases (non-seekable file-based
      codestreams with layer-progressive order) this could cause content
      loaded with a reduced quality layer setting in place to remain
      stuck, in the sense that decoding again without the quality layer
      constraint might not produce the full quality result expected.
   -- Added "const" modifiers to a number of additional API functions
      in "kdu_compressed.h" that serve only to return state information.
   -- Modified `kdrc_ilayer::activate' in "kdu_region_compositor.cpp"
      slightly to make sure that inactive compositing layers that become
      active appear to have new content in their composited buffers, from
      the perspective of the `kdu_compositor_buf' member functions.
      Otherwise, applications like "kdu_show" that use these member functions
      to determine whether re-rendering is required might not render the
      newly active layer's content to a display.
   -- Fixed a typo in the definition of macro SSSE3_SET_HSHUF_FIX16_2TAP_FUNC
      when compiling with KDU_NO_SSSE3 defined (a comma was missing).
   -- Fixed a bug in "kdu_macshow", where `kdms_animation_bar' was not
      removing itself from the default notification centre before being
      destroyed -- this is the reason why "kdu_macshow" would sometimes
      mysteriously crash after opening and closing images repeatedly for
      soem time.
   -- Replaced the "sscanf(...,"%f",...)" calls in the `kdu_client' and
      the `kdu_connection' part of the "kdu_server", that use floating
      point conversion to check for HTTP version numbers, with a more
      explicit conversion strategy that is insensitive to locale settings
      that may affect the way floating point numbers are represented and
      hence converted -- HTTP version numbers use a period separated format
      regardless of locale.
   -- Fixed an unmatched mutex lock within an unlikely-to-be-taken code
      branch inside `kdu_client::augment_with_cache_file'.
   -- Corrected a non-fatal yet harmful race condition in the implementation
      of `kdu_server' that may occasionally have left a session temporarily
      suspended.
   -- Corrected an inefficiency in the "kdu_server" app related to use of
      the HTTP-only transport and stateless services, where the content
      preparation epochs were far too small to allow efficient processing;
      transmitted data chunks were also too small.  These problems did not
      affect the higher performance HTTP-TCP or HTTP-UDP transports.
   -- Removed assert statements from `kdu_client::thread_cleanup' that
      inisted all cache model managers and associated references must have
      been cleaned up by completion of the main JPIP client's thread.
      In rare circumstances, cache model managers may be deliberately
      retained by an incomplete "preserve-descriptor" between
      disconnect/reconnect/cache-augment events, for efficiency reasons.
   -- Corrected a bug in the writing of 64-bit integers by `kdcs_message_block'
      that caused invalid "qid" values to be written by Kakadu's JPIP
      client when using the HTTP-UDP transport -- this bug only affected
      JPIP sessions with more than 1000 distinct requests.
   -- Corrected a minor bug in the printing of 64-bit integer types by
      the `kdu_messaging' base class -- possibility that huge values
      might have been printed wrongly.
   -- Removed an erroneous assert statement from the "kdu_server" app's
      `kds_jpip_channel::push_reply' function, that could have been
      triggered while using the HTTP-UDP transport over low delay channels
      in debug mode.

Changes from KDU-7.5 to KDU-7.6
-------------------------------
This is a huge new release, with many enhancements and quite a few
important fixes.  Most importantly, this new release provides just about
everything one could need to build outstanding mobile applications
based on JPEG2000.  The major changes are outlined, and occasionally
explained, under the 8 sub-headings that follow.

1. New core system features:
  -- The `kdu_codestream::apply_input_restrictions' function (+ overloads) now
     take an optional extra `kdu_quality_limiter' argument.  This new feature
     provides a highly disciplined mechanism for discarding coding passes
     during decompression, based on computed visually weighted error bounds.
     This new feature turns out to be very important for highly efficient
     rendering of content at reduced resolutions, since the precision at
     which lower resolution components are encoded within a high resolution
     source is almost always much higher than required when the content is
     viewed at reduced resolution.  The new `kdu_quality_limiter' object
     allows display resolutions (at a nominal viewing distance) to be set,
     along with error targets.  Tests show that when using this mechanism in
     the manner recommended via the documentation (includes information on
     how to use it with retina- and non-retina resolution, mobile and desktop
     displays), no noticeable visual distortion occurs, yet computational
     load can reduce by a factor of 3 or more when rendering originally high
     resolution content at reduced resolutions.  Special provisions are
     included to ensure that content that was losslessly compressed will be
     rendered losslessly at full resolution.  The new feature is easy to use
     and is recommended for all interactive rendering applications based on
     Kakadu.  As explained below, mechanisms are provided to easily access
     this new feature from the higher level rendering API's, notably
     `kdu_region_decompressor' and `kdu_region_compositor', where scaling
     processes are automatically taken into account when configuring effective
     display resolutions for assessing visual sensitivity.
  -- Profile support has been upgraded to include all of the IMF
     (Interoperable Master Format) profiles defined in IS15444-1/AMD8,
     including a reasonable level of profile conformance testing.  These
     profiles are accessed via the `Simf' parameter attribute.

2. `kdu_region_decompressor' enhancements:
  -- Provided a new `kdu_region_decompressor::set_quality_limiting' function
     for accessing the new core codestream visual quality limiting feature
     described above.  This function is easy to use, automatically handling
     the translation of rendering pixel display resolutions to codestream
     resolutions.  This function also provides colour space hints to the
     underlying `kdu_codestream::apply_input_restrictions' function to help
     it reliably detect image components that drive only chroma channels,
     since these have a different visual significance to luma channels.

3. New SIMD acceleration paths for ARM and x86 platforms
  -- This release of Kakadu completes the work of introducing X86-AVX2 and
     ARM-NEON accelerated processing paths to all significant sample processing
     operations in Kakadu.  This work was commenced with KDU-7.5, where
     AVX2 and NEON accelators were added to the existing architecture-based
     accelerators for most aspects of the core codestream system and to
     all significant data transfer paths encountered by `kdu_stripe_compressor'
     and `kdu_stripe_decompressor'.  In this release, we have done the same
     for the composition/blending operations in `kdu_region_compositor', and
     the data transfer and resampling (expansion/contraction) operations
     performed by `kdu_region_decompressor'.  The changes are particularly
     important for mobile devices, where resampling is heavily used to match
     fixed window sizes and where ARM processors are widely deployed --
     previously, the expensive resampling operations had no accelerated
     implementation on ARM processors.  On X86 processors with AVX2 and FMA3
     support, the resampling, data transfer and compositing operations are
     now approximately twice as fast as they were previously.

4. `kdu_region_compositor' changes to improve efficient interactive rendering:
  -- Provided a new `kdu_region_compositor::set_quality_limiting' function
     which passes quality limiting parameters along to the
     `kdu_region_decompressor::set_quality_rendering' function, taking into
     account the scaling and re-orientation operations performed during
     image composition.
  -- Provided a new `kdu_region_compositor::configure_scaling_params' function
     that can be used to configure the way in which resampling operations are
     used in conjunction with DWT level discarding to implement arbitrary
     rendering scales.  This function allows applications to adopt less
     computationally expensive resampling policies for retina displays, or
     during coarse initial renderings, which is particularly useful in
     maximimizing responsiveness and minimizing energy consumption on mobile
     devices.
  -- Added a `kdu_region_compositor::add_frame' function to complement the
     existing `set_frame' function.  This allows you to add multiple JPX
     composited frames to a single composited image surface, each with
     its own positioning offset.  This allows you to easily create
     "contact sheets" or "multi-page" compositing surfaces that greatly
     simplify the implementation of interactive page-flipping paradigms
     and thumbnail grids, all constructed dynamically during viewing.
     These are very useful for mobile devices, where there is usually a
     fixed size application window into which the content should be
     arranged in the most intuitive manner.  Since mobile devices usually
     employ touch-based paradigms for navigating between images, the
     ability to create multi-frame compositions facilitates the rendering
     of transitions from frame to another.
  -- Added several information-bearing functions to `kdu_compositor_buf',
     allowing the rendered scale, full composited image dimensions, buffer
     surface region and rendering progress information all to be retrieved
     directly from the buffer.  These features are useful for interactive
     rendering applications, especially where multiple buffers may be
     rendered and pushed onto a rendered buffer stack, or perhaps uploaded to
     display textures, since the `kdu_region_compositor' itself can only
     report dimensional parameters for the buffer that is currently being
     rendered.  The rendering status information now available from
     `kdu_compositor_buf' can be more useful than the progress information
     returned via `kdu_region_compositor::process'; in particular, the
     buffer itself now reports the number of rows that have been fully
     rendered (as opposed to a region that contains modified content) and
     also provides sufficient information for an application to determine
     whether or not the entire buffer needs to be redrawn to a display to
     correctly reflect any novel content.  These features allow for highly
     efficient display drawing, OpenGL texture upload, etc., for cases where
     content is being rendered and drawn progressively.
  -- Augmented `kdu_region_compositor:set_scale' with an optional extra
     argument that allows the region-compositor to manage two different
     scales: a "rendering scale" that determines the size and content of
     rendered buffer surfaces; and a "notional scale" that drives the
     interpretation of locations and dimensions passes across virtually all
     `kdu_region_compositor' API functions.  While this may sound confusing,
     it actually greatly simplifies the implementation of interactive
     rendering applications that maximise responsiveness by rendering
     content initially at a coarse scale and then at a finer scale; this is
     a common paradigm, particularly for low power mobile devices with
     retina displays.  To implement such a rendering stategy with the
     new `kdu_region_compositor' most of the application can remain
     oblivious to the existence of anything other than the notional scale,
     which is the scale at which the application intends to use, perhaps
     after an initial coarse resolution rendering. The notional scale
     drives viewport dimensions, focus box dimensions, spatially-sensitive
     metadata queries, constuction of JPIP requests and so forth, all of
     which are facilitated by `kdu_region_compositor' member functions.
     The rendering scale and associated rendered dimensions are largely
     hidden by the `kdu_region_compositor' interaces, but stored with the
     `kdu_compositor_buf' objects themselves, which are typically passed
     to a function that uploads rendered buffers to display textures of
     some form; only that function need know about the rendering scale
     itself, scaling the content during texture upload or display, as
     desired.
  -- Removed a minor inefficiency in the `kdu_region_compositor::process'
     function, where calls to `process' that actually complete all processing
     did not leave the compositor indicating that all processing was complete
     until a subsequent call to `process'; moreover, if the caller supplied the
     `KDU_COMPOSIT_DEFER_REGION' flag, the initial call to `process' would
     always return an empty `new_region'.  While there was never anything
     wrong with this, or incompatible with the definition of the `process'
     function, it could lead to reduced throughput/responsiveness in some
     applications.
  -- Augmented `kdu_region_compositor::inspect_composition_queue' with an extra
     optional argument, allowing the buffer surface to be returned for any
     element in a buffer queue managed by the `kdu_region_compositor' object.
     This facilitates careful comparison of animation frame buffers that
     have been pushed to the queue to determine whether they are compatible
     with a viewport that might be dynamically changing.

5. Complete overhaul of `kdu_cache'
  -- Prior to this release, the `kdu_cache' component, on which `kdu_client'
     builds, did not really provide support for deletion of data from the
     cache.  Since the cache lives in memory, this could eventually start to
     create problems, especially for mobile applications where the memory
     is not usually backed by the file system.  Some of our licensees have
     asked specifically for this feature, which has been a while in coming.
     The new implementation provides lots of tools to support robust
     deletion of content.  It also supports automatic deletion of content
     to conform to prespecified memory bounds, based on a "least-recently
     touched" policy. This has been tested and proven to be effective even
     with very small bounds on the overall size of the cache.
  -- Along with deletion of content, and especially automatic trimming of
     content to memory constraints, it becomes important to be able to
     preserve some specific content against eviction from the cache.  The
     new implementation provides mechanisms to prevent whole classes of
     traffic (e.g., meta-data or codestream main headers) from being
     evicted.  It also allows individual data-bins to be flagged for
     preservation; the `kdu_client' super-class can use these tools to
     define a preserved subset of the cache that corresponds to an arbitrary
     JPIP window-of-interest (actually any valid JPIP request can be used to
     define the preserved subset without actually sending the request to a
     server).  Tools are provided to scan the cache, restricted just to
     the preserved subset, or restricted to everything but the preserved
     subset.  These tools are much more efficient and robust than scanning
     mechanisms in the old cache, and they can be used to store the cache
     to an external file that is structured into a preserved preamble
     (presumably small, yet very important) and the non-preserved remainder.
  -- The new `kdu_cache' implementation is much better designed from the
     perspective of multi-threaded interaction.  It is now possible for many
     threads to interact with a common (primary) cache without having to
     take exclusive locks on critical sections that may hurt the performance
     of the overall system.  There are internal critical sections, but
     threads that read from the cache only rarely need to enter them.
  -- The `mark_databin' function has been significantly extended to allow
     for the recovery of information about content that has been deleted
     (explicitly or implicitly), as well as content that has been
     augmented in ways that require such marks.  The intent of all marks
     is to reflect changes that have occurred in the cache that a JPIP
     server would not be aware of (e.g., deletions and recovery of data
     from a local cache file).  The `kdu_client' super-class uses this
     marking mechanism to signal both additive and subtractive cache
     model changes to a connected JPIP server, in a particularly efficient
     manner.
  -- For most applications, all of these new capabilities are automatically
     exploited in the interaction between a `kdu_client' and its `kdu_cache',
     so the only thing you may need to know about is that the function
     `kdu_cache::set_preferred_memory_limit' is available for you to bound
     the cache's memory consumption, and the `kdu_client' offers a very
     nifty function `kdu_client::set_preserve_window' that defines the
     preserved data subset via a JPIP window-of-interest, just like any
     regular call to `kdu_client::post_window'.

6. `kdu_client' enhancements
  -- Added a `kdu_client::reconnect' function that can be used to
     re-establish a server connection that was dropped either by the client
     (via `kdu_client::disconnect') or by the server, re-using the existing
     cache contents.  While this capability is of general interest, it is
     particularly valuable for mobile applications where an application that
     is moved into the background should usually disconnect any JPIP service
     so as to avoid silent consumption of precious bandwidth resources.
     Once the application is brought back into the foreground, the connection
     can now easily be re-established, without needing to save the cache to
     a file and reload it.  If reconnection discovers incompatibility
     between the original source's target-id and the one currently
     advertised by the server, the `kdu_client::reconnect' function can
     be configured either to keep the existing cache, or to delete it and
     start from scratch with the server's (possibly different) version of
     the source.
  -- Added functions `kdu_client::open_from_cache_file' and
     `kdu_client::augment_with_cache_file' that allow a cache file that was
     written previously by a `kdu_client' object to be opened directly,
     rather than waiting until a URL connection is established to
     determine whether the cached data can be used.  The first function
     opens the cache as another JPEG2000 data source and allows the
     new `reconnect' function to be used at any time to connect back to
     the server and get more data.  The second function allows a cache
     file to be used to augment the live cache being used by an active
     `kdu_client' object that may currently be communicating with a
     server.  Together, these functions allow you to add contributions
     to the cache from any number of cache files that might have
     been stored from other JPIP browsing sessions, not necessarily
     on the same platform.  For example, an email attachment may
     contain a JPIP cache file that can be opened directly using the
     "kdu_macshow" or "kdu_winshow" utilities.  Another email or
     web-site might contribute additional information to the cache of
     an open JPIP browser such as the kdu_show utilities.  At any time,
     reconnection with the server can also occur to retrieve additional
     information of interest.
  -- Various additional tools are provided with `kdu_client' to facilitate
     the discovery of compatible JPIP cache files prior to calls to
     `open_with_cache_file' and `augment_with_cache_file'.  These include
     the static `kdu_client::check_cache_file' function and the
     `kdu_client::get_cache_identifier' member function, that is part of
     a system for generating and checking identifiers that should uniquely
     characterise JPIP content.
  -- Greatly enhanced application control over the way in which cache
     files are read, written (or even deleted). This is done by adding
     an optional `cache_file_handling' argument to `kdu_client::connect'
     and `kdu_client::open_with_cache_file', and by supplying new member
     functions `kdu_client::set_cache_file_handling',
     `kdu_client::get_cache_file_handling' and
     `kdu_client::will_lose_data_if_closed'.  Together, these allow an
     application to enter or leave a "private browsing" experience at any
     point, to temporarily disable cache file saving so that the client
     can be disconnected without the computational burden of saving content
     to disk, but without losing the ability to save the cache contents
     later.  This is highly useful for mobile applications, where a
     JPIP browsing session may need to be temporarily suspended to answer
     a phone call.  The transition can occur without delay, but there need
     be no risk that data is lost, since the cache file can always be
     saved after browsing has resumed, or if the application needs to be
     terminated or trimmed down due to memory pressure.
  -- The cache file format has been extended to allow for a specially
     identified preamble, that corresponds to the subset of cache data
     that was flagged for preservation (e.g., by passing a window of interest
     to `kdu_client::set_preserve_window').  The preserved subset is generally
     configured to correspond to a low resolution version of the first
     compositing layer or codestream in a source, but could be a complex
     subset of the imagery.  As mentioned above, the preserved subset is
     protected against automatic eviction from the `kdu_cache' as it
     attempts to satisfy defined memory constraints.  Storing the preserved
     subset of cache data in a defined preamble of the cache file has many
     benefits.  The `kdu_client' loads the preamble first and then loads
     the rest of the file in a background thread while rendering of the
     content can proceed, ensuring maximum responsiveness for rendering
     applications.  The `kdu_client::open_with_cache_file' function can
     also be configured to load only the preamble, which is ideal for
     rendering thumbnails or summary views to a contact sheet or image-picker.
  -- The `kdu_client' object previously created codestream model managers
     whenever it needed to signal cache model corrections to a JPIP
     server (usually after reading pre-existing content from a pre-existing
     cache file) but never destroy them.  In some cases, this could lead to
     a large number of model mangers, occupying a non-trivial amount of
     memory.  Now the client discards model managers as soon as the
     relevant codestreams need no longer be modeled and it also keeps
     track of actively used model managers and those that can be recycled,
     potentially being created again in the future if they are needed.  This
     is all very helpful when navigating through content such as video or
     rich animations with a pre-existing cache file.
  -- While the above tools will be of interest probably to all interactive
     rendering and media browsing applications, they are especially valuable
     for mobile applications, where memory must be managed carefully and
     computational and I/O efficiency are critical to maintaining good
     battery life.
  -- Many of the new features offered by `kdu_client' are demonstrated in
     one way or another by small changes in the "kdu_macshow" and
     "kdu_winshow" applications.  In particular, these apps both now limit
     the maximum sizes of their JPIP caches, they define sensible preserve
     windows for their caches and use them to save and re-load cache files
     efficiently and responsively.  The cache size limits are defined at the
     start of "kdms_controller.h" and "kdws_manager.h", respectively, while
     preserve-windows are configured inside the `kdms_renderer::open_file' /
     `kdps_renderer::open_file' functions.  Both viewers can now reconnect
     to a JPIP server, without closing and reloading content; they can also
     move in and out of private style browsing modes.  We have also developed
     a separate IOS version of the viewing tools that exploits and validates
     all of the new features.
  -- Slightly augmented `kdu_client::get_window_in_progress' to allow
     discovery of the last window in progress on any queue before all
     request queues terminated, which is quite useful for discovering
     outcomes from single-shot requests that might already have completed,
     with all communication terminating.  This is used in the kdu_show tools
     now to ensure that an interactive user always gets visual feed-back
     concerning regions and resolutions associated with single-shot (single
     URL) JPIP requests even if they complete almost instantaneously.
  -- Added `kdu_client::check_stateless' to allow applications to discover
     what type of communication resulted from handshaking with the server.

7. Other minor improvements
  -- The `jpx_metanode' interface is now able to directly check for
     certain types of UUID boxes via member functions `is_xmp_uuid',
     `is_iptc_uuid' and `is_geojp2_uuid', where previously you had to
     use `get_uuid' and compare the UUID code with the standard 16
     byte sequence for each UUID sub-type.
  -- The "kdu_merge" demo app gets a new command-line option "-album2"
     that produces photo albums without the introductory catalog pages
     that are produced by "-album".  We believe that this is actually
     a better album construction technique since it leaves the rendering
     application to decide whether to build catalog pages (or contact
     sheets) from consecutive frames of the animation at very small
     rendering scales.  Building such dynamic pages is now very easy
     with the new `kdu_region_compositor::add_frame' function.
  -- Arranged for "kdu_macshow" to release its `NSBitmapImageRep' objects
     after each use, rather than keeping them around.  While the latter
     sounded like a good idea, there are some undocumented yet extremely
     annoying cacheing policies implemented by `NSBitmapImageRep' that
     cause memory to grow to huge levels before anything is released.  Not
     a leak, but bad for system performance.
  -- Moved all new/delete calls within in-line headers in public API
     functions into out-of-line implementations, except in a few cases
     where an entire class is implemented in-line.  This change avoids
     problems that can occur when using DLL's or shared libraries compiled
     with tools that use incompatible heap managers.

8. Bug Fixes
  -- Fixed a major bug in the code that generates interpolation filters for
     resampling (expansion and contraction) within `kdu_region_decompressor'.
     This bug has been present in Kakadu for some time, degrading the
     otherwise highly disciplined resampling machinery to the point where
     visual impairments could be noticed.  After fixing this bug, the
     resampling machinery in `kdu_region_decompressor' clearly outperforms
     the bilinar interpolation mechanism commonly employed by GPU's, even
     during expansion.
  -- Also fixed a bug in two of the SSSE3-based SIMD optimization paths for
     expansion-oriented resampling of floating point precision sample
     buffers.
  -- Fixed bugs in the ARM/NEON accelerator implementations for 32-bit
     precision vertical and inter-slice DWT's based on arbitrary transform
     kernels (not the standard Part-1 transform kernels).  These bugs
     primarily affect volumetric medical image compression and decompression
     on ARM processors.
  -- Fixed a bug in `jx_composition::assign_layer_indices' which affected
     the interpretation of some complex repeating patterns when parsing
     and translating compositing instructions in JPX files.
  -- Fixed some typos in the macro names used to include SIMD accelerators
     for the "kdu_vex_fast" demo app so that there are no compiler errors
     when building for non-x86 target platforms.
  -- Fixed an error in the resampling code used by `kdu_region_compositor'
     when interpolating imagery by the specific integer factor of 4.0 and
     working with fixed-point sample data (a simple typo that was not caught
     earlier because it does not occur in SIMD optimized paths).
  -- Fixed an error in `mj2_source::count_codestreams', which caused the
     function to identify the number of codestreams in an MJ2 source as 0,
     if called a second time.
  -- Fixed a minor bug in the generation of Java native interfaces
     that resulted in 3 Kakadu classes not being assigned valid Java
     constructors.  These are all classes whose native implementation
     deliberately keesp destructor private.  The only significant class
     affected by the bug was `Kdu_block'; without this fix, calls to
     `Kdu_subband.open_block', for example, would generate a missing
     method assertion in Java.
  -- Corrected a minor oversight in "kdu_v_compress" where the wrong
     comparison was used to check whether the efficient rate allocation
     logic might be too aggressive.  This bug was discovered accidentally,
     not in response to any reported problem.  It is not clear whether the
     bug would have been accidentally imported into code based on
     this demo app.
  -- Fixed a bug in `kdu_client' that has existed for the last few major
     releases, where byte limits were accidentally applied to non-interactive
     requests.  The fix was trivial, but non-interactive (one-shot) requests
     are not frequently tested with JPIP.
  -- Fixed a bug in `jpx_source' and `jpx_target' associated with the advanced
     JPX containers feature, where resources were being cleaned up upon
     destruction in an incorrect order, leading to the possibility of
     invalid heap accesses.  We found this problem by accident, without
     actually observing the problem or receiving a bug report.
  -- Fixed a bug in "kdu_client" which could occur only if a new request
     queue is started while other queues are managing timed requests (for
     browsing of animations) and the new queue is made to share the same
     JPIP channel as the timed request queues, for one reason or another.
     The first request of the new queue is pushed ahead of the timed requests
     so as to get it started as soon as possible, but in previous versions
     there was a small error in the way this was done that affected only
     this specific case where timed requests were ongoing within the
     shared JPIP channel.  The problem is most likely to have manifested if
     the special "out-of-bounds" (OOB) queue was used for the first time
     after JPIP based animation was started.
  -- Fixed a subtle use-after-free bug that was reported by Google's Jose
     Duart, that could occur when handling fatal exceptions (e.g., when
     processing corrupted content); the bug could be induced during
     `kdu_codestream::destroy' if a multi-threaded processing
     environment (`kdu_thread_env') was being used at the time when the
     exception was generated/caught within the multi-threaded network.
     The bug fix involved rearranging the order of execution of a few
     statements in the internal `kd_codestream' object's destructor.
  -- Fixed a subtle bug in `kd_global_rescomp::initialize' and
     `kd_global_rescomp::notify_tile_status' that were reported by Google's
     Jeff Breidenback.  The bug could be excited only by content with
     invalid coding parameters, leading to undefined large bit-shifts in
     coordinate calculations.

Changes from KDU-7.4 to KDU-7.5
-------------------------------
1. Software structural changes:
a) All native Kakadu symbols (class names, non-member functions, typedefs,
   and so forth) are now declared within a small set of carefully chosen
   namespaces that mirror the structure intended by the original naming
   conventions.  The Kakadu names themselves have not changed, but
   applications built using Kakadu will need to add a "using namespace"
   statement, as explained below.  One benefit of the new namespaces is
   that static builds can now be constructed without running the risk of
   name collisions at link time.  The new namespaces are as follows:
   - `kdu_core' holds all exported definitions from the core system
   - `kd_core_local' holds internal definitions within the core system.
        You should never need to access any names in this space!
   - `kd_core_simd' holds all coresys platform-specific SIMD accelerator names.
        You should never need to access any names in this space!
   - `kdu_supp' holds all exported definitions from the supporting API's
        defined within the "apps" folder and associated workspaces -- i.e.,
        right now, this is everything other than `kdu_core'.
   - `kd_supp_local' holds the internal definitions used to implement the
        API's exposed within the `kdu_supp' namespace.
        You should not generally need to access any names in this space!
   - `kd_supp_simd' holds all platform-specific SIMD accelerator names used to
        accelerate the implementation of the API's exposed within `kdu_supp'.
        You should never need to access any names in this space!
   ---
   The implications of this for application code are:
   A) applications that include any of the non-core headers (i.e., ones that
      were originally found in the "apps" sub-directory) should add a
      "using kdu_supp;" statement after the headers are included; and
   B) applications that include only core system headers should add a
      "using kdu_core;" statement.
   ---
   Additionally, to facilitate the portable re-use of objects that
   are specific to certain demonstration applications, the following
   additional namespaces are defined:
   - `kdu_supp_vex' contains the names of re-usable API's from the
     "kdu_vex_fast" demo app.
   - `kdu_supp_vcom' contains the names of re-usable API's from the
     new "kdu_vcom_fast" demo app (see below).

b) The "kdu_hyperdoc" tool has been augmented to recognize namespace
   declarations in the header files and to incorporate the relevant
   namespaces into the marshalling code that is auto-generated for
   Java and Microsoft managed language bindings.  Note that users
   of the Java, C# and VB language bindings do not need to make
   any adjustments to account for the new namespace definitions that
   have been introduced into the native code.

2. Platform-specific optimizations:
a) Refactored all the SIMD accelerations found in Kakadu so that function
   pointers are configured during initialization rather than in-line
   acceleration macros determining the best code branch at call time.  One
   motivation for doing this is that it allows all SSSE3 accelerations to be
   moved into separate source files so that the core system can be compiled
   under gcc/llvm without the need for global application of the "-ssse3"
   switch.  This avoids rare problems with very old x86 CPU's that have no
   SSSE3 support.  A stonger motivation is the introduction of new acceleration
   paths for AVX2 and also the NEON instruction set for ARM CPU's.
b) AVX2 accelerations have been rolled out right across Kakadu.  In the
   core system, this means AVX2 for all Part-1 and most Part-2 (arbitrary
   transform kernel) spatial and multi-component wavelet transforms.  It
   also means AVX2 accelerations for the decorrelating colour transforms.
   In the application support API's, AVX2 accelerators are available for all
   common data sample conversion and buffer reorganization operations
   performed by the `kdu_stripe_compressor' and `kdu_stripe_decompressor'
   objects, as well as the "kdu_vex_fast", "kdu_v_compress" and "kdu_v_expand"
   demo applications.
c) Kakadu now provides an extensive set of optimizations for ARM processors,
   based on NEON SIMD intrinsics.  NEON acceleration is provided for colour
   transforms, for all Part-1 DWT transform paths, for most Part-2
   ATK (Arbitrary Transform Kernel) options that are likely to be used,
   and for all quantization/dequantization processing paths.  Part-2
   Multicomponent Transforms based on the DWT (e.g., DWT-based slice
   transforms for volumes) also have comprehensive NEON acceleration.
   Most of the data conversion paths taken by the `kdu_stripe_compressor'
   and `kdu_stripe_decompressor' API's also now have NEON SIMD accelerations.
d) Removed the now obsolete in-line assembly versions of the x86-family SIMD
   accelerator functions, all of which have been fully superceded by
   implementations based on the intrinsics that are activated by defining
   `KDU_X86_INTRINSICS'.  The older options `KDU_PENTIUM_MSVC' and
   `KDU_PENTIUM_GCC' are both now automatically converted to
   `KDU_X86_INTRINSICS'.  This simplifies the maintainance of Kakadu going
   forward and also prevents less efficient SIMD optimizations from being
   selected at compile time.

3. Changes in the Kakadu core system:
a) In previous versions of Kakadu, the provisions for assigning CPU affinity
   to threads was limited to at most 64 logical CPUs (32 on 32-bit operating
   systems), which is insufficient for modern high performance platforms.
   The CPU affinity mechanisms associated with `kdu_thread' and
   `kdu_thread_entity' have been substantially augmented and it is now
   possible to assign different affinities to threads that may play specific
   roles within a `kdu_thread_entity' thread-group.
b) Augmented the core `kdu_multi_analysis' and `kdu_multi_synthesis' workhorse
   objects with the ability to use externally managed `kdu_sample_allocator'
   objects for all sample data buffering.  This provides applications and
   higher level API's with the abliity to preserve buffers across
   instantiations of the tile-compression/tile-decompression machinery,
   and potentially to customize the way in which memory is allocated.
   NUMA (Non-Uniform Machiney Architecture) platforms with multiple physical
   processor packages/dies can arrange for memory to be allocated close
   to the physical CPUs to which working threads may be bound via the
   CPU affinity features mentioned above.
c) Fixed a typo bug in the background tile opening/closing logic introduced in
   version 7.4, where tiles closed in the background were being unintentionally
   re-opened.  The solution was to replace `KD_TREF_FLAGS_CLOSE_PENDING' with
   `KD_TREF_FLAGS_OPEN_PENDING' in the flags removal logic found within
   `kd_tile::open'.  This may have had serious consequences in some
   multi-threaded tile processing applications, but only if you were
   using the background opening features introduced in version 7.4
d) Fixed a longstanding oversight in the `kdu_thread_entity::signal_condition'
   function.  This function advertises the fact that it can be called from
   any thread, even one that is not participating in the thread-group whose
   `kdu_thread_entity' object is used to signal the arrival of the condition.
   In fact, this property is used to implement `kdu_thread_queue::force_detach'
   for emergency detachment of thread-queues that have not been properly
   terminated or waited upon.  However, it turns out that an optimisation
   trick employed within the `signal_condition' function can cause the
   signalled thread not to wake up if the function is invoked on another
   thread's `kdu_thread_entity' object.  To resolve the problem, an
   optional `foreign_caller' argument has been added to the function and we
   are now careful to pass this `foreign_caller' situation on to other
   parts of the multi-threaded environment where appropriate.
e) Added more readily tested core system version identification macros to
   "kdu_compressed.h", so as to supplement the text-based `KDU_CORE_VERSION'.
   The new version macros: `KDU_MAJOR_VERSION', `KDU_MINOR_VERSION' and
   `KDU_PATCH_VERSION' were suggested by Robert Coup and will be maintained
   by all future releases.  Additionally, speed-pack versions of Kakadu
   are guaranteed to have the symbol `KDU_SPEEDPACK' defined along with these
   other version macros in "kdu_compressed.h".
f) Enhanced the instruction set support tests found in "kdu_arch.cpp" to
   include separate tests for AVX2 and BMI2 instructions.
g) The `kdu_subband::open_block' function has been augmented with two
   optional arguments that can be used to indicate that code-blocks are
   being opened in a horizontal sequence of some extent.  This information
   is enough to allow the internal implementation to avoid the need for
   an atomic interlocked operation on every call to `kdu_subband::close_block'
   and these calls can often be the most frequently used synchronisation
   operations in an entire workload.  Reducing the number of bus locking
   operations may help to increase throughput in heavily multi-threaded
   deployments on platforms with a massive number of CPU cores.
h) Corrected an error in `kdu_thread_entity::join' for cases in which the
   queue being joined upon has already been detached by a call to
   `kdu_thread_entity::handle_exception'.  This only affects worker threads
   that join on other conditions (relatively uncommon) and then only when
   exceptions are thrown, caught and handled by other threads.
i) Corrected an error in the core encoder machinery (`kd_encoder') that could
   have manifested itself only in cases where quadruple stripe buffering
   was auto-selected to process lower resolution subbands on platforms
   with a very large number of CPUs will minimal hold-ups.  This bug would
   have generated assertion failures in debug mode and deadlocks in
   release mode, but only under rare conditions or when processing small
   images with a massive number of parallel threads.
j) Corrected an error accidentally introduced with KDU-7.4 in the modified
   tile closing logic, that could have manifested itself within a JPIP
   client during remote image browsing, where tiles opened as empty
   shells (due to lack of any header information in the JPIP cache) might
   have been closed in a manner that prevented them from being reopened
   when more data arrived in the cache.
k) The XCODE workspaces now also build an additional static version
   of the core library that is optimized for IOS applications.

4. Changes to higher level API's and application support tools
a) Augmented the `kdu_stripe_compressor' high level API as follows:
   ** The `kdu_stripe_compressor::start' function now accepts additional
      arguments which allow full control over the way in which codestream
      flushing occurs and `kdu_multi_analysis' engines are configured --
      previously, some of the more exotic capabilities could only be
      accessed by using the lower level API's directly in place of
      `kdu_stripe_compressor'.
   ** A new `reset' function has been provided, which can be used in place
      of `finish' to clean up and potentially re-use a `kdu_stripe_compressor'
      instance.  The problem with `finish' is that in multi-threaded
      deployments, it uses the multi-threaded environment installed by
      `start', but this multi-threaded environment might no longer exist if
      an exception has been caught and handled.  The `kdu_stripe_compressor's
      destructor now invokes `reset' in place of `finish', which is more
      robust against such situations.
   ** The internal machinery now keeps memory buffers allocated for sample
      data processing around beyond calls to `finish' so that they can
      be used again if `start' called again.  This can have significant
      benefits on platforms with multiple physical platforms, were memory
      placement is important.  If you intend to use the object in a very
      different way, or want to deallocate resources immediately, a
      subsequent call to `reset' can be used.
   ** A new `get_set_next_queue_sequence' member function has been provided
      to allow multiple `kdu_stripe_compressor' objects to be instantiated
      and attached to a single multi-threaded environment (`kdu_thread_env')
      using consecutive sets of thread-queue sequence indices so that worker
      threads will automatically migrate from processing jobs in the first
      stripe compressor to jobs in the next stripe compressor only as the
      availability of work within the first stripe compressor dries up.
      Previously, to achieve this type of behaviour for highly efficient
      multi-threaded processing of image sequences, it was necessary to use
      the lower level API's (focussed around `kdu_multi_analysis') directly.
b) Introduced similar modifications to the `kdu_stripe_decompressor' high
   level API, i.e., extra configuration parameters, introduction of a
   `reset' function to complement `finish' and preservation of memory
   buffers between calls to `finish' and `start'.
c) Extended all variants of the `kdu_stripe_decompressor::pull_stripe'
   function by adding a final `vectorized_store_prefs' argument that allows the
   application to request non-temporal (streaming) vector store operations
   that write around the cache when storing data to the stripe buffer;
   can improve throughput in some high performance applications.  Also,
   all of the `pull_stripe' calls now support the interleaved buffer
   padding features previously offered only by those functions that
   wrote to 8-bit/sample buffers.
d) Fixed a bug in the `kdu_client::sync_timing' function that caused the
   JPIP client requests to get way ahead of playback during remote browsing
   of animated content, including video.
e) Fixed a minor bug in the logic used to pad interleaved target buffers in
   calls to `kdu_stripe_decompressor::pull_stripe', whereby the padding
   data got written multiple times (wasting processing resources).
f) Fixed a minor weakness that prevented some SIMD accelerated data transfers
   from irreversibly decompressed data using the fixed-point processing
   path to 16-bit/sample output buffers within the `kdu_stripe_decompressor'
   object.
g) Augmented the `mj2_target' class with a `destroy' member.  Prior to this,
   the only way to clean up resources within an MJ2 target was by invoking
   `mj2_target::close', but that function attempts to actually finalize the
   movie file and this will cause an error to be generated if no video frames
   have been written at all.  This can be annoying if you are trying to
   clean up the object after catching exception (e.g., to process a user
   interaction error or to abort some kind of processing), so we provide
   `destroy' to clean things up without file writing, errors or warnings.
h) Augmented `jp2_input_box::load_in_memory' with the ability to use an
   externally supplied memory buffer rather than always allocating one
   from scratch.
i) Fixed a minor issue for cross-platform Mingw builds, being the replacement
   of "<Ws2tcpip.h>" with "<ws2tcpip.h>" in "comms_local.h".
j) Corrected a flaw in the memory allocation logic within the
   `kdu_region_compositor::internal_allocate_buffer' function that could cause
   faults when compositing to extremely large buffer surfaces.

5. Changes and additions to the Kakadu demo apps
a) Corrected a longstanding error in the way that the logic within
   "kdu_v_compress" that automatically configures visual weights.  The
   error applies only in cases where input video frames have sub-sampled
   image components (e.g., 4:2:0 video where Cb and Cr components are
   sub-sampled by 4 relative to Y).  The error resulted in inappropriate visual
   weights being set for the sub-sampled (typically chrominance) components.
   Also, in multi-threaded cases (the usual case) the visual weights for
   the even frames would have been slightly different to those for the
   odd frames.  If you have been using the `set_default_colour_weights'
   function from this demo app as-is in a commercial application, you are
   advised to incorporate the changes that have been made to that function
   to fix the above-mentioned problems.
b) A new demo application has been introduced, "kdu_vcom_fast" which does
   all the same things as "kdu_v_compress" but uses a processing paradigm
   based upon multiple multi-threaded frame processing engines that is
   roughly a mirror image of that used in "kdu_vex_fast".  Where
   "kdu_vex_fast" does high performance video decompression/rendering,
   "kdu_vcom_fast" does high performance video compression.  The new
   demo app can generate MJ2 files or JPX animations and it allows you
   to explore the optimal number of threads to deploy to each independent
   frame processing engine.  Platforms with only a modest number of CPUs
   will not see any throughput advantages from using multiple processing
   engines, but for massively parallel platforms with 30 or 40 logical
   CPUs or more, this application allows you to use all available resources
   for maximum throughput, while still retaining low delay and memory
   utilization.  The demo-app has been designed to be highly re-usable so
   that it can be readily adapted to most video compression applications,
   including those that involve no files.
c) Added support for "kdu_compress" to read TIFF files with the FillOrder=2
   (LSB-to-MSB) bit order.  Files of this form are unusual and warned against
   by the TIFF standard, but do exist in some quarters.  Previous versions
   silently failed on such files, yielding scrambled compressed imagery.
d) Added a "-precise" option to "kdu_vex_fast".
e) Modified the command-line syntax used with "kdu_vex_fast" to set
   processor affinity -- the new syntax is much more comprehensive and
   hopefully also easier to understand since there are not multiple variants.
   The new "kdu_vcom_fast" uses exactly the same syntax to assign CPU
   affinity.

Changes from KDU-7.3.3 to KDU-7.4
---------------------------------
This release involves some substantial improvements to the core codestream
and multi-threaded processing machinery, to Kakadu's JPIP client, and to a
number of the demo applications, in addition to some important bug fixes.
The update summary is organized under various categories as follows:
1) Significant changes and bug fixes in the core system
-- Fixed an error in the core multi-threaded processing machinery which
   could sometimes lead to growing lists of unused thread-domain-sequence
   objects within thread-domains.  The logic for cleaning these up had
   become partly broken with changes introduced in previous versions but this
   would have gone unnoticed in almost all applications, because the problem
   only surfaces in some applications which create a large number of thread
   queues with progressively increasing sequence indices for prioritised
   work flow.  One type of application whose performance may have suffered
   from this is video compression/decompression using work flows similar to
   those found in "kdu_v_compress", "kdu_v_expand" and "kdu_vex_fast", but
   only with long video sequences.
-- Increased the efficiency of calls to `kdu_thread_entity::join' and
   `kdu_thread_entity::terminate' when invoked on thread-queues that have
   already completed (often happens).
-- Arranged for `kdu_thread_entity::advance_work_domains' to be invoked
   regularly from within core codestream background processing jobs, since
   some of these jobs might have a long duration if, for example,
   background codestream parsing operations are having trouble keeping up
   with the processing throughput of the rest of the system.
-- The core codestream machinery has been augmented with the ability to
   schedule tile opening operations to internal background processing jobs
   (as with other background processing that parses/allocates precincts
   slightly in advance of when they are needed and/or performs incremental
   codestream flushing operations).  This is achieved via the new
   `kdu_codestream::open_tiles' function that allows you to open multiple
   tiles at once or schedule their opening in the background.  This function
   is matched by a `kdu_codestream::access_tile' function that allows you to
   access already opened tiles or wait for background opening to complete,
   as well as an augmented `kdu_tile::close' function that allows tile
   closure to also be completed in the background.  These are important
   enhancements for multi-threaded processing with a large number of thread
   resources, since they allow an implementation to avoid the risk that any
   thread gets blocked contending for a critical section within the core
   codestream machinery.  For single-tiled codestreams, these features make
   no difference, but for codestreams with a very large number of small tiles,
   they can significantly increase performance.  These improvements are
   particularly well demonstrated by the "kdu_buffered_compress" and
   "kdu_buffered_expand" demo apps, both of which now exhibit much better
   CPU utilization on highly parallel systems when processing codestreams
   with small tiles.
-- Introduced a new `ORGtpart_interrupts' parameter attribute, for use in
   bounding the number of tile-part boundaries that might be introduced by
   incremental flushing, due to temporary interruptions in the set of
   compressed precincts available from multi-threaded processing engines
   during a background incremental flush operation.  This attribute allows
   you to carefully control the maximum number of tile-parts for a tile, as
   supplied with the `ORGgen_tlm' attribute, without having to worry about
   the possibility of occasional extra tile-parts being introduced by the
   interaction between incremental flushing and multi-threaded processing.
   Such interaction was unlikely in the past but is becoming more likely
   with the multi-threaded processing efficiency improvements introduced
   in this version.  See example (h) in the "Usage_Examples.txt" file for
   the "kdu_buffered_compress" demo app in order to understand this new
   attribute better.
-- Added an extra level of indirection to the `kdu_tile' interface so as to
   provide much better checking for invalid tile references that might arise
   due to application-level programming errors.
-- Corrected an error in the documentation of the slope threshold values
   found within the `kdu_codestream::flush' function.  This is now the second
   time we have corrected the documentation for `flush' in regard to its
   definition of the slope-threshold values, for which we apologise.  However,
   the actual formula used to compute the slope thresholds has not changed
   since version 6.4, where an offset was introduced to gain more headroom
   in the 16-bit logarithmic representation.
-- Modified the definition of KDU_MAX_L2_CACHE_LINE from 128 to 64, as found
   in "kdu_arch.h" (may slightly improve memory utilization efficiency and
   should not adversely affect anything else, since most CPU's do indeed use
   64-byte cache lines for L1 and L2 caches).

2) Changes to `kdu_client' and `kdu_region_animator' for JPIP video/animation
-- Major changes have been introduced into the operation of the
   `kdc_flow_regulator' object inside `kdu_client', along with some more
   minor changes to the way in which this function is used within
   Kakadu's JPIP client machinery.  Ultimately, these changes are all
   to do with the synthesis of byte-limited JPIP requests to maintain
   responsiveness and high communication throughput when working with:
   a) timed requests (as generated when browsing video and animations); and
   b) non-timed HTTP-only transported requests (because the HTTP-only
   JPIP transport requires the client to take charge of flow control).
   The changes mean that JPIP video and more elaborate animations work
   much more robustly over a wider range of channel conditions.  The
   changes have been tested quite extensively over high delay
   trans-continental links with round-trip delays in excess of 300ms,
   browsing high and low frame rate content compressed using multi-component
   transforms.  While occasional drop-outs can occur due to the tight
   balance between responsiveness and throughput which the client aims
   to maintain, overall the performance appears to be very good and will
   get much better once we introduce true timed requests into the
   JPIP protocol -- letting the server manage the timed flow control is
   fundamentally much better.  For the moment, we recommend the use of
   the HTTP-TCP and HTTP-UDP transports with high frame rate animated
   content, but the HTTP-only transport now also works.
-- Modifications have also been introduced into the `kdu_region_animator'
   object to improve JPIP animations.  In particular, the region-animator
   often needs to generate advance requests for metadata, without which
   the main imagery requests cannot be formulated.  When browsing
   animated content at high frame rates, these metadata requests need to
   be staged well in advance of the main imagery requests and posted over
   the OOB (out-of-band) request service that is provided by Kakadu's
   JPIP client.  Prior to the modifications in this release, this was
   all happening, but the policy used to stage the requests was not
   completely suitable for high delay channels.  With the new modifications
   there are rarely if ever any hold-ups due to lack of availability of
   metadata after animated streaming has commenced.
-- Modified the JPIP client machinery in `kdu_client' slightly so that
   requests involving a very large number of cache model manipulation
   statements are split into smaller requests.  This ensures that the
   posted HTTP request is kept at a manageable size and not discarded
   by server imposed length limits.  The need for these changes arises
   only in extreme cases.

3) Changes to the `kdu_stripe_compressor/decompressor' support API's
-- Modified the way in which the `kdu_stripe_compressor' object handles
   tiled imagery in multi-threaded contexts.  Previously, tile processing
   engines were cleaned up immediately after all data was pushed into them,
   which usually forced the main thread to wait until all jobs associated
   with the tile were complete, before another tile processing engine could
   start doing anything.  Now, we overlap the processing of tiles with their
   neighbours, or entire rows of tiles with the next row of tiles, cleaning
   a tile (or row of tiles) up only once all data has been pushed into the
   next one.  This tends to use multi-threaded processing resources better.
-- The `kdu_stripe_compressor' has also been modified to use the new
   background-scheduled tile opening and closing capabilities of the core
   system that are described above, which ensures that these operations
   do not need to contend with incremental codestream flushing or memory
   trimming operations for access to critical sections in the core codestream
   processing machinery.
-- The `kdu_stripe_compressor::start' function has been augmented
   with an optional `env_tile_concurrency' argument that can be used to
   control the number of concurrently open tiles.
-- The `kdu_stripe_decompressor' object has been modified in similar ways
   to achieve better utilization of multi-threaded resources when
   processing heavily tiled codetreams.  Again, an extra `env_tile_concurrency'
   argument has been added to `kdu_stripe_decompressor::start' which allows
   you to optimize tiled decompression performance for your platform.
-- Fixed a bug in the implementation of `sse3_floats_to_uint8_ilv3' where
   the source pointers were not being advanced sufficiently on each iteration
   of the SIMD processing loop.  This, combined with earlier fixes, seems
   finally to have corrected all branches of the SIMD sample data conversion
   logic that helps make `kdu_stripe_compressor' and `kdu_stripe_decompressor'
   the preferred API's for generic compression/decompression, except for
   interactive applications where the preferred API's are
   `kdu_region_decompressor' and `kdu_region_compositor'.

4) Significant changes to demo apps
-- The modifications to `kdu_stripe_compressor' described above allow much
   more efficient compression of codestreams with a large number of small
   tiles.  These features are demonstrated by the "kdu_buffered_compress"
   demo app, which has been enhanced with a "-tile_concurrency" argument
   to give you control over the new stripe compressor features.  Moreover,
   the new implementation of "kdu_buffered_compress" is arguably clearer
   and easier to understand, while also providing better default parameter
   selection by passing the values -1 for the `env_dbuf_height' and
   `env_tile_concurrency' arguments to `kdu_stripe_compressor::start',
   by default.
-- If you are interested in compressing tiled codestreams, you are strongly
   recommended to review the latest usage examples for "kdu_buffred_compress"
   in "Usage_Examples.txt".
-- Parallel changes have been made to the "kdu_buffered_expand" demo app, to
   exploit enhancements in the `kdu_stripe_decompressor' object.  Again,
   these changes should result in significant throughput enhancements for
   decompression of heavily tiled codestreams, especially when executed without
   any output files (no "-o" argument) so that content is rendered directly
   to a memory buffer but without the costly step of writing to disk which
   often presents the main bottleneck.
-- The "kdu_buffered_compress" and "kdu_buffered_expand" demo apps have
   both been augmented with the ability to read/write image samples from/to
   disk via an auxiliary thread so that file I/O is less likely to hold
   up the main processing pipeline.  While this is not important to
   applications that work with image samples that are held in memory buffers,
   it does help to ensure that the performance of these demo apps reflects
   what Kakadu can really achieve, even when working with files -- of course
   this is contingent on having a high performance file system that can
   keep up with the very high processing throughput that is usually
   achievable on modern multi-core platforms.  Many decompression workloads
   generate data faster than the disk system can absorb the results, but you
   can always eliminate this issue either by not specifying any output files
   or by supplying "/dev/null" to the "-o" argument -- on Linux/OSX systems,
   each occurrence of /dev/null in the "-o" argument will be considered as
   a single-component output file.
-- The "kdu_compress" and "kdu_expand" demo apps have also been modified
   slightly to take advantage of the new background tile opening/closing
   capabilities introduced to the core codestream machinery, although the
   impact is less for these demo apps than it is for "kdu_buffered_compress"
   and "kdu_buffered_expand", because the latter provide better work flows
   for maintaining concurrent tile processing activities.  
-- The "kdu_compress" demo app now reports the overall compressed bit-rate
   in three ways: as bits/sample taken over the entire codestream (as before);
   as bits/sample taken over only the J2K packets (i.e., excluding
   the parameter marker segments that specify the compression configuration,
   which is usually fixed for any given application); and as bits/sample
   taken over just the packet bodies -- i.e., excluding both codestream and
   packet header bytes, leaving only the bytes that come from the arithmetic
   (or bypass) coding engine.  The second metric can be especially useful
   in view of the fact that the `Creslength' attribute explicitly controls
   only the size of the codestream packets, not the codestream headers.
   The third metric is reported so as to help the user judge the impact
   (rate overhead) associated with different quality layering schemes.
-- The sample data conversion logic used by "kdu_compress" has been expanded
   slightly to support conversion from higher precision source samples
   (up to 16 bits/sample) all the way down to low precision fixed-point
   values for fast (less precise) irreversible compression. This capability
   was requested so that a single 16-bit source image could be compressed
   as an 8, 10, 12, or 16 bit source using the "-fprec" option.

5) Minor changes and bug fixes
-- Corrected an oversight that arose in the separation of "kdu_ubiquitous.h"
   from "kdu_elementary.h", as a result of which "unistd.h" was no longer
   being included in the compilation of "kdu_arch.cpp", as a result of which
   Linux systems failed to correctly report the number of platform CPU's via
   `kdu_get_num_processors'.
-- Fixed a bug in the "kdu_hyperdoc" utility relating to the generation of
   Java bindings for callback functions.  Previously, each call to such a
   callback function associated with a Java-derived Kakadu class added one
   local reference to the JVM's thread-specific local reference table.
   While these get GC'd upon return to Java (or when the thread exits), this
   may not actually happen until a very large number of such calls have been
   dispatched.  To avoid overflow of the local reference table, these
   local references are now deleted immediately, which is good practice
   in general.
-- Slightly modified the `jx_codestream_target::adjust_compatibility' function
   to correct an oversight in which JP2 compatibility was not recognized
   for codestreams using the more recently defined broadcast profiles and
   scalable digital cinema profiles.
-- Added a core function `kdu_codestream::get_packet_header_bytes' that can
   be used to assess the contribution of packet headers to the size of
   the compressed representation -- e.g., by comparing with the value
   returned by `kdu_codestream::get_packet_bytes'.
-- Made adjustments to avoid the myriad of "unused private field" warnings
   that are issued when using the clang compiler with -Wall, as is done in
   the command-line builds for OSX -- XCODE appears to disable these
   warnings by default.  The warnings arise from private separator fields
   that were introduced a long time ago to ensure that portions of some
   classes live in different processor cache lines so as to reduce
   contention in multi-threaded applications.
-- Fixed a minor issue for Mingw builds, being the replacement of
   "<Windows.h>" with "<windows.h>" in "kdu_ubiquitous.h".

6) Various security fixes
-- Quite a number of potential security issues have been corrected in
   the JP2 file format sub-system, in addition to a few in the core
   system.  Many of these were contributed and/or inspired by suggestions
   from Google's Jeff Breidenbach.

Changes from KDU-7.3.2 to KDU-7.3.3
-----------------------------------
-- Took steps to eliminate all of the new warning messages produced by
   XCODE-5.  These were all related to the use of local variables that
   the compiler thought might not always be initialized; in all cases,
   the code branches that may have left the variables uninitialized
   could not possibly occur, but we made adjustments to make sure the
   compiler would not be misled into believing they might not be
   initialized so as to avoid having legitimate warnings obscured by the
   invalid ones.
-- Fixed a bug in the interaction between `kdu_region_compositor' and
   `kdu_region_decompressor' for CMYK imagery, that was introduced with
   the use of a different `kdu_region_decompressor::process' function by
   the compositor in recent releases.  The solution was to augment the
   single buffer `kdu_region_decompressor::process' overloads with a final
   `max_colour_channels' argument that applications and the region compositor
   can use to insist that no more than a certain number of channels be passed
   back to the caller as colour channels -- take a look at the API function
   descriptions and you will see that this is a good general way of
   resolving this issue.
-- Fixed a bug in the `ssse3_floats_to_int16_ilv1' conversion function within
   "ssse3_stripe_transfer.cpp", where the condition for the for loop was
   accidentally mistyped.
-- Fixed a bug in `kd_precinct_server::get', where the `plock_size_classes'
   and `glock_size_classes' members accidentally get interchanged.  This
   could have resulted in memory leaks and assertion failures (debug mode).
-- Fixed a bug in `kd_block_encoder::schedule_new_jobs' and
   `kd_block_decoder::schedule_new_jobs', where a possibility existed that
   one thread might identify all jobs as having been scheduled within a
   queue sequence, while another thread was still in the process of
   scheduling earlier jobs; we have not observed this to actually occur,
   but the risk existed and has been eliminated.
-- Fixed a bug in the implementation of `kdsx_stream::read' within
   the "kdu_server" application, which could (with very low probability)
   have caused incorrect reading of content from a source file being
   served.
-- Fixed another minor flaw in the "kdu_server" that could affect the
   serving of highly complex JPX sources -- the minor flaw in the function
   `kd_window_context::sequence_active_bins' may have prevented the
   server from correctly issuing the "window-done" end-of-message reason
   code to the client, adversely affecting any clients that are counting
   on the "window-done" status.
-- Corrected a bug in the `kdu_run_queue' class that is provided mostly
   as a starting point for advanced applications to build upon Kakadu's
   multi-threading environment.  The constructor for this object failed
   to initialize one of the member variables.
-- Corrected a bug in `j2_channels::finalize' involving consistency tests
   for the `have_chroma_key' option.  In practice, the use of chroma
   keys with JPEG2000 is not all that appealing, so this error has
   not surfaced before.

Changes from KDU-7.3.1 to KDU-7.3.2
-----------------------------------
-- The main reason for this release is to fix a tiny, yet potentially
   serious bug accidentally introduced into KDU-7.3 while allowing
   precinct allocation to proceed in a different thread from other
   core codestream maintainance functions, without acquiring a
   critical section.  The bug had nothing to do with the added
   functionality, but represents 2 lines from an intermediate
   version of the code that were accidentally not purged.
-- In addition to the above, this version has the following enhancements:
   a) Additional accelerations for the resampling functions that are used
      within `kdu_region_decompressor' to map all image components to a
      desired target resolution.  The resampling logic can represent a
      bottlneck on systems with a large number of CPU's since it proceeds
      in a single thread -- especially relevant when rendering video
      with sub-sampled chrominance components.
   b) The XCODE builds now target OSX10.8 at the platform level (not just
      building against the 10.8 SDK) and this exposed some deprecated
      file-system related functions within "kdu_macshow".  These deprecated
      functions relate to the low level FSRef type; we have substituted
      them with methods based on NSURL that is now the preferred interface
      for file system functionality.  These fixes, together with the
      gesture recognition that was added in version 7.3.1, leave "kdu_macshow"
      in a position where it can most likely be ported to iOS with
      considerable ease -- however we have not tried this ourselves.

Changes from KDU-7.3 to KDU-7.3.1
---------------------------------
-- This release is primarily about improving the user interaction and
   responsiveness of the "kdu_macshow" and "kdu_winshow" application, as
   follows:
   1) Previously, both applications temporarily halted animation playback
      during live resize and continuous scrolling/panning operations; this
      is no longer an issue.
   2) Scrollbars on kdu_winshow were previously broken during
      metadata-driven animations -- they now move around correctly to
      reflect what the metadata is driving, while disabling user control.
   3) Rendering now occurs continuously during live resize/scroll operations
      so that holes in the view are filled in almost instantaneously.
   4) Mouse-controlled panning (moving the mouse around the view while the
      shift key is depressed) is now greatly improved; moreover, when if the
      cursor is within a user-defined focus box when the mouse button is
      depressed, the focus box is shifted around instead.
   5) The "kdu_macshow" application responds to rendering and JPIP client
      events in a more rapid manner than before, without having to wait for
      idle points between user interaction events.
   6) The "kdu_macshow" application now supports trackpad gestures for pan
      (two finger glide) and interactive magnification (two finger pinch),
      which affect either the view or a user-defined focus box, depending
      on the location of the pointer when the gesture begins.  These are
      smooth and efficient for still images, full frame rate video, and
      dynamic metadata-driven animations.
   7) Other minor imperfections in the "kdu_show" user interfaces have
      been ironed out, such as dynamic updating of all status bar variables
      during animation, the introduction of efficient focus box highlighting
      during animation, and elimination of frame update stalls that
      previously affected "kdu_macshow" during metadata-driven animations.
-- A bug has been fixed in `kdu_region_animator' that could cause an
   infinite loop to present itself in KDU-7.3 when rendering high frame
   rate animations while interacting with a remote JPIP server.
-- A number of potential security issues have been fixed in the core
   system and JP2-family file format parsing logic.  Many of these were
   suggested by Felix Grobert from the Google security team.

Changes from KDU-7.2.2 to KDU-7.3
---------------------------------
-- Augmented the multi-threaded background processing capabilities of the
   `kdu_codestream' machinery to allow incremental codestream generation
   and new precinct resource allocation tasks to proceed in parallel, within
   separate threads, thereby reducing the likelihood that the main compression
   tasks get starved of resources while codestream generation and flushing
   is taking place in the background.  This enhancement increases the
   throughput that can be achieved on platforms with a very large number
   of CPU cores.
-- Added support for the Scalable Digital Camera profiles that are known
   to Kakadu as CINEMA2S (2 quality layer version of CINEMA2K),
   CINEMA4S (2 quality layer version of CINEMA4K) and CINEMASS (long term
   storage profile with up to 5 quality layers, supporting frame sizes up to
   16386 x 8640).  The profile testing and default parameter configuration
   logic has been augmented to handle these profiles, so that you only need
   to add Creslengths specifications and (preferably) "-rate" constraints to
   create high quality profile compliant codestreams.  See "Usage_Examples.txt"
   for how to supply the Creslength parameters for these scalable digital
   cinema profiles.
-- Augmented the `kdu_region_compositor' object with a new member function
   `set_colour_order', which can be used to control the order of colour
   channels within the rendered surface buffers that are produced.  This
   allows sufficient flexibility to support the most efficient graphics
   formats directly on both Windows and OSX platforms without additional
   application-layer data manipulation.
-- Updated the "kdu_macshow" demo program's image painting procedure to
   avoid a problem with deprecated functions that are not supported under
   the OSX 10.8 SDK.  The XCODE workspaces now compile against the 10.8 SDK
   by default, but older SDK's are supported.
-- Also updated the "kdu_macshow" program to use OpenGL when rendering
   animations and video.  Everything looks the same, but tear-free 60Hz
   refresh on all animations now happens even with multiple video windows
   open at once; this should also consume less power when rendering
   animation intensive content and should port well to similar implementations
   on mobile devices.
-- Very slightly modified by "kdu_macshow" and "kdu_winshow" (one line change)
   so that they now render animation frames to a slightly larger buffer than
   the viewport would dictate so as to make interactive panning more
   compelling during animations.  This has always been done for non-animated
   content, but in animation mode the pan-facilitating buffer margin was
   set to 0 in previous versions.  This does take a little extra rendering
   effort, but Kakadu is fast enough that this is rarely an issue.
-- Made minor tweaks to the channel estimation logic in "kdu_server" that
   have improved performance on wide area networks using the HTTP-TCP
   transport.  Also removed a problem created by using the rate estimation
   machiner for HTTP-only transports where flow control is supposed to be
   managed by the client rather than the server.  HTTP-only transported
   services now run much faster than before on most networks.
-- Corrected a minor error in the `kdu_region_animator' that could manifest
   when slowly animated frames are refreshed frequently (due to dynamic
   rotation, panning, scaling etc.); the problem may have resulted in
   some intermediate frames being dropped, but nothing disastrous.  Along
   with this, a minor logic change was introduced to the function
   `kdu_region_animator::backup_to_display_event_time' which effectively
   eliminates the possibility of display flickering during interactive
   pan, zoom and rotation within ongoing animations.
-- Corrected an oversight related to the function
   `kd_compressed_stats::get_pcrd_opt_min_threshold' function that was first
   introduced in KDU-7.2.  This oversight meant that the rate control logic
   may have selected too large a distortion-length slope threshold (generating
   too little data) in multi-threading environments under some unusual
   conditions on the source statistics.  The oversight has been corrected
   by arranging for the `kd_codestream::stop_multi_threading' function to
   invoke `kd_compressed_stats::transcribe' to transcribe statistics from
   threads that have finished into the master statistics object, while also
   ensuring that `get_pcrd_opt_min_threshold' returns 0 if there are no
   statistics available at all.
-- Corrected a subtle race condition within `kd_encoder::update_dependencies'
   and `kd_decoder::update_dependencies' that could, under extremely rare
   conditions result in an invalid attempt to schedule non-existent code-block
   processing jobs within multi-threaded deployments.
-- Fixed an oversight in `ssse3_floats_to_uint8_ilv1', which
   could cause the floating point `kdu_stripe_decompressor::pull_stripe'
   function to retrieve invalid data in the final few columns of a
   decompressed image region, depending on the dimensions.  This accelerator
   function was introduced with many others in KDU-7.2.1.
-- Corrected an oversight in the checking for legal code-block dimensions,
   as supplied by reading a COD/COC marker segment or when compressing
   a new codestream.  The JPEG2000 standard strictly bounds the code-block
   dimensions, so that no code-block may have more than 4096 samples.  While
   this bound was checked during marker writing, it was not previously
   checked prior to block encoding, which generally starts before the
   parameters marker segments are written.
-- Fixed a minor bug in `jp2_input_box::read' that could be triggered with
   weird declared box sizes.
-- Fixed a minor bug in the testing for legal codestream parameters within
   `kd_codestream::construct_common'.
-- Fixed an oversight in `jx_metanode::finish_reading' in that rubber length
   boxes were not handled properly.
-- Incorporated length consistency checks for ICC profiles embedded within
   JP2/JPX colour description boxes.
-- Replaced an "assert(0)" statement in `kdu_tile::initialize' with a
   run-time error through `kdu_error' to as to ensure that the issue is
   caught in release mode.
-- Fixed a bug in `kd_compressed_input::load_buf' that could have caused
   an endless loop while reading a codestream with errors in it, if this
   were done from a compressed data source with the `KDU_SOURCE_CAP_IN_MEMORY'
   attribute -- i.e., fully buffered compressed data sources, containing
   codestreams with errors in them.
-- Fixed a minor (compiler-dependent) error in the sample data conversion
   logic in `kdu_stripe_compressor', which affected only the conversion of
   32-bit integers representing unsigned original samples to 16-bit signed
   data types -- the issue was that subtraction of 0x80000000 instead of
   (1<<31) causes some compilers to interpret the value as unsigned,
   causing the ensuing downshift to be an SRL rather than SRA operation.
-- Corrected a very minor error in the compatibility testing logic for
   calls to `kdu_line_buf::get_floats' and `kdu_line_buf::set_floats', both
   of which contain typos in the condition that tests for compatibility of
   the underlying buffer type, reducing the strength of the compatibility
   test -- this bug is unlikely to affect anything other than the run-time
   checking of non-native (Java/C#) interface calls that involve floating
   point data.
-- Fixed a problem in `kdws_renderer::menu_FocusHighlighting' (windows version
   of kdu_show) and `kdms_renderer::menu_FocusHighlighting' (Mac version of
   kdu_show) that could result in the stalling of an animation/video 
   rendition if the focus highlighting policy is changed dynamically.

Changes from KDU-7.2.1 to KDU-7.2.2
-----------------------------------
-- Finally found and fixed a long standing bug in the channel state
   estimation machinery within the "kdu_server" application.  The
   main issue was that the transmission window threshold was being
   calculated incorrectly, using a division operation in place of a
   multiplication.  This problem has existed in many Kakadu releases
   and has artificially limited the maximum achievable transmission
   rate.  Some other modifications have been made to the channel state
   estimation machinery for both TCP and UDP transport protocols, so as
   to ensure that channel estimates can adapt quickly and robustly.
-- Fixed a minor bug in "kdcs_comms.h" where textualization of 64-bit
   integers for the purpose of HTTP communications produced comma
   separated digit triplets (easy to read, but not valid for JPIP
   query/response fields).
-- Fixed a bug in the UDP transport implementation within `kdu_client',
   where data chunks that arrive after the EOR (End-Of-Response) chunk in
   response to a request were not correctly attributed to the request,
   leading to the unnecessary accumulation of incomplete requests under
   high packet loss conditions -- these chunks must then be explicitly
   abandoned by the client.
-- Increased the relative thread priority associated with the network channel
   service threads in Kakadu's client and server components.
-- Fixed a minor logic error in `kdcs_channel_monitor::run_once', which
   provides all networking services to Kakadu's client and server components.
   Specifically, there was previously a potential problem created by
   invoking the channel service functions for channels with outstanding
   timeout deadlines before calling looking for I/O events -- if a channel
   constantly reinstalls timeout conditions which have already passed, the
   I/O conditions will never be checked.  As it turns out, this problem
   degraded the performance of the "kdu_server" application when servicing
   clients that use the UDP transport protocol.
-- Made a minor change to the logic used to decide when JPIP requests
   should be issued within `kdu_client' so that channel close ("cclose")
   requests are not forced to wait for the number of incomplete requests
   to reduce below an internal limit.  This allows connections to be shut
   down rapidly even if communication of previously requested data has
   been unusually delayed.
-- Corrected a problem with the JPIP "abandon" field that is used with
   UDP communications.  Specifically, it can easily happen that a
   massive number of packets need to be abandoned, leading to HTTP
   requests that exceed the legal length.  The adopted is to collapse
   an excessively long list of abandoned chunk ranges into a smaller
   set of ranges, which still span the chunks that need to be abandoned.
   This can occasionally result in the abandonment of more data chunks
   than necessary, resulting in some inefficiency, but this is very rare.
-- Fixed a potential problem with serving HTTP-UDP transported JPIP channels,
   where the server might not close a session immediately if the client
   closes the HTTP channel and shuts down before acknowledging all in-flight
   transmitted data on the UDP channel -- it turns out that there is no
   need for the server to wait for potential acknowledgements in this case,
   the JPIP protocol does not guarantee delivery over UDP unless the client
   is prepared to issue "abandon" requests and it cannot do so after
   issuing a "cclose" request.
-- Fixed a minor issue with the `kdu_region_animator' object which resulted
   in incorrect handling of "pause" frames when playing backwards through a
   JPX animation.
-- Removed an erroneous assert statement from
   `kd_request_queue::fix_timed_request_discrepancies'.

Changes from KDU-7.2 to KDU-7.2.1
---------------------------------
-- Rationalized the "makefiles" for non-MSVC/non-Xcode builds, adding
   a complete set of makefiles for 64-bit Windows builds under MINGW64
   (Minimal GNU for Windows) toolchain.
-- Added SIMD processing speedups to the front-end data format conversion
   logic found in `kdu_stripe_compressor' and `kdu_stripe_decompressor'
   so that conversion between Kakadu's four native formats and the
   application's preferred format does not represent a bottleneck for
   performance, where developers choose to use these simplified API's.
   The documentation for these API's has also been expanded somewhat to
   identify what buffer configurations are likely to be handled most
   efficiently.
-- Extended the "kdu_buffered_compress" and "kdu_buffered_expand" demo
   apps to work with PPM and BMP files (1, 3 or 4 bytes/pixel) as well as
   PGM and raw files, adding other features such as the ability to
   control internal processing precision via "-precise" and "-fastest"
   options.
-- Extended the "kdu_buffered_compress" application to allow an image to
   be buffered in memory and automatically replicated during compression,
   so that the performance of Kakadu on very large images can be tested
   without dependence on the file system; on powerful platforms, Kakadu can
   compress data at much higher rates than can be delivered by a typical
   disk or SSD-based file system.  Also added the option for
   "kdu_buffered_compress" to send its output to a null "structured cache"
   data target if the "-o" option is omitted (as done by "kdu_compress").
-- "kdu_buffered_compress" has also been augmented with a "-stats"
   option to report compression stats (working memory, achieved bit-rate,
   distortion-length slope threshold).  Moreover, it has been modified
   to use the same set of visual weights as "kdu_compress" by default, with
   a "-no_weights" option to turn this default weighting policy off.
   This ensures that both compression applications behave the same way,
   to avoid any confusion that might otherwise be caused.
-- All of the compression and decompression demo apps (kdu_compress/expand,
   kdu_buffered_compress/expand, kdu_v_compress/expand and kdu_vex_fast)
   have had the default policy associated with their "-double_buffering"
   argument modified so that the default is now to pass the value -1 to
   `kdu_multi_analysis::create'/`kdu_multi_synthesis::create' (possibly
   via higher level API's) for its `buffer_rows' argument.  The
   `kdu_multi_analysis::create' and `kdu_multi_synthesis::create' functions
   now interpret a -ve `buffer_rows' argument as a request for the
   stripe buffers that interface the multi-component transform machinery
   with the spatial DWT machinery to be automatically allocated a suitable
   height, taking into account the width of the stripes to be processed --
   this usually leads to a better trade-off between cache demands and
   thread scheduling flexibility than can be achieved by assigning a fixed
   buffer height for all applications.  Of course, you can still explicitly
   dimension the buffers, as before.
-- Fixed a problem with the "kdu_client" implementation that caused
   requests for a large number of new JPIP channels to be unduly
   delayed; moreover, in some circumstances, this problem may have
   manifested itself in slow transport over HTTP-TCP and HTTP-UDP
   transports, because the creation of new JPIP channels sometimes
   left existing channels with an artificially enlarged understanding
   of their number of outstanding requests.
-- Split the "kdu_elementary.h" core system file into two parts, with the
   most elementary part, "kdu_ubiquitous.h", free from class and in-line
   function definitions.  This has no effect upon applications, which should
   continue to include "kdu_elementary.h" as and when they did before; however
   it does improve the structure of some of the Kakadu internals, especially
   where AVX optimizations are incorporated.
-- Modified the XCODE project files and MAC-xxx makefiles so as to use
   the "clang++" compiler in place of "g++" and to build AVX optimizations
   where appropriate.  This was previously not possible on MAC platforms,
   for which the highest GCC version was 4.2.
-- Fixed a minor error in "kdu_winshow" that caused it to open MJ2 files
   initially in single component mode, as opposed to full colour composited
   frame mode.
-- Fixed a subtle yet important problem in the distortion estimation
   algorithm used by the block encoder that could, under extremely rare
   conditions, cause code-block bit-streams to be excessively truncated,
   resulting in unexpectedly large distortion reconstructed image distortion.
   As it turns out, this problem has existed in all previous versions of
   Kakadu, but not in any of the speed-pack releases of Kakadu.  The
   problem affects lossy compression only (not lossless compression).
-- Made a very minor change to the one of the atomic intrinsics in
   "kdu_elementary.h" for Win64 builds so that 64-bit versions of the
   Managed library ("kdu_mni.dll") can be built despite a latent
   bug in the VS2008 and VS2010 compilers -- but was apparently fixed
   in VS2012.
-- Fixed a minor problem in "kdu_macshow" that could result in tiny
   cracks appearing during image navigation when rendering to a retina
   display: solution was simply to extend the region invalidated by
   new imagery rendered in `on_idle' by one pixel all around so that
   the GPU scaling machinery has sufficient data to work with.
-- Fixed a subtle bug in the UDP transport logic within	 `kdu_client'.
-- Fixed a minor bug in the auxiliary rate calculation in "kdu_server" and
   also modified the `estimated_network_usecs_per_byte' member to use
   a floating point representation, rather than an integral type.
-- Removed a typo from the core system, where "KDU_NO_SSSE3" was
   accidentally entered as "KDU_NO_SSSE3n" in one place -- relevant only
   if building in environments where SSSE3 instructions are not available.
-- Made minor corrections to the documentation in "Usage_Examples.txt".

Changes from KDU-7.1 to KDU-7.2
-------------------------------
1) Introduced a new job queue design at the heart of the core multi-threaded
   sub-system that involves an absolute minimum of bus locking operations
   to enqueue and dequeue processing jobs.  There are absolutely no
   spin locks or critical section locks associated with the distributed
   job enqueueing and dequeueing processes -- mutexes are employed only
   for macroscopic operations such as the creation and destruction of
   job queues.
   -- NB: There has been a small change in the `kdu_thread_queue::bind_jobs'
      and `kdu_thread_queue::schedule_jobs' functions in that these functions
      now take arrays of `kdu_thread_job' pointers, whereas previously they
      took arrays of `kdu_thread_job_ref' pointers.  The additional level
      of indirection through opaque pointers has been removed, for a variety
      of reasons.  This is unlikely to impact any third party applications,
      but any required changes should be readily apparent if existing apps
      fail to compile.
2) Added comprehensive support for what we call "JPX containers"
   (these are the new Compositing Layer Extensions boxes introduced by
   IS15444-2/AMD3), as well as JPX files with aggregated codestreams
   (Multiple Codestream boxes).  The new `jpx_source' and `jpx_target'
   implementations correctly generate and interpret metadata embedded
   within JPX containers, which is by no means simple but can be used
   in very powerful ways.  As a result, Kakadu and its demo applications
   now support multiple JPX presentation threads (or tracks), each with
   its own animation stream.  It means that you can effectively build
   metadata that references whole presentation tracks or portions of them
   in an efficient way.  It also means that Kakadu can very efficiently
   encapsulate video (including live video -- i.e. where the sequence
   length is unknown a priori or unbounded) within a JPX file and access
   it in a random or streaming fashion via JPIP, without the linearly
   growing random access overhead that was previously incurred by using
   JPX files to store video -- prior to these innovations, all codestreams
   and their header boxes needed to live at the top level of a JPX file so
   that a massive number of placeholder boxes (each ~28 bytes) might need
   to be sent to a remote client as minimal representation for the codestreams
   preceding one that might be of interest.
      These innovations make JPX files an excellent choice for embedding
   video, especially where auxiliary navigation or annotation
   metadata is to be embedded as well, or for advanced applications such as
   hyperspectral video using multi-component transforms for both efficient
   compression and to enable a diversity of presentation modes (via
   presentation threads).  For example, a multi-component transform may
   provide numerous output components that are synthesized from the source
   content through custom transformation steps.  JPX compositing layers may
   be defined to present these output components in interesting ways and
   these rather complex descriptions can be embedded within JPX containers
   so that they self-replicate to describe video content with any number
   of distinct presentation threads.
3) Greatly enhanced the JPX metadata and animation management/discovery
   features offered by Kakadu.  This has been done largely in response to
   the new features implied by JPX containers (Compositing Layer Extensions
   boxes).  You can now: count tracks; count frames in a track; count
   temporal duration of tracks or of the frames in a track that precede
   one that is of interest; and search for composited frames that match
   number list descriptions, search forwards and backwards.
   These features are very efficient and are designed to work with content
   that may contain millions of frames and numerous presentation tracks.
4) Extended the JPX file writing capabilities of Kakadu.  Specifically,
   it is no longer necessary to write all of the auxiliary metadata via
   a single call to `jpx_target::write_metadata'.  You can now interleave
   metadata and imagery writing and you can add metadata on the fly as
   content becomes available.  Moreover, all of this is done while being
   careful to avoid polluting the top-level of the file with a huge number
   of boxes -- that would interfere with efficient random access and
   interactive JPIP browsing.
5) Extended the "kdu_server" demo application to correctly serve JPX
   files that use containers (Compositing Layer Extensions boxes) and
   Multiple Codestream boxes.
   -- Note 1: this means that JPIP works correctly with such files, which
      can greatly reduce the amount of file format metadata that needs to
      be communicated when interactively browsing a source with a large
      number of compositing layers and codestreams.
6) Upgraded the "kdu_merge" demo app to support generation of JPX files
   with containers -- this was done largely for testing purposes, but it
   allows some very interesting configurations to be synthesized using
   command-line options (admittedly not trivial to get your head around).
7) Upgraded the "kdu_show" (windows and Mac versions) demo app to support
   editing, interpretation and navigation for metadata embedded within
   JPX containers.  The "kdu_show" application now also supports the
   playback of muliple presentation tracks (a.k.a. threads) induced by
   JPX containers and the synthesizing of metadata-driven animations that
   take advantage of all of the features of JPX containers.  For example,
   holding the shift key down while double-clicking on a metadata entry
   that is embedded within a JPX container will synthesize a metadata-driven
   animation that plays out all the associated content from the container,
   taking repetitions of the container into account.  These capabilities are
   all realized using the underlying powerful API's, rather than being
   specific to the "kdu_show" demo app itself.
8) Upgraded the "kdu_v_compress" demo app to support writing of video directly
   to a JPX file, along with metadata, taking advantage of JPX containers.
   This demo app now supports four compressed video formats: MJ2, JPX, JPB
   (broadcast streams) and MJC (raw) video streams.  This demo is the best
   platform on which to build live video compression applications.  If you
   want to use Part-2 codestream features for your video or embed rich
   auxiliary metadata, or facilitate interactive access via JPIP, JPX is by
   far the best choice, except that there is not currently any support for
   audio in the JPX file format.
9) Upgraded the "kdu_v_expand" demo app to support JPX input files,
   processing the first codestream in each animation frame -- of course JPX
   animations are not as simple as regular video, potentially involving many
   different codestreams, composited in interesting ways, but this upgrade
   allows "kdu_v_expand" to decompress content written to a JPX file by
   "kdu_v_compress", which also means that video constructed using advanced
   Part-2 codestream features, such as multi-component transforms, or
   arbitrary wavelet kernels, can be passed through "kdu_v_expand".
10) Upgraded the "kdu_vex_fast" demo app to support JPX input files, handling
    them in exactly the same way as "kdu_v_expand".
11) Substantially reduced the number of file handles and synchronization
    objects required by the "kdu_server" application, especially when serving
    source files that contain a large number of embedded codestreams.
12) Augmented `kdu_region_animator' with the ability to calculate and
    issue its own window-of-interest requests to a `kdu_client' interface
    and to analyze the current status of those requests.  This is achieved
    primarily through two functions, `generate_imagery_requests' and
    `notify_client_progress', that are used by an application interested
    in presenting animated content that resides on a remote server.  The
    `kdu_region_animator' object provides all the state information and
    machinery to manage the timing and request/response management for
    JPIP video and other more ellaborate animations, including
    complex composited animations and metadata-driven animations.  These
    new capabilities are demonstrated by the "kdu_macshow" and "kdu_winshow"
    demo applications, whose animation and JPIP-related auto-refresh logic
    has also been rationalized and simplified.  In other words, JPIP video
    is now here!
13) To support JPIP video/animation, the `kdu_client' interface has been
    augmented slightly to support application-defined `custom_id' values
    that can be used to tag and keep track of the status of posted
    windows of interest, along with a `get_window_info' function that
    provides enhanced status information about posted windows of interest,
    including an indication of the amount of "service time" that a given
    window of interest may have received from the server.  The
    `post_window' function has also been augmented with the ability to
    associate a `service_time' with a window of interest, allowing the
    application to post a sequence of so-called "timed requests".
       Three new member functions, `sync_timing', `get_timed_request_horizon'
    and `trim_timed_requests', have been added to simplify the process of
    determining when and how timed requests should be issued during an
    interactive video/animation, so as to maximize the responsiveness of
    the experience.
       Exactly how timed requests are handled by the `kdu_client'
    implementation should be irrelevant to the application, but the current
    approach is based on the issuing of requests that specify a byte limit
    ("len" request field), that is based on estimates of the channel
    statistics and subject to a control loop that aims to achieve actual
    cumulative service times that match the cumulative requested service
    times within a sequence of timed requests.  Normally, the "len" field
    is used only for requests over HTTP-only transports, but for timed
    requests, the client uses the "len" field also with the HTTP-TCP and
    HTTP-UDP transports, although the division of responsibilities between
    client and server for the maintainance of channel responsiveness is
    different in these transports.  Timed requests also work in the
    stateless communication mode, although it is fundamentally less efficient
    than the stateful modes.
       In the next release, the `kdu_client' implementation will also be able
    to use the "twait" (timed-wait) request field that is part of
    Amendment 5 of the JPIP standard -- this allows the server to manage
    service time distribution for timed requests, which is fundamentally
    more flexible and efficient than the byte limit approach.
       In the next release, the `kdu_client' implementation will embody
    several other important efficiency improvements for JPIP video,
    including "jpxf" context requests (JPX-frame requests), the "mset"
    (model-set) request field for cache model management with indefinite
    content streams, and most likely some new features which are proposed
    for Amendment 6 of the JPIP standard.
14) Extended the core `kdu_compressed_target' class to support the writing
    of structured codestream elements to a cache, as opposed to sequentially
    writing a conventional codestream.  This is part of a long standing plan
    to realize fully integrated live encoding, rendering and
    JPIP services, all brokered by a real-time compressed data cache.
    Prior to this point, compressed data caches were used only by `kdu_client'
    to allow live rendering of data sent asynchronously by a JPIP server.
    Currently, we do not provide any serious implementations of the
    caching compressed data target paradigm, but the "kdu_compress" demo app
    offers the option to send all of its compressed data to a "null target" --
    a caching compressed data target that discards everything.  Other, more
    interesting compressed data targets that support structured writing can
    readily be built by third parties, on top of the stable interfaces that
    have now been established.
       At present, the most important benefit that can be derived from
    `kdu_compressed_target' targets that offer structured writing to a cache
    is that incremental codestream flushing is much more flexible and efficient
    for such targets than it is for linear codestream targets.  For a
    structured cache target, incremental flushing (see, e.g., the
    `-flush_period' argument to "kdu_compress") does not require the lower
    resolution components of the image to have very small precincts.  This
    means that huge untiled images can be compressed and incrementally
    flushed to a structured cache target, regardless of how many resolution
    levels (`Clevels') are defined, so long as you define modest sized
    precincts (e.g., `Cprecincts={128,128}').  See the extensive documentation
    of the `kdu_codestream::flush' function for more on the merits of
    incremental flushing with structured cache targets.
15) Considerable effort has been invested in reducing the number of simulation
    passes required by Kakadu's rate control machinery, since these
    simulation passes are inherently single-threaded so that they can
    become a bottleneck on machines with many CPU cores that can be
    effectively exploited by the rest of the compression machinery.  These
    enhancements include:
    A) better prediction of the distortion-length slope thresholds that are
       likely to be most suitable;
    B) a new  algorithm is now used to synthesize quality layers whose target
       lengths are not explicitly provided (when the target lengths array
       passed to `kdu_codestream::flush' or `kdu_codestream::auto_flush' has
       some zero-valued entries);
    C) when working with slope-based rate-control, where distortion-length
       slope thresholds are provided explicitly, redundant simulation passes
       are removed and in fact no simulation passes are required at all if
       the `kdu_compressed_target'-derived compressed data target supports
       structured writes to a cache; and
    D) an additional option has been added to the `flags' argument to
       `kdu_codestream::flush' and `kdu_codestream::auto_flush', allowing
       the caller to identify supplied distortion-length thresholds as
       "hints" to be used to accelerate the rate control algorithm used
       to achieve layer target sizes.
    All of the Kakadu demo apps that perform compression ("kdu_compress",
    "kdu_buffered_compress" and "kdu_v_compress") now provide a "-tolerance"
    argument that allows the accuracy of the rate control process to be
    specified and the default tolerance is now set to 2% (as opposed to 0%).
    Moreover, the "kdu_v_compress" algorithm now uses the new "threshold
    hinting" option (item D above), to pass the distortion-length slope
    thresholds determined for one frame as hints to the rate control
    algorithm for a subsequent frame.
       Together, these changes mean that the overall complexity of the rate
    control process is substantially reduced (perhaps by 10x or more, but
    typically by at least 3-5x, depending on the number of quality layers of
    interest and how they are specified).
       The modified algorithm for determining quality layers whose sizes are
    not specified in the `layer_bytes' array passed to `kdu_codestream::flush'
    and `kdu_codestream::auto_flush' (item B above) does, however, mean that
    those quality layers are not affected by any `Creslengths' attribute --
    this is unlikely to be a problem in most applications, because
    `Creslengths' is not normally used with the target size driven rate
    control mode, except where layer sizes are all specified explicitly.
16) To support the enforcing of new High Frame Rate (HFR) Digital Cinema
    profile specifications, the `Creslengths' attribute has been supplemented
    with a companion `Cagglengths' attribute that can be used to specify
    components whose compressed data tallies should be aggregated,
    subjecting the aggregated totals to specific `Creslengths' constraints.
    The primary intent is to allow constraints to be specified for the
    combined bit-rate of the chrominance channels, rather than applying
    separate constraints to each individual chrominance channel.  However,
    the `Cagglengths' feature provides much more general support for
    setting up aggregated component sets, on a resolution-specific
    (even tile-resolution specific) basis.  See "Usage_Examples.txt" for
    one example of how to set up aggregated component length constraints.
17) Introduced a very tiny change to the way in which distortion is
    calculated during block encoding, which serves two purposes:
    1) the new calculation is potentially slightly more accurate; and
    2) the new calculation avoids the possibility that a truncated
       code-block bit-stream can appear to have zero distortion.
    Previously, this latter possibility existed in certain highly
    pathalogical cases, which could encourage agressive truncation of
    code-block bit-streams to the point where third party JPEG2000 decoders
    that might use a different dequantization rule to Kakadu (dequantization
    is not formally specified by the standard) could potentially render an
    image with much larger distortion.  This change means that the
    distortion-length slope thresholds associated with a given compressed
    bit-rate are very slightly different between KDU7.1 and KDU7.2, but
    not enough to warrant any significant attention.  For a fixed
    distortion-length slope threshold, the compressed bit-rate associated
    with KDU7.2 may differ by perhaps 1% or less from that generated with
    the same fixed distortion-length slope threshold in KDU7.1.
18) Augmented the `mj2_video_target' class to implement the `start_rewrite'
    and `end_rewrite' interfaces advertised by `kdu_compressed_target'.
    Without this, attempts to compress to a DCINEMA profile or any codestream
    that requires TLM markers will fail to write the TLM data, generating a
    warning that the rewrite functionality is missing.
19) Augmented the TIFF file reading and writing logic associated with the
    classes defined by "image_in.h" and "image_out.h" (used only by the
    demo apps "kdu_compress" and "kdu_expand") with the ability to preserve
    ICC profiles within TIFF files -- a simple feature that has always been
    supported by Kakadu's JP2/JPX file format reading/writing API's, but
    whose omission from the demo apps was a cause of concern for some
    Kakadu users.
20) Added a new function, `kdu_input_box::close_without_checking' that is
    now called in place of `kdu_input_box::close' by the `kdu_input_box'
    destructor.  The new function itself calls `close', ensuring that
    derived objects continue to behave as expected; however, the new function
    avoids a potentially regrettable call to `jp2_input_box::is_complete'
    that might access an underlying `jp2_family_src' object whose existence
    cannot be relied upon, especially during exception handling.

Bug Fixes:
A) Fixed a bug in the generation of Java and C#/VB language bindings by
   "kdu_hyperdoc" regarding the order in which temporary marshalling
   arrays are allocated and deleted; the main (perhaps only) current API
   function affected is `kdu_client::check_compatible_url'.
B) Corrected a bug in `kdu_stripe_compressor' in the conversion of unsigned
   16-bit words to internal 16-bit samples for `kdu_multi_analysis'.
C) Corrected an accidental premature "return" statement in the encode-time
   ROI logic that caused a segmentation fault to appear if the "kdu_compress"
   demo app is used with an "-roi" argument but no other parameters
   (such as Rshift) to specify what should be done to the region of interest.
D) Corrected two other bugs in the offsets computed for interfacing with
   ROI line buffers in the block encoding logic -- only for region-of-interest
   encoding.
E) Corrected a very subtle bug in the "kdcs_comms" machinery (network
   comms for the client and server components) which could lead to
   a race condition between acceptance and serving of an incoming
   socket connection.
F) Corrected another subtle race condition in the `kdu_cache' object that
   could cause an attempt to access invalid memory contents when browsing
   remote content with multiple codestreams.
G) Found an error in the interaction between `kdu_codestream::restart'
   (for input codestreams) and the way in which multi-component transform
   parameters are translated and finalized (does not apply to Part-1
   codestreams).  This led to a revision of the way in which parameter
   changes are deteced when using the efficient `kdu_codestream::restart'
   feature, so as to robustly detect all changes between codestreams.
H) Corrected a minor oversight in the "kdu_v_expand" demo app, where the
   `io_queue' object was passed to `kdu_thread_entity::terminate' before
   wrapping up the application, rather than `kdu_thread_entity::join'.  In
   some cases, using `terminate' resulted in premature termination of the
   background multi-threaded processing jobs so that the final decompressed
   video frame might not be written to the file.
I) Fixed some subtle race conditions in "kdu_server" that could occur under
   unusual network conditions or when an application runs out of file handles.
J) Corrected an oversight in `kdu_client::get_window_in_progress' and
   `kdu_client::get_oob_window_in_progress' with respect to the issuing of
   the `KDU_CLIENT_WINDOW_IS_COMPLETE' status flag.  That flag is now set
   only if the server has indicated in an EOR message that it has sent the
   complete response to the window of interest AND the response data from
   all potentially concurrent requests on all other JPIP channels has also
   been received -- this is a subtle correction that is relevant only when
   using multiple request queues, and then only if multiple JPIP channels
   are actually created.
K) Introduced a very minor correction to the way the "kdu_merge" tool
   rounds the relative locations and cropping dimensions provided via its
   `-composit' argument.  Previously, all parameters were rounded
   down while converting to integer quantities, but this can make it
   impossible to specify certain precise cropping or placement configurations
   via the "kdu_merge" command-line; they are now rounded to the nearest
   integer.
L) Slightly modified the way the "kdu_show" application posts JPIP requests
   when there is insufficient information in the cache to begin image
   rendering yet.  Previously, under this case the application always asked
   for the entire composition surface, unless a focus box was manually
   defined, accidentally ignoring the location and dimensions of the
   user's view port.
M) Minor change in `kdu_region_compositor::get_max_available_quality_layers'
   to take the maximum of the number of quality layers used by all
   opened tiles in relevant codestreams, together with the `Clayers'
   attribute in such codestreams' main header.  This helps ensure the
   reliability of the value returned, when no (or few) tiles have yet been
   opened.
N) Fixed some subtle race conditions inside the implementation of `kdu_cache'
   that never appeared until some of our customers started building
   applications that involve heavy multi-threading, very high bandwidths
   and continuous streaming over days/weeks.
O) Fixed a race condition in the `kdu_client' object during primary channel
   address resolution, where a call to `signal_status' was accidentally
   invoked outside the critical section within which such calls should
   originate.
P) Corrected an error in `kdu_window_model::add_instruction' (both versions)
   that is used by the "kdu_server" application and any third party apps
   that build on top of the `kdu_serve' interface.  Specifically, the
   test for an additive cache model statement with 0 bytes (a no-op) was
   accidentally testing for a subtractive model statement with 0 bytes, as
   pointed out by a licensee.
Q) Made sure that all demo apps that might involve multi-threaded processing
   install a thread-safe core message handler for error and warning messages.
   In most cases, this simply means that an error/warning message handler that
   was previously derived from `kdu_message' is now derived from
   `kdu_thread_safe_message' -- this avoids the possibility of garbled
   error messages arriving from multiple threads when the core
   system encounters something wrong with the content or commands supplied.

Changes from KDU-7.0 to KDU-7.1
-------------------------------
This is an important upgrade that incorporates major improvements to
the multi-threaded core processing sub-system introduced in KDU-7, along
with numerous enhancements to other aspects of Kakadu and some new
features.  The KDU-7.1 release is the most stable and comprehensive
version of the regular Kakadu SDK on which to build applications.  This
release incorporates quite a few fixes for bugs that have been exposed
with the help of our licensees who are building applications on top of
some of Kakadu's most advanced features.  Changes from KDU-7.0 are as
follows:
1) Multi-threaded processing sub-system improvements
   -- This version completes the implementation of background codestream
      processing machinery, a feature that was introduced but hardly used
      in KDU-7.0.  The background processing machinery accomplishes tasks
      such as incremental flushing of compressed content (during compression),
      background reading and pre-parsing of codestream content (during
      decompression), background cleanup of compressed data and resources
      that are no longer needed, and background allocation and configuration
      of resources that will be needed in the near future (e.g., precinct
      and code-block structures to accept encoded data during compression).
      Where these background tasks have dependencies (block encoding or
      decoding operations that need the task to be performed), the completion
      of relevant background work contributes to the scheduling of dependent
      tasks so that the dependencies are always satisfied.  As a result,
      block encoding and decoding operations where most threads spend most
      of their time need NEVER BLOCK on the availability of codestream
      resources.
   -- This release greatly reduces the number of points at which threads
      might need to take out locks on critical sections.  Already this was
      greatly reduced with the transition to KDU-7 from previous versions
      of Kakadu, but additional improvements have been made that help
      further improve throughput in heavily multi-threaded platforms.
   -- A number of changes have been introduced to reduce the frequency with
      which the system heap management functions need to be invoked, so as to
      minimize or totally avoid contention for the heap.
   -- A few possible race conditions that we identified in KDU-7.0
      following its release, have been eliminated.
   -- This release includes some special templates for derived
      `kdu_thread_queue' and `kdu_thread_job' objects that can be used in
      heavily multi-threaded environments to ensure that the work typically
      done by a master processing thread (pushing image data into a compressor
      or pulling it out of a decompressor) can be done in an automatically
      rescheduled job that always runs on the first available thread.  This
      can reduce the appearance of delays on the master processing thread as
      bottlenecks to the overall processing throughput of an application.
      To learn more about this feature, consult the documentation for
      `kdu_run_queue'.
   -- This release introduces new "preferred" versions of the
      `kdu_multi_analysis::create' and `kdu_multi_synthesis::create' functions
      that accept control flags rather than a multitude of discrete options.
      The `kdu_multi_synthesis' object has also been augmented with a `start'
      function that can be used (optionally, if the relevant flag is supplied
      to its `create' function) to achieve optimal interleaving of
      multi-threaded processing jobs for cases where multiple tiles are being
      processed concurrently using separate `kdu_multi_synthesis' objects.
   -- Fixed a bug in the dependency propagation logic in
      `kdu_multi_analysis' and `kdu_multi_synthesis' which could occur
      when the multi-threaded (asynchronous) DWT implementation was not
      selected (i.e., when `KDU_MULTI_XFORM_MT_DWT' or `KDU_MULTI_XFORM_DBUF'
      was passed in the `flags' argument to `create').
   -- This release slightly modifies the definition of
      `kdu_thread_queue::update_dependencies' to incorporate a meaningful
      return value.  The return value is used to discover whether dependency
      propagation is of interest to super-queues (or dependency monitors),
      allowing multi-threaded processing machinery to avoid the overhead of
      careful synchronized dependency propagation beyond the point in the
      queue heirarchy where dependencies are of interest.  This feature is
      exploited within `kdu_multi_analysis' and `kdu_multi_synthesis' to
      reduce the overhead incurred within the synchronous DWT path (this is
      used where the `KDU_MULTI_XFORM_MT_DWT' or `KDU_MULTI_XFORM_DBUF' flag
      is not supplied to `create'), which is often used with very small
      stripe buffers (typically only 1 line).
   -- In this release, the logic associated with identifying the point at
      which all jobs have been scheduled for a `kdu_thread_queue' object
      has been changed very slightly.  You will only find this relevant if
      you have developed your own multi-threaded processing objects that
      schedule their own jobs and also take the effort to provide the
      earliest possible notification of the point at which all jobs have
      been scheduled (none of this is mandatory but can improve efficiency
      in some circumstances).  Previously, the way to do this was to invoke
      `kdu_thread_queue::all_scheduled', but the new approach is to specify
      the `all_scheduled' condition in the `kdu_thread_queue::schedule_jobs'
      call itself.  For multi-threaded processing objects that rely entirely
      upon lockless processing (i.e., no critical sections at all), the new
      approach helps the programmer avoid possible race conditions that
      could arise if execution of the final scheduled job sets in motion events
      that could clean up the object's resources, before an explicit separate
      call to the old `all_scheduled' completed.  The new mechanism are all
      100% robust against such race possibilities and can be safely executed
      without taking out critical section locks.
2) Changes related to precinct/packet parsing and compressed data caches
   -- Fixed a possible source of race conditions (long present in Kakadu)
      whereby calls to functions like `kdu_resolution::get_precinct_packets'
      could potentially interfere with ongoing processing in other threads by
      code-block decoding engines.  In reality, it is unlikely that
      applications do such things concurrently within the same open tile, but
      it is possible and without the fix, could have caused a subtle and hard
      to track failure condition.  This problem was fixed while at the same
      time improving the efficiency of the JPEG2000 packet parsing process
      for cases in which the source of data is a dynamic cache whose contents
      are evolving as new data arrives.
   -- Slightly modified `kdu_resolution::get_precinct_packets' so that it
      never returns a value larger than the currently installed maximum number
      of apparent quality layers -- previously, the behaviour of the function
      under quality layer limitations was too dependent upon the underlying
      structure and random accessibility of the compressed source. This change
      should not adversely affect existing applications but make their
      behaviour more consistent.
   -- Added an extra argument to `kdu_region_composior::get_codestream_packets'
      so that the caller can limit the maximum number of quality layers over
      which the completeness of the available codestream content will be
      assessed.  This is useful for determining whether or not a dynamic
      cache contains all content up to some required number of quality layers
      for a given region of interest, for certain types of JPIP browsing
      applications.  This feature is also now employed by the "kdu_winshow"
      and "kdu_macshow" apps, whose JPIP progress bar now displays the
      degree to which a Window-of-Interest request has been satisfied, taking
      into account any restriction on the number of quality layers
      that are of interest (as specified via the `<' and `>' accelerators or
      the corresponding menu items).
3) New core codestream generation features
   -- Added a new codestream parameter attribute `ORGplt_parts', that can
      be used to control the points at which packet length information (for
      random access to precinct data) is partitioned into PLT marker segments.
      This is useful in constructing codestreams that adhere to recommended
      configurations described in the NITF JPEG2000 profile specification.
   -- Added a new core interface function `kdu_codestream::get_packet_bytes'
      that can be used to discover the total number of bytes generated for all
      written JPEG2000 packets (packet bodies and packet headers).  The
      difference between this value and that returned by
      `kdu_codestream::get_total_bytes' identifies the total number of bytes
      consumed by markers and marker attributes (i.e., non-packet header and
      marker bytes).  This value, in turn, is of interest because the
      `Creslengths' attribute specifically constrains the number of packet
      bytes (packet header and packet body bytes) only, whereas some profiles
      (notably broadcast profiles) require the overall codestream length to be
      constrained.  The "kdu_v_compress" application uses this feature to
      report the number of non-packet bytes (i.e., header and marker bytes)
      which can then be subtracted from a profile limit on the overall
      codestream size in order to determine a `Creslengths' value that is
      guaranteed to avoid profile violation.
   -- Significantly improved automatic selection of coding parameter
      attributes that have not been set where `Sprofile' indicates a broadcast
      profile.
   -- The rate control capabilities realized via `kdu_codestream::flush' and
      `kdu_codestream::auto_flush' have been extended to allow for the
      simultaneous use of both distortion-length slope thresholds and
      layer size limits (measured in bytes).  Previously, one or the
      other of these two methods were accepted for controlling the amount of
      content generated for each quality layer.  In particular, if both
      layer size limits and slope thresholds were supplied, only the slope
      thresholds would be used.  These two methods were demonstrated via the
      `-slope' and `-rate' arguments to demo apps like "kdu_compress" and
      "kdu_v_compress".  It is now possible to pass the auxiliary flag,
      `KDU_FLUSH_USES_THRESHES_AND_SIZES', to `kdu_codestream::flush' or
      `kdu_codestream::auto_flush', along with both a `layer_bytes' array
      and a `layer_thresholds' array (with a non-zero first entry), in
      which case the quality layer generation procedure is governed primarily
      by the slope thresholds found in the `layer_thresholds' array (as
      before), but the information found in the `layer_bytes' array will be
      interpreted as preferred lower bounds on the number of bytes included
      for each quality layer.  The idea is that unexpectedly compressible
      content would receive more bytes than might otherwise be assigned by
      the slope thresholds, so as to (preferably) achieve the specified
      lower bounds.  This feature has been requested for applications
      in video compression.  The feature works together with upper bounds
      specified via `Creslengths' -- the upper bounds always take preference
      over lower bounds or any other rate control specifications.  The
      feature also works with incremental flushing.
4) New/improved file format support: broadcast streams, video, JPX & jp2info
   -- Added support for the file-format aspects of the broadcast specification
      found in Annex-M of IS15444-1/AMD3.  Previous versions of Kakadu already
      supported the broadcast profiles defined in IS15444-1/AMD3 at the
      codestream level, but did not offer the JP2 box types defined for
      construction of broadcast streams.  These box types are now defined in
      "jp2.h", while interfaces to read and write broadcast streams may be
      found in "jpb.h" and "jpb.cpp".  Broadcast streams have been added as
      one of the file types supported by the "kdu_v_compress" and
      "kdu_v_expand" demo apps, which now support some additional command-line
      arguments -- see the `-usage' statement, as usual.
   -- Improved support for interlaced video in the interfaces offered for
      Motion JPEG2000 and Elementary broadcast streams, and extended the
      "kdu_v_compress" and "kdu_v_expand" demo apps to support interlaced
      video, as well as mixed interlaced/progressive content.
   -- Added a new generic facility for generating names and textual
      representations for JP2 boxes, as used in JP2, JPX and MJ2 files as
      well as elementary broadcast streams.  This facility is demonstrated
      by the new "kdu_jp2info" application that prints an XML-compatible
      representation of any JP2-family file or raw codestream, with options
      to control the level of detail printed.
   -- Greatly improved the usage of "const" qualifiers for "get" and other
      attribute accessor methods associated with the numerous interface
      classes offered in support of the JP2 and JPX file formats and
      JP2-family files in general.
   -- Added new methods `jp2_input_box::open_as' and `jp2_input_box::fork'.
      The former, allows a box's header to be read, after which the box can
      be closed and later re-opened without incurring any I/O overhead.  The
      latter allows a box's reading state to be reproduced in another
      `jp2_input_box', similar to `jp2_input_box::transplant', except that
      the forking source box is also left open.
5) Improvements to rendering by `kdu_region_compositor'
   -- Improved the way in which `kdu_region_compositor' scales and places
      compositing layers on the compositing surface.  Specifically, the
      underlying rational scaling factors now approximate the ideal scaling
      factors to sufficient accuracy to ensure placement and sizing of image
      layers correctly, even for compositions involving many image layers
      composed onto very large surfaces.  Previously, the accuracy with
      which scaling factors were approximated was related to the dimensions
      of the image layers in question, rather than the final composition
      surface -- this had the potential to lead to placement errors for
      composition surfaces much larger than the individual image layers.
   -- Fixed a minor bug in the `kdu_compositor_buf::get_region' function.
6) JPIP client/server improvements and fixes
   -- Slight improved the way in which JPIP `context' requests are converted
      into individual window regions and resolutions within each codestream
      associated with the `context', so as to avoid possible accumulation
      of sizing or positioning errors associated with complex JPX compositions
      with large compositing surfaces formed from smaller compositing layers.
      These improvements appear within both "kdu_servex.cpp" and
      "kdu_clientx.cpp".
   -- Corrected an oversight in the support of JPX composition instructions
      with orientation/flipping information.  Support for this feature was
      first introduced with KDU-7.0, within the `jpx_source' and `jpx_target'
      objects, along with corresponding support in `kdu_region_compositor' and
      the "kdu_server" application.  However, support for the feature was
      accidentally omitted from the `kdu_clientx' object that is used by
      Kakadu's JPIP client to inform servers of existing cached content.
   -- Fixed a problem in "kdu_server" that arose in rare circumstances where
      the `-phld_threshold' configuration option is set to a value larger than
      the size of the JPX composition box (where there is one).  In such
      cases, the contents of the box were not used by the server to translate
      JPIP "context" requests of type "jpxl" with composition instruction
      qualifiers.  As a result, in these circumstances the server would serve
      the wrong content in response to sophisticated JPIP client requests
      involving complex composited imagery.
   -- Fixed a bug in `kdu_client::read_tcp_chunk' which could have resulted
      in the return of multiple acknowledgement messages to the server under
      very high bandwidth conditions where the operating system's internal
      TCP buffer became temporarily full.
   -- Added a new status member function `kdu_client::target_started' that
      can be used to check whether or not the first reply to a JPIP client
      (the one that verifies the existence of the target resource on the
      server) has been processed.  This is useful because it allows a JPIP
      browsing application to easily avoid the situation in which image
      rendering (e.g., via `kdu_region_compositor') attempts to start up
      before the first reply has been fully processed, in which case the
      renderer may or may not see any information that can be loaded from a
      local cache file storing the results of previous browsing sessions.
      There is nothing fundamentally wrong with such a scenario, except that
      it can lead to inefficiencies and unnecessary client-server
      communication.  While the function is mainly of interest in improving
      the efficiency of the control flow where a local cache file is
      available, it may have other uses and it is open to extension in the
      future if Kakadu's client/server components support multi-target JPIP
      sessions -- something that has been requested by some users.
7) Implementation of new JPEG2000 features
   -- This release contains a preliminary implementation of the new
      fast block coding mode that is the subject of Ammendment 4 to
      IS15444-2.  This new coding mode is actually an extension of the
      original BYPASS mode to additional bit-planes in the embedded block
      coding process.  It may be accessed by the two new mode flags
      `Cmodes_BYPASS_E1' and `Cmodes_BYPASS_E2' and is supported by the
      demo apps that write and read JPEG2000 Part-2 codestreams and JPX files.
      Experimental results suggest that this feature can increase both
      compression and decompression throughput of Kakadu by approximately 30%,
      depending on the target compressed bit-rate, without significant loss
      in compression efficiency.
8) Other core system and Java interface fixes:
   -- Made minor changes to the Solaris makefiles and "kdu_elementary.h" to
      correct Solaris build problems that arose in the transition from Kakadu
      v6.x to KDU-7.
   -- Fixed a problem with the interaction between Java interfaces and
      Kakadu's multi-threading sub-system, where Kakadu funcions that called
      into a class member function implemented in Java would invoke the JNI's
      `AttachCurrentThread' function but never invoke the corresponding
      `DetachCurrentThread' function.  The consequence of this oversight was
      that applications that create and destroy `kdu_thread' objects regularly
      and pass to these threads (perhaps indirectly) instances of Java-derived
      objects with Java-implemented callback functions (examples include
      objects descended from `kdu_compressed_data_source',
      `kdu_compressed_data_target', `kdu_message' or `kdu_client_notifier')
      would experience a progressive accumulation of unreleased
      thread-specific resources, eventually leading to out-of-memory errors.
      For efficiency, and to ensure correct behaviour even with recursive
      calls in and out between C++ and Java, the new implementation caches
      the thread-specific jniEnv reference with the underlying `kdu_thread'
      object and detaches it when the thread exits from its entry-point
      function.  As a side benefit, `kdu_thread' objects now provide a
      generic facility for storing thread-specific instances of custom
      objects that are automatically cleaned up as the thread terminates.
      Moreover this information can be accessed from any point in the thread's
      execution.
   -- Fixed a bug in the DWT synthesis machinery that occurred during
      reversible decompression of a limited region of interest involving
      exactly one column, when a transposed codestream appearance transform
      is in effect.
   -- Fixed errors in the implementation of the 32-bit version of the
      `kdu_region_decompressor::pull_stripe' function.
   -- Fixed offset and minor scaling errors in the floating-point versions
      of the `kdu_region_compressor::push_stripe' and
      `kdu_region_decompressor::pull_stripe' functions and added rounding
      offsets to the high precision data conversion functions, where
      appropriate.
   -- Introduced rounding offsets to the data conversion code that handles
      floating point TIFF files in "image_in.cpp".
   -- Fixed a documentation error in the API documentation for the function
      `kdu_codestream::flush'.  Specifically, this function previously
      reported an equation for the relationship between logarithmic slope
      thresholds and the slope (delta_D/delta_L) on the operational
      distortion-length characteristic of the compression system.  The
      reported relationship was correct but for the case in which delta_D
      measures the change in total squared error in the image after
      normalizing the sample values to the range -128 to +128 (not -0.5 to
      +0.5 as previously reported).  The corrected API documentation makes
      reference to the earlier error, for clarification.

Changes from version 6.4.2 to KDU-7
-----------------------------------
This is a massive upgrade, along many dimensions.
The following is a summary of the principle features:
1) Changes to the core multi-threaded processing sub-system
   -- Kakadu's multi-threaded processing sub-system has been completely
      redesigned.  Major features of the new architecture are as follows:
   -- External interfaces are as similar as possible to what existed
      before.  For simple applications, little or no porting will be
      required to the new architecture.  The main step that should be taken
      for almost all applications is to call `kdu_thread_env::cs_terminate'
      prior to destroying a codestream that has been used with multi-threaded
      processing.  For quite a few applications, there may be nothing else
      to change.  In any case, when porting existing applications to the new
      multi-threading architecture, you should use the demo applications as
      a guide and pay particular attention to the documentation of the
      interface functions `kdu_thread_entity::join',
      `kdu_thread_entity::terminate' and `kdu_thread_env::cs_terminate' --
      reading the documentation of just these three functions should be
      sufficient as a guide to porting most applications.
   -- The new architecture is mostly lock-free, meaning that atomic
      manipulation of synchronization variables is used in place of critical
      sections to avoid the overhead and potential stalls associated with
      mutual exclusion primitives.
   -- Previously, safe multi-threaded processing involved a few exception-safe
      mutual exclusion locks that were shared by all codestreams that might
      be used with a `kdu_thread_env' environment.  However, now each
      codestream contributes its own locks to the threading environment, and
      they are also used much less often.  This will particularly improve
      the performance of systems that deploy groups of threads to work on
      multiple codestreams.
   -- Scheduling of executable jobs is now done in a distributed manner.
      There is the appearance of a central scheduler, but processing and
      scheduling operations no longer need to content for access to any
      kind of central dispatch mechanism.
   -- The previous architecture embodied a particular method for implementing
      multi-threaded processing within a tree-structured dependency graph,
      whereby threads that encountered a blocking dependency would donate
      their resources (if possible) to other processing jobs when they
      encountered blocking dependencies.  This approach essentially discovers
      the dependencies at the point when they become problematic; one can
      think of the approach as propating "dependency needs" downwards towards
      the leaves of the tree-structured graph.  These features are preserved
      in the new architecture, but they are augmented with the ability
      propagate "dependency availability" upwards towards the root of the
      tree-structured graph.  Actually, the system supports topologies other
      than trees, but that is most of what we need for JPEG2000 compression
      and decompression.  The upward propagation of "dependency availability"
      can be used by individual data processing entities (derived from
      `kdu_thread_queue') to ensure that jobs are scheduled only once their
      dependencies are satisfied.  All of Kakadu's major data processing
      objects (`kdu_encoder', `kdu_decoder', `kdu_analysis', `kdu_synthesis',
      `kdu_multi_analysis', `kdu_multi_synthesis') support fully implement
      the upward propagation of dependency availability as well as the
      downward discover of dependency needs, so that threads will always
      block and donate resources to other jobs if they need to, but jobs
      can be scheduled so that the only jobs that run are those which can
      run to completion with blocks or context switches.  As a result, the
      new architecture generally involves shorter thread execution stacks,
      fewer context switches and fewer blocking waits.
   -- The notion of a thread queue (embodied by `kdu_thread_queue') has
      changed radically in the new design.  For the purpose of backward
      compatibility, it is still possible to obtain an internally allocated
      thread queue using `kdu_thread_entity::add_queue' that has a
      compatible interface with its namesake in the old architecture; these
      can be used as before for structuring the queues allocated internally
      by data processing engines such as `kdu_encoder', `kdu_decoder',
      `kdu_analysis', `kdu_synthesis', `kdu_multi_analysis',
      `kdu_multi_synthesis' and higher level objects like
      `kdu_stripe_compressor', `kdu_stripe_decompressor' and
      `kdu_region_decompressor'.  However, `kdu_thread_queue' is no longer
      an opaque object, but a public class from which you can derive your
      own sophisticated processing objects if you wish.
   -- Codestreams themselves now come with a dynamically created
      multi-threading context that contributes background jobs to the
      workload managed by a `kdu_thread_env' environment.  This allows
      incremental flushing, trimming of initial code-block bit-streams
      during compression and pre-parsing of codestream structures that will
      be needed in the near future to all be performed concurrently with
      the main data processing operations, so that threads are blocked as
      little as possible by common dependencies.
   -- Although full exploitation of the new approach is still in progress,
      already we find substantial throughput improvements on systems with
      larger numbers of CPU cores (or hardware threads).  In some cases,
      throughput improvements of 2 or more can be obtained on modern
      processors with 8 or more hardware threads.
   -- The internal codestream memory management sub-system has been completely
      redesigned so that memory can efficiently moved to, from or between
      threads.  We no longer share the critical section associated with
      codestream parsing or content generation and in fact rarely need to
      enter a memory-management critical section at all.
   -- There is no longer any concept of a "synchronized job" that is performed
      once scheduled jobs have finished executing.  In fact, there is no
      real central dispatcher that knows or cares when jobs that were launched
      have been fully executed.  Although this may seem like a concern, the
      only place where synchronized jobs were used in any of the extensive
      set of Kakadu demo applications was for incremental flushing during
      codestream generation.  This is now handled much more elegantly by
      a new `kdu_codestream::auto_flush' function that can be configured
      to do incremental flushing at meaningful points, as they arise, without
      the need for an application to explicitly trigger this activity by
      periodically launching synchronized jobs.
   -- The member function `kdu_thread_entity::synchronize' no longer exists,
      but has been replaced by `kdu_thread_entity::join', which in most cases
      is a drop-in replacement.  In any case, the documentation explaining
      what these functions actually do should be clear now, having been
      very carefully written.
   -- The interfaces to major Kakadu processing objects remain unchanged,
      retaining the convention that multi-threaded processing is governed
      by constructing (or starting) a processing object with a non-NULL
      `kdu_thread_env' reference and (optionally) a non-NULL `kdu_thread_queue'
      reference from which the internal processing queues should be
      descended.  However, in many cases, the `kdu_thread_queue' objects
      used with these objects previously had to be unique -- that is,
      multiple processing engines could not be attached as descendants of
      the same super-queue.  In the new implementation, this restriction does
      not apply, so application developers should encounter fewer surprises.

2) Processor-specific enhancements
   -- The SIMD speedups that are enabled by the KDU_X86_INTRINSICS macro have
      been extended to incorporate SSSE3 instructions more widely, as well as
      the new AVX instructions, where appropriate.  In environments where
      AVX instructions instrinsics are not available, but X86 intrinsics are
      otherwise available, the `KDU_NO_AVX' macro must be defined.  Currently,
      this macro is defined for compilers other than Visual Studio 2010, but
      you can modify the relevant Linux makefiles if you have a recent version
      of GCC that supports AVX.
   -- As more SIMD optimization branches are added to the code, optimizations
      for older processors are not generally removed.  However, to minimize the
      clutter created by the numerous optimization branches offered by Kakadu,
      a new KDU_MIN_MMX_LEVEL macro has been introduced.  This macro identifies
      the minimum level of X86-style SIMD technology for which accelerated
      routines are compiled into the code, depending on the availability of
      multiple acceleration options.  Specifically, the macro takes values of
      1 for MMX, 2 for MMX to SSE2, 3 for MMX to SSE3, 4 for MMX to SSSE3,
      5 for MMX to SSE4.1 and 6 for MMX to AVX (subject to extension in the
      future).  If multiple accelerated code variants exist that would be
      supported at KDU_MIN_MMX_LEVEL, only the most advanced such variant is
      actually compiled into the code.  Where enabled, More advanced variants
      (i.e., beyond KDU_MIN_MMX_LEVEL) are also compiled.  Of course, if
      the CPU is found not to support any of the compiled acceleration
      options, the regular non-SIMD will be executed.
   
3) Changes to the JPX sub-system
   -- Added support for the new "group" box ("grp_") which allows
      non-semantic grouping of metadata boxes into arbitrary (potentially
      nested) groups.  Previously, Kakadu synthesized the funcionality of
      a grouping box by offering "free-asoc" boxes -- i.e., association
      boxes ("asoc") whose first sub-box was a free box ("free") and hence
      semantically meaningless.  Kakadu still supports free-asocs and can
      be forced to generate them in preference to grouping boxes, but this
      is not recommended for interoperability.  The grouping box is being
      introduced in IS15444-2/AMD3.  Non-semantic grouping is important in
      facilitating efficient access to JPX metadata stored in a remote file
      via JPIP.
   -- Major changes have been introduced into the way JPX metadata is read and
      written.  One of the major changes is that grouping boxes (and
      free-asocs) are now parsed in such a way that their contents (in the
      recursive sense) appear to be siblings -- that is, non-semantic grouping
      containers are removed from the view of the application.  Similarly,
      when a file is written, non-semantic containers are automatically
      inserted in a judicious fashion without the need for any application
      intervention.  This is all done to ensure efficient transport of
      rich metadata via JPIP.
   -- Numlist ("nlst") and Region of interest description ("roid") boxes are
      now correctly handled regardless of where they appear in the metadata
      hierarchy.  In particular, these boxes are properly understood when
      they appear as descendants of labels, XML and other metadata box types.
      The file writer preserves such hierarchical relationships without
      trying to force these boxes to appear at the top level of the file, as
      in previous versions of Kakadu..  Advanced archiving of
      these box types within searchable libraries is performed regardless of
      where the boxes appear.  All applications and API's which access
      number lists (i.e., image entity references) and region of interest
      descriptions for one purpose or another have been modified to ensure
      that they work uniformly regardless of where the numlist or roid box
      might appear within the metadata hierarchy.
   -- For some time, Kakadu has adopted a semantic classification of
      cross-reference boxes into "Grouping Links", "Alternate Child Links", and
      "Alternate Parent Links".  We believe this semantic classification is
      very sound and consistent with the original standard's specification
      for how cross-references should be interpreted.  In this latest release,
      the potential of "Grouping Links" has been expanded to fully support the
      construction of semantic groups through chains of Grouping Link nodes
      that point to each other.
   -- Quite a number of new interface functions have been added to the
      `jpx_metanode' interface to facilitate interaction with metadata.  The
      most notable additions are `jpx_metanode::get_next_descendant' and
      `jpx_metanode::get_prev_descendant' which present a variety of options
      for walking through lists of descendants of a metadata node, where an
      arbitrary subset of the descendants might be missing (typical during
      an interactive JPIP session).  These functions should be used in
      preference to the older, less flexible functions `get_descendant' and
      `count_descendants', because the list of apparent descendants of a node
      may change in unexpected ways due to the transparent collapsing of
      non-semantic grouping containers (as explained above).
   -- In previous versions of Kakadu, it was necessary to explicitly invoke
      `kdu_meta_manager::load_matches' to parse a JPX source for new metadata
      that might have become available (typical when browsing a file remotely
      via JPIP).  Now, however, certain `jpx_metanode' functions automatically
      perform whatever parsing is required to ensure that the results that they
      return are as up-to-date as possible.  This is generally much more
      efficient than a global call to the `kdu_meta_manager::load_matches'
      function, and a lot more convenient.  The `jpx_metanode' object's
      descendant walking and link following functions, as well as the
      functions that synthesize JPIP metadata requests are amongst those that
      implement this parsing-on-demand feature.
   -- Quite a bit of effort has been invested in ensuring that functions
      which might be used to repeatedly parse the metadata structure of a
      JPX source (including `jpx_meta_manager::load_matches') are as
      efficient as possible.  This is done by keeping hierarchical internal
      lists of boxes that are currently incomplete.
   -- The amount of memory required to keep track of a node in the metadata
      hierarchy has actually decreased due to more efficient internal
      representation of the information.  This means that very large
      collections of metadata can be handled in memory.
   -- JPX animation instructions with the special LIFE value 0x7FFFFFFF have
      always supposed to be treated differently, creating "pause for user
      input" frames.  However, Kakadu did not provide special treatment for
      this case.  This has been remedied.  In particular, the
      `jpx_composition::get_frame_info' and `jpx_composition::add_frame'
      now interpret illegal `duration' value of 0, as a reference to
      the presence of such "pause" instructions.  This is explained carefully
      with the API functions.

4) Client/Server changes
   -- The client now offers a special "Out-of-Band" (OOB) virtual request
      channel that applications can use to route high priority (typically
      small) requests around on-going (typically much larger) requests.  For
      example, an application can use this new mechanism to request important
      metadata while imagery is continually being streamed from the server.
      As with all communication mechanisms offered by the `kdu_client',
      the underlying communication is implemented using either physically
      separate JPIP communication chnannels (to the extent supported by the
      server) or by multiplexing virtual channels onto a single JPIP channel
      (or a smaller number of established JPIP channels), so the application
      does not have to worry about how the data is actually communicated.
   -- Both the "kdu_server" application and the `kdu_client' class now
      support communications via the new UDP transport described in
      IS15444-9/AMD5.  However, the UDP implementation should be considered
      somewhat experimental for the moment, in part because the implementation
      does not yet take full advantage of the opportunity to communicate
      with low delay over unreliable channels.  Communication has been
      tested under Windows and OSX so far, but should also work under Linux
      and other operating systems.
   -- Superior channel state estimation for auxiliary TCP and UDP channels.
   -- New JPIP request fields introduced with IS15444-9/AMD5 are now supported,
      such as the "handled" request, which allows clients to discover what
      features the server offers.
   -- Totally re-implemented the way in which cache model manipulation
      instructions are handled.  The new implementation is much more efficient
      in that cache model manipulation is deferred (wherever possible) to the
      point when the relevant cache model entries are actually accessed
      while serving a request.  This certainly makes stateless request
      handling much more efficient, since stateless requests typically involve
      a lot of cache model statements.
   -- Totally re-implemented the way in which cache models are stored
      internally.  The new implementation allows for the retention of
      information about holes in the cache model for any given data-bin,
      which is very useful when working with lossy communication channels
      (notably the new HTTP-UDP transport channel defined by JPIP).
   -- The new cache model implementation actually requires less memory than
      the old one on 64-bit architectures -- typically just 64 bits per
      data-bin.

   -- NOTE: need to modify the channel state estimation machinery just a
      little bit more.

5) Enhancements to `kdu_region_compositor' and `kdu_region_decompressor'
   -- Added support for the automatic generation of JPIP metadata requests
      that are relevant to a defined region of a composition.  This is done
      through the sophisticated `kdu_region_compositor::generate_metareq'
      function.  This function greatly simplifies the implementation of
      applications that need to work intelligently with content that is
      located on a remote server.  Use of this function is demonstrated
      by the "kdu_show" demo applications.
   -- Enhanced the way in which queued composition buffers are handled so as
      to facilitate better interaction between separate rendering and
      display threads for animated display applications.  Changes should all
      be backward compatible with existing applications.
   -- Added more SIMD accelerations to the internal processing machinery.
   -- Introduced significant changes to the sequence of internal steps that
      are performed when composited frames (or buffers with metadata overlays)
      are generated during animations (i.e., when composition buffers are
      pushed onto the composition buffer queue).  These changes should be
      transparent to the application but they serve to avoid redundant
      buffer initialization, copy and composition steps so long as the
      `kdu_region_compositor::set_surface_initialization_mode' function has
      been used to turn off pre-initialization (always recommended for
      applications involving animation).
   -- Fixed a problem that prevented regions of interest that were described
      on a codestream canvas with higher resolution than any image plane
      from being painted with full precision onto metadata overlays.  For
      example, the image data in a JPEG2000 codestream may be defined on a
      sub-sampled grid (not often done, but legal), allowing regions of
      interest to be described through the metadata to sub-pixel precision
      (with respect to the imagery itself).  Previously the faction bits in
      such ROI descriptions were discarded during metadata overlay painting --
      only really an issue when drawing a heavily zoomed-in version of the
      content.
   -- Slightly modified the region mapping functions offered by
      `kdu_region_compositor'  and `kdu_region_decompressor' to better handle
      the mapping of vertices and regions between composited buffer coordinates
      and codestream canvas coordinates.

6) New animation support
   -- A new `kdu_region_animator' object has been added to the special
      support objects found in the "apps/support" directory.  This object
      is designed to work with `kdu_region_compositor' in rendering
      applications that also need animation.
   -- In addition to managing the scheduling of frames for regular MJ2
      video tracks and JPX animations, this new object is capable of
      synthesizing novel animations from metadata within a JPX file.
      For example, a sequence of metadata nodes that contain or link to
      ROI description boxes can be automatically converted into an animation
      that pans between the various regions of interest within their
      respective compositing layers or within the fully composited frames
      to which they belong.  The animator provides special dynamic panning
      features which work together with `kdu_region_compositor' and a
      screen update engine (provided by the application) in a very
      simple way to synthesize smooth pans around the metadata of interest,
      with configurable speed and acceleration parameters, while keeping
      memory consumption very low (a constant feature in Kakadu).
   -- The animator is designed to work well with JPIP when the actual source
      file is located on a remote server.  The animator is even
      able to synthesize JPIP requests for the metadata required to
      determine the regions of interest and/or image entities that will
      comprise a metadata-driven dynamic animation, so that the number,
      sequence and identities of the animation frames is discovered
      dynamically in "just-in-time" fashion, while the animation is in
      progress.  In general, Kakadu strives to implement the dream of
      delay-free interactive rendering, by pre-requesting important
      structural elements using the special OOB request queue now offered
      by the `kdu_client', while relying heavily upon the fact that JPEG2000
      content can generally be rendered from a cache with almost any arbitrary
      amount of content, being asynchronously populated by the client as and
      when it receives data.  There are relatively few stall conditions for
      an animation and the animator attempts to avoid these by synthesizing
      its own JPIP requests.

7) Enhancements to the "kdu_show" applications
   -- The metadata catalog sidebar now offers much improved handling of
      ROI (region of interest) and numlist (image-entity of interest) nodes,
      providing an intuitive colour coding scheme to indicate whether metadata
      is associated with specific regions or just with specific image entities.
      Moreover, this works for region or image-entity metadata associations
      that are found anywhere in the metadata hierarchy.  When interacting with
      a remote server, ROI or numlist nodes whose descendants are not yet
      available are displayed using a special notation which disappears when
      descendant metadata arrives to provide more information.
   -- The application now has the ability to add grouping links to other
      link nodes, allowing for the construction of chains of related
      metadata.
   -- Cut, link and paste operations (using ctrl-X/cmd-X, ctrl-L/cmd-L and
      ctrl-V/cmd-V) now work in an expanded range of contexts.  For example,
      regions of interest can be used to form links or cut and pasted under
      other nodes, and this can be done either from the catalog sidebar or
      from the image view (based on mouse position).
   -- The application now provides metadata-driven animations.  The easiest
      way to access these is by holding the Shift key down while
      double-clicking on a metadata item of interest -- all of its descendants
      are then used to build and play an animation that visits the metadata
      elements of interest in an appealing manner.
   -- Video playback and animation is now implemented through a derived
      version of the `kdu_region_compositor' class that allows much
      better decoupling between the display thread and the main application
      thread.  Management of the composition buffer queue is now handled in
      a more collaborative fashion between these two threads so that the
      display thread does not require the queue to be serviced rapidly by
      the application thread.  Focus and dynamic Region-of-Interest
      information is now stored as part of the state of a queued composition
      buffer so that all aspects of a displayed frame can be updated
      synchronously.
   -- Additionally, video playback and animation is managed by the new
      platform-independent `kdu_region_animator' support object, which
      moves a lot of complexity from the demo application and also allows
      JPIP support for video and animations to be handled in a uniform
      manner in this and future versions of Kakadu.
   -- An animation control bar automatically opens if video/animation is
      offered by the source -- the animation bar fits in neatly with the
      status bar, JPIP progress bar and metadata catalog sidebar, all of
      which open and position themselves around the frame of each
      view window, as appropriate.  The animation bar exposes all the
      animation-related menu options in an easily accessible environment,
      along with a slider allowing the video timescale to be visualized
      and dragged around. In the future, this animation bar will also
      allow visualization of the JPIP cache occupancy as a function of time.
   -- The multi-threaded processing environment associated with each
      window opened by "kdu_show" is now destroyed each time the associated
      file (or data source) is closed, and recreated when one is opened.
      This ensures that the window can be used again properly, with full
      multi-threaded processing, after a data source is found to produce
      an error (unexpectedly) and closed.

8) Enhancements to the "kdu_v_compress" application
   -- For a long time now, we have been intending to provide a video
      compression demo application that can take full advantage of multiple
      CPU resources, in the same way that "kdu_vex_fast" does for
      decompression.  With Kakadu's new core multi-threading sub-system,
      however, it is not necessary to partition the video compression
      problem across multiple parallel frame processing engines.  Instead,
      all that is required to achieve most of the throughput available from
      machines with a modest number of CPU cores (e.g., 4 to 8) is to
      modify "kdu_v_compress" to handle file I/O (video reading and
      compressed video flushing) in background threads.  This is exactly
      what we have done.
   -- The new `-overlapped_frames' option provided by "kdu_v_compress" should
      be used to obtain maximum throughput.  For this reason, the option is
      almost always selected by default and must be explicitly disabled
      using the `-disjoint_frames' option, unless single threaded processing
      is selected.
   -- For high bit-depth video content, "kdu_v_compress" now also accepts
      VIX files whose header declares that the data bits are stored in the
      least significant bits of each data word, rather than just the most
      significant bits of each data word -- see the usage statement for an
      explanation of how the used bits are identified in the header.
   -- The new "kdu_v_compress" demo app can be run without an output file
      (i.e., "-o" is an optional command-line argument now).  In this case,
      all compression, rate control and codestream flushing operations are
      performed as normal, but the output is discarded rather than being
      written to disk.  This mode of operation is useful for estimating
      true throughput performance in applications where the compressed data
      is to be passed in memory to other parts of the application.  Some
      disk systems can present large latencies when programs attempt to
      interleave reads and writes to the same disk, which is what happens
      during video compression with both input and output files supplied.

9) Enhancements to the "kdu_v_expand" application
   -- Previously, the "kdu_v_expand' application offered overlapped frame
      processing and an "-in_memory" option that allows compressed data to
      be loaded into memory before decompression.  However, some of these
      tasks were performed on the main thread so that the throughput of the
      decompression processing could be unnecessarily limited by I/O
      bottlenecks.  Now, the "kdu_v_expand" application uses the same
      approach as the new "kdu_v_compress" application (their internal
      structures are almost identical). Kakadu's core multi-threaded
      machinery is used to perform background loading of compressed data
      (if `-in_memory' is specified) and saving of decompressed frames (if
      an output file is provided), in addition to decompression processing
      jobs (wavelet transform and block decoding).  This means that thread
      processing resources are maximally utilized, going idle only when there
      really is nothing to do.

10) Changes to the way "kdu_compress" and "kdu_expand" handle resolution
    tags/boxes.
   -- Although these are only demo applications, they are often used to
      perform conversion between JP2 and TIFF files.  It has come to our
      attention that TIFF resolution tags are usually best interpreted as
      describing CAPTURE resolution, rather than DISPLAY resolution, which
      was not the interpretation adopted in earlier versions of Kakadu.
      There is also now a more careful description of the interpretation
      of capture and display resolution sub-boxes in Ammendment 6 of the
      JP2 file format (IS15444-1/AMD6).  Accordingly, these applications
      now do the following:
      + If TIFF resolution tags with known units are found in a source file,
        they are reproduced with quite some care in a JP2 CAPTURE resolution
        box.  If resolution tags with unknown units are found, and the
        aspect ratio is not 1:1, the information is reproduced in a JP2
        DISPLAY resolution box, using a resolution of 1 grid-point per metre.
        This is done because the absolute resolution information provided by
        DISPLAY resolution boxes is to be treated as indicative/preferred
        rather than a physical property.
      + When expanding a JP2 file into a TIFF output, if a CAPTURE resolution
        box is found in the source, its contents are reproduced in TIFF
        resolution tags.  Failing this, if a DISPLAY resolution box is found
        in the source, its contents are reproduced in TIFF resolution tags,
        but if the vertical display resolution is lower than 10
        grid-points/metre (a ridiculous resolution to display anything
        at), the TIFF resolution units are set to UNKNOWN.
      + The effect of the above changes is to preserve TIFF resolution
        information across most applications and to preserve it as CAPTURE
        rather than DISPLAY resolution wherever this makes sense.

11) Other changes and fixes:
   -- The demo applications that accept a `-double_buffering' command-line
      argument now automatically configure a reasonable double-buffering
      stripe height whenever the number of processing threads is greater than
      1, but it is possible to disable double-buffering within the DWT
      machinery by supplying 0 for the `-double_buffering' argument.  Double
      buffering costs very little in memory, but almost always offers
      improved throughput with Kakadu's new multi-threaded processing
      sub-system.
   -- The "kdu_merge" demo application has been modified slightly to
      produce nicer photo albums with the "-album" option.  The initial
      set of "album pages" now have the special JPX "pause" attribute,
      meaning that a player will pause when it encounters them, waiting for
      user input, and skip over consecutive pause frames when started from
      a pause frame.  This is all part of the existing JPX standard and
      makes for a more natural interactive experience.
   -- Fixed a number of places in "kdu_params.cpp" where memory leaks could
      potentially arise if an error condition generated an exception through
      `kdu_error'.  This is important only for long-lived applications that
      may open numerous images, possibly containing errors.
   -- Fixed a subtle source of race conditions on 32-bit platforms,
      inside the `kdu_precinct_ref::active_deref' function.
   -- Fixed an error in the `kdu_resolution::get_precinct_samples' function
      which caused it to return 0 when executed at resolution 0 -- this bug
      may have had follow-on implications in creating an endless loop within
      "kdu_server" when serving content involving many codestreams, at the
      lowest resolution only.
   -- Fixed a minor memory leak in "kdu_vex_fast".
   -- Modified all processing time measurements to use the `kdu_clock' class
      defined in "kdu_elementary.h" rather than directly calling the ANSII C
      clock() function -- the `kdu_clock' class uses the clock() function on
      Windows, where the clock() function actually measures system time; on
      most other operating systems, however, the `kdu_clock' class uses
      other functions (like `gettimeofday') that measure system time.  This
      prevents inconsistent times being reported on non-Windows platforms
      where the ANSII C clock() function typically measures ellapsed CPU
      time.
   -- Modified "demo 5" within the "kdu_render" demo application to use N
      threads, where N is the number of available physical/virtual CPU's,
      rather than just 2 threads.  Also modified "demo 6" to use multi-threaded
      processing, so as to somewhat enhance the demonstration.
   -- Added multi-threaded processing (just a few lines of code) to the
      "KduRender2.cs" C# example application and the "KduRender2.java" Java
      example application.


Changes from version 6.4.1 to 6.4.2
-----------------------------------
These are all minor bug fixes, as follows:
-- In `kd_meta::find_active_scope_and_sequence',
   replace
            for (cne=0; ((rge=rg->expansion->access_range(cne)) != NULL) &&
                 !have_stream_match; cne++)
              have_stream_match = true;
   with
            for (cne=0; ((rge=rg->expansion->access_range(cne)) != NULL) &&
                 !have_stream_match; cne++)
              have_stream_match = scope->entities->test_codestream_range(*rge);
-- In `kdu_range_set::expand',
   replace
          from += (min_idx-from)/rg->step;
   with
          from += rg->step * ((min_idx-from)/rg->step);
-- In `kd_window_context::process_window_changes'
   replace
      for (csn=0; !(csrg=stream_set.get_range(csn)).is_empty(); csn++)
        { 
          ...
   with
      for (csn=0; !(csrg=stream_set.get_range(csn)).is_empty(); csn++)
        { 
          if (csrg.context_type == KDU_JPIP_CONTEXT_TRANSLATED)
            continue;
          ....
-- In "block_encoder.cpp", the convex hull analysis function has been modified
   slightly to improve efficiency and avoid introducing misleading convex
   hull points after the 8th coding pass.
-- In `kdu_servex::read_codestream_summary_info', a call to a new function
   `ensure_monotonic_log_slopes' has been introduced in order to better
   massage problematic RD slope information that might be found in the
   codestream comments and later used for rate-distortion optimized
   content delivery.  The original version could potentially leave the
   slope threshold tables without the strictly monotonic property that is
   expected by the `kdu_serve' object -- if this happens an endless loop
   could arise while simulating increments.

Changes from version 6.4 to 6.4.1
---------------------------------
-- Fixed three small bugs in "kdu_server" which could easily cause memory
   faults.
-- Fixed a bug in the support for broadcast profiles in "kdu_params.cpp",
   in which the x and y sub-sampling factors were accidentally reversed
   so that 4:2:2, for example, was not properly handled.
-- Added a "-no_decode" option to "kdu_expand", so that comments could be
   printed using "-com" without having to decode the entire image.
-- Also added a "-no_res" option to "kdu_expand", so that resolution
   information is not written to the output file (if the format supports it);
   this is only important for TIFF files containing Geo referencing TIFF
   tags, where at least one application (OSX Preview) currently cannot
   handle the combination of both types of tags -- almost certainly a bug
   in that application, rather than "kdu_expand".
-- Added static build configurations to the Xcode workspaces for the core
   system (builds a static library) and demo apps (builds a static version of
   the "kdu_show" application called "kdu_shows") for the convenience of
   MAC users.
-- Augmented the Java native interfaces generated by "kdu_hyperdoc" to include
   a check for NULL objects passed in place of by-reference arguments to the
   underlying Native C++ API function.  This could happen if a developer does
   not inspect the form of the C++ API function, in which case a segmentation
   fault might occur at run-time.  Of course, it is the programmer's
   responsibility to know how to use a function, but additional checks in the
   JNI marshalling interface help improve robustness.  The new implementation
   generates an appropriate Java exception (of type KduException) which
   contains an informative text message -- unlike the other Kakadu-inspired
   Java exceptions, which contain only the exception code.

Changes from version 6.3.1 to 6.4
---------------------------------
This release represent a very big leap forward indeed.  We would probably
be calling it version 7.0, were it not for the fact that we don't want to
save our licensees from having to take action to update their licensed
version.

Major changes in this release:
* This release includes major changes to Kakadu's server technology.  The
  main server application provides full support for IPv6 now, but the
  core `kdu_serve' object has been completely re-implemented from scratch
  so as to provide the following important features:
  >> JPIP service preference are now supported at the core data-bin increment
     generation machinery, enabling the client to control the sequencing of
     codestream data and, most importantly, whether or not the server limits
     the scope of the requested Window of Interest.
  >> The new `kdu_serve' implementation can support arbitrarily large
     window-of-interest requests, referring to an almost unlimited number of
     codestreams at once.  The implementation automatically sequences content
     through a multi-level cache hierarchy, keeping only a modest amount of
     the compressed imagery active in memory at any given time, yet still
     managing to achieve good progressive delivery of content.  This is done
     by managing progressive increments in resolution and quality over the
     entire content window, while staging content into active memory in a
     fashion which does not overly burden the server.  Tests suggest that
     hundreds of clients could be served simultaneously while each requesting
     enormous windows of interest.  It is still beneficial for the client to
     keep its requests small enough to fit entirely within the server's active
     memory window, since that leads to the most effective quality progressive
     experience.  Depending upon prevailing JPIP preferences, the server
     decides whether to limit the window of interest in this way or not.  The
     "kdu_winshow" and "kdu_macshow" applications take advantage of this to
     provide the best possible browsing experience, requesting the server's
     best progressive delivery or the server's full window support, depending
     on whether or not the interactive user is working with a defined focus
     box.
  >> The new `kdu_serve' implementation attaches itself to only a limited
     number codestream resources in the backing `kdu_servex' target at
     any given time (currently at most 6, but easily changed by modifying the
     constants defined in "serve_local.h").  This ensures that sources with
     a very large number of codestreams can now be served without having the
     server memory grow rapidly.  With regard to bounded memory consumption,
     the current implementation is not entirely complete, since it provides
     mechanisms to throw away intermediate transcoding resources and even
     arbitrary portions of its cache model, but these mechanisms are not yet
     used.  Importantly, though, the new implementation is a platform which
     will support highly efficient memory bounded servers in the future.  The
     next release should see further gains in this area.
  >> The new `kdu_serve' implementation handles stateless JPIP requests much
     more efficiently than its predecessor.  Stateless behaviour is now
     built in as an optional mode for `kdu_serve', rather than being
     implemented on top of a stateful server by explicitly clearing the
     cache model on each request.  The new implementation still needs to
     maintain cache model information within the span of servicing a single
     request, but it provides highly efficient means to completely discard
     the cache model information created during the service of a stateless
     request, in preparation for another one.
  >> The new `kdu_serve' implementation now natively supports multiple
     independent JPIP channels.  Previously, JPIP channels were supported at
     the application level by switching a single `kdu_serve' context back
     and forth between the windows of interest associated with each active
     JPIP channel -- of course, this was not efficient, but it did offer the
     first ever implementation of multiple JPIP channels.  The new
     implementation preserves separate contexts for each channel, each with
     their own windows into the internal cache hierarchy.
  >> Finally, the new `kdu_serve' implementation provides a more efficient
     mechanism for handling JPIP cache model manipulation statements.  These
     are collected into an object from which the relevant cache model
     manipulation statements are extracted on-demand while the relevant
     content is active in memory -- this can greatly reduce the memory and
     computational burden of handling cache model manipulation (and also
     stateless requests).  Care is taken, of course, to comply fully with
     the JPIP standard, which means that model manipulation statements which
     do not relate to requested content must still be applied to the cache
     model at some point (if not immediately), except in the stateless mode,
     moreover the point at which this is done correctly takes into account
     the presence of any requests on other JPIP channels -- the method is
     similar to that required to manage memory cache integrity in a multi-CPU
     environment.
  >> We have done our best to maintain the interface to `kdu_serve' as it
     was before, at least for applications which may have been using its
     most basic features; however, some interface changes were required to
     support multiple window contexts, service preferences and management
     of cache model manipulation instructions in a clean way.
  >> The new implementation has been extensively tested and debugged, but
     of course there are a lot of changes here, so please report any anomalies
     that you find.

* Kakadu now supports arbitrarily shaped region of interest metadata in
  JPX files.  This is a new features which is being standardized as part
  of Ammendment 3 to IS15444-2.  Previously, regions of interest were
  limited to horizontally/vertically aligned rectangles and ellipses.  Now,
  arbitrarily oriented ellipses and arbitrary quadrilaterals are supported,
  along with all unions of these shapes -- e.g., polygons, polygons with
  rounded corners, etc.  Kakadu provides full support for these general
  regions of interest through its JPX file format support, through the
  metadata overlay rendering machinery of `kdu_region_compositor' and through
  new tools created to support ROI shape editing and discovery of geometric
  attributes.  The `jpx_roi' object has been massively extended (in a
  backward compatible manner) to support the definition of quadrilaterals
  and oriented ellipses and the computation of important geometric properties
  such as skew; area; centroid; tightest bounding rectangle of any orientation
  (for orientation-independent width and length); and major/minor axis lengths
  and angles.
     In addition to this, Kakadu now provides a very powerful and
  completely platform independent tool to edit and discover higher level
  attributes of regions of interest.  This is the `jpx_roi_editor' object.
  It can be used to build a very powerful interactive shape editor, with
  the addition of an almost negligible amount of GUI-dependent code to
  detect mouse clicks and the like -- for a demonstration, see the
  "kdu_macshow" application which provides a very capable and intuitive
  shape editor.  The `jpx_roi_editor' object manages anchor points, selection,
  dragging, deletion and addition/extension of existing regions, while
  providing a set of simple functions which a GUI application can call to
  discover the edges and arcs that need to be drawn as changes occur.  The
  object presents three different editing modes, which affect the behaviour
  which occurs when a user drags anchor points around or adds new regions.
  The editor includes sophisticated "snap to vertex", "snap to boundary" and
  "snap to shape induced guidelines" features which work properly even for
  curved boundaries.  The editor recognizes the connections formed between
  vertices, edges and elliptical curves and exploits them to make editing
  natural.
     Of particular interest is the fact that `jpx_roi_editor' provides
  high level capabilities to interpret and create complex regions representing
  filled polygons and paths.  Paths are created by individual ROI segments
  which line up in a natural way, optionally with circular junction points.
  Paths are discovered automatically and revealed in the "path editing mode".
  The editor can automatically set the thickness of a path around its
  skeleton, based on user specifications and this is a very powerful way
  to draw useful regions of interest for inclusion in the metadata of a JPX
  file.  The editor can also detect and report path segments, path loops,
  and the like so that automatic tools could navigate metadata-induced
  paths within an image or composition.  Finally, the editor can automatically
  fill a path with an optimized polygon representation, using as few ROI
  elements as possible.  This makes it relatively easy for an interactive
  user to set up arbitrary polygon regions of interest without having to
  explicitly form them from the underlying quadrilateral/elliptical elements.
     Right now, only "kdu_macshow" provides a graphical interface to the
  `jpx_roi_editor' to show off its features for interactive editing.  However,
  the same features could (and almost certainly will) easily be incorporated
  into "kdu_winshow" or any interactive viewing tool on any platform.

* Along with the introduction of arbitrarily shaped regions of interest,
  the `kdu_region_compositor' object's metadata overlay drawing code has
  been dramatically improved.  Many new customization options exist and the
  regions are drawn much more beautifully than in the past.  Perhaps of
  greatest interest is the fact that metadata overlays are now blended onto
  their underlying compositing layers with a global opacity value that can
  be dynamically selected.  This allows the strength of the overlay to be
  visually modulated without having to redraw the metadata or re-render
  anything.  The whole system is optimized to update the minimal amount
  of the buffer surface as overlays are modulated, which allows metadata
  of interest to be efficiently "flashed" on/off or "pulsed" briefly to
  reflect changes, say, in the metadata selected within a catalog.  These
  features are exploited to create a much more visually pleasing and
  meaningful browsing experience within "kdu_macshow" and "kdu_winshow", tying
  together the textual metadata catalog side bar and the visual imagery
  view in an impressive manner.

* The `kdu_region_compositor' object manages multiple composited image layers,
  each of which is constructed from codestreams.  The object also provides
  important functions to access regions and metadata associated with these
  image layers.  In previous versions, these functions identified the
  underlying imagery through a combination of indices: JPX compositing layer
  indices; codestream indices; image component indices; MJ2 track and frame
  indices; etc.  Unfortunately, this was not always done in a uniform way
  as the function interfaces evolved.  Moreover, these indices remained
  insufficient to distinguish between different imagery layers which were
  constructed from the same underlying compositing layer (for example),
  but composited differently.
     In this new release, rather than trying to patch more changes onto the
  existing machinery, there has been a sweeping reform of many of the
  interface functions offered by `kdu_region_compositor', which is based
  around two new classes: `kdu_ilayer_ref' (an unambiguous reference to an
  imagery layer) and `kdu_istream_ref' (an unambiguous reference to a single
  codestream, as it is used within a single imagery layer).
     By and large, all the same functions are available as before except that
  the terms "compositing_layer" and "layer" have generally been replaced by
  "ilayer" in the function name, and the clumsy collection of indices which
  are passed across their interfaces (along with the sometimes torturous API
  documentation) have been replaced with a `kdu_ilayer_ref' or
  `kdu_istream_ref' instance.  New functions are provided to query the
  constitution of the imagery layers and/or codestreams associated with
  these ilayer/istream references.  Although the changes are likely to
  impact almost any application which currently uses `kdu_region_compositor',
  the changes required for compatibility with the new interface functions
  are generally minor and very obvious.
     Ultimately, this change is a necessary and very good thing.  With these
  changes in place, `kdu_region_compositor' now supports arbitrary cropping,
  rotation, mirroring, scaling and composition of arbitrary imagery pieces
  (JPX compositing layers, MJ2 tracks, even raw codestreams), reusing them
  at will to create interesting image surfaces.  With these changes,
  `kdu_region_compositor' can easily and transparently be adapted to handle
  multiple simultaneous image sources in the future -- e.g., dynamically
  composit an MJ2 video track with a raw codestream.
     The main catalyst for the change in this version was the need to uniquely
  and persistently identify imagery against which a JPX region of interest
  is considered registered during interactive editing of its shape, for
  cases where the same region of interest may appear simultaneously in
  different imagery layers.

* The core system's sample data processing machinery has been modified
  to ensure that whenever `kdu_codestream::change_appearance'
  has been used to transpose imagery (possibly with additional flipping), the
  underlying wavelet transform analysis and synthesis operations will be
  performed in such a way as to ensure that the transposition operation
  is implemented perfectly, even for reversible transforms.  In previous
  versions of Kakadu, transposition merely caused the coordinate systems
  to be transposed and code-block samples to be transposed on the fly during
  block encoding/decoding, after suitable rearrangement of the code-block
  and subband indices.  Unfortunately, this led to the introduction of minor
  imperfections in the least significant bits when used with reversible
  wavelet transforms, since the non-linear integer rounding steps in these
  transforms means that vertical and horizontal transform stages do not
  perfectly commute.  In the current version, the internal wavelet analysis
  and synthesis steps now take into account the presence of any such
  transposition, performing the horizontal and vertical operators in a
  different sequence if necessary.  This means, for example, that a rotated
  view of a losslessly compressed medical image (generated using Kakadu's
  super-efficient appearance transforms) is indeed 100% lossless,
  whereas only the original orientation was truly lossless in previous
  versions of Kakadu.

* The interpretation of the 16-bit unsigned `slope_thresholds', used by
  Kakadu's core codestream generation system for managing R-D optimization,
  has been changed slightly, but the actual description of these values,
  as supplied to `kdu_codestream::flush', has been converted from a vague
  one (with an arbitrary unspecified offset) to a precise description.
  Specifically, in previous versions of Kakadu, the slope thresholds were
  given by 256*(256-32+log_2(Delta_D/Delta_L)), where Delta_D is change in
  total squared error distortion (with samples normalised to a unit nominal
  range) and Delta_L is change in code length, measured in bytes.  This
  expression was never formally given in the Kakadu API docs, however, and
  all one could count upon was that the slopes would be of the form
  A + 256*log_2(Delta_D/Delta_L) for some constant A.  Now, the slope
  thresholds are formally defined to be: 256*(256-64+log_2(Delta_D/Delta_L)).
     Along with this change of definition, the R-D information which is written
  to a codestream COMMENT marker segment (by default) has also been changed,
  but a new heading is given for the comment, which is actually correct this
  time.  Previously, the heading was
  "Kdu-Layer-Info: log_2{Delta-D(MSE)/[2^16*Delta-L(bytes)]}, L(bytes)", but
  the actual values found in the first column were equal to
  log_2(Delta_D/Delta_L) - 32.  From version 6.4, the heading is
  "Kdu-Layer-Info: log_2{Delta-D(squared-error)/Delta-L(bytes)}, L(bytes)", and
  the values found in the first column are equal to log_2(Delta_D/Delta_L)
  exactly.  Applications wanting to use R-D information from codestream
  comments should now parse each of these two types of headings and
  interpret the column data accordingly.  Note that the original heading
  was quite misleading, because the offset of 2^16 inside the log, suggested
  log_2(Delta_D/Delta_L) - 16 instead of log_2(Delta_D/Delta_L) - 32.

Other Significant Changes:
* Substantially modified the support for Networking through "kdcs_comms.h",
  in the following ways: a) IPv6 is now fully supported; b) The services
  defined in "kdcs_comms.h" are now fully abstracted from the underlying
  platform-dependent specifics, so that dependent services and applications
  (notably, `kdu_client' and "kdu_server") are implemented in a platform
  independent way.  Name resolution, address discovery, etc., are all
  abstracted through the API's defined in "kdcs_comms.h" and this is now
  done through function and class interfaces, without the use of any macros.
  In this way, dependent tools need not explicitly or implicitly import
  any of the networking-specific system headers.
* Further to the above, the parsing and interpretation of HTTP headers and
  URL's, as performed by "kdu_server", "kdu_macshow" and "kdu_winshow", and
  as provided by various functions offered by the `kdu_client' class, has
  been enhanced to offer full support for IPv6 addresses.  Specifically,
  URI's are now formed and parsed according to RFC 3986, which prescribes
  a bracketed scheme for encapsulating IPv6 literals.  Moreover, such
  bracketed literals are explicitly excluded from the usual hex-hex
  coding procedures that are applied to host names.
* Added support for the Broadcast profiles, which are the subject of
  Ammendment 4 to IS15444-1.  Specifically, `Sprofile' now accepts the
  `BROADCAST' option and `Sbroadcast' has been added to describe the
  specific broadcast profile level, multi/single tile and reversibility
  attributes of the profile.  During codestream generation, the profile
  requirement are checked and informative messages are provided to help
  you set things up correctly.
* Modified the interpolation code in `kdu_region_decompressor' which is
  responsible for scaling imagery by arbitarily selected amounts.  The
  modified implementation uses 6-tap interpolation kernels (rather than 7,
  which was a bit silly upon reflection) and allows the kernels to gracefully
  degrade into 2-tap (bi-linear) interpolators as the image becomes heavily
  expanded.  There are two benefits of this: one is implementation speed; the
  other is the ability to control overshoot/undershoot which can become
  visible at large expansion factors when interpolating content with strong
  step edges.  A `ku_region_decompressor::set_interpolation_behaviour' function
  allows you to decide how these things happen.  The new implementation also
  contains additional SIMD acceleration paths for x86-family processors.
* Added an `ORGtlm_style' parameter attribute to the core system, which can
  be used to specify the formatting style of TLM marker segments introduced
  if `ORGgen_tlm' is used during compression.  This allows any of the 6
  possible TLM structures to be selected, which is needed to meet the
  expectations of at least one application-specific profile.
* Added a progress indicator feature to "kdu_compress" and "kdu_expand" which
  can give you a decent idea of what is going on inside at regular intervals --
  useful if you're processing some massive images.  This feature is accessible
  via the `-progress' argument in both applications.
* Added YUV file support to the "kdu_v_compress" demo app.
* Added an option to the kdu_render" demo app to include the rendering of
  region-of-interest metadata overlays, blended on top of the imagery itself,
  so as to provide a very simple example of the powerful overlay rendering
  features offered by `kdu_region_compositor'.  This may also be used with
  floating-point rendering precision, demonstrating the correct generation
  of metadata ovelays even for floating point frame buffers.
* Slightly modified the way in which codestream scaling factors are selected
  by the "kdu_merge" demo app when constructing a custom JPX compositing layer
  from multiple codestreams, each having different sizes.  The new
  implementation selects the rational scaling factors which best approximate
  the ideal codestream scaling factors required to perfectly register
  the component codestreams for a single compositing layer, subject to the
  constraints imposed by JPEG2000 Part 2 (IS15444-2).  This is only relevant
  for very advanced applications, since compositing layers almost always
  correspond to individual codestreams, or a colour codestream and an opacity
  codestream having simply related dimensions.
* Added an optional `flags' argument to `kdu_region_compositor::process' which
  provides the application with further control over the way in which the
  compositor sequences work and chooses to report regions which have been
  processed.  For example, you can now choose to defer the reporting of
  processed regions, arranging for them to be internally accumulated and
  aggregated across multiple calls to the `process' function.  The new
  options can be used to improve the efficiency of interactive applications
  (this is done in "kdu_winshow" and "kdu_macshow"), minimizing redundant
  or fragmented screen updates while still providing a highly responsive
  experience.
* Fixed a bug in `jpx_input_box::url_fopen' which could cause a segmentation
  fault if reading a JPX file which has relative links to codestreams when
  the original JPX file was not opened directly by `jpx_family_src'.
* Slightly modified the way in which regions of interest are mapped between
  codestream canvas coordinates and the composited image created by
  `kdu_region_compositor', such that the integer rounding conventions lead
  to much more precise localization under arbitrary scaling and rotation of
  the imagery.  This does not affect the way imagery itself is rendered or
  scaled, but it does very slightly affect the way in which regions of interest
  (as found in JPX ROI Description boxes) are aligned and rendered onto
  metadata overlays.  Previously, regions of interest did not quite perfectly
  line up with the imagery at less than full resolution.
* Augmented the `kdu_region_compositor' functions `map_regions',
  `find_point' and `search_overlays' in various ways.  For example, the
  last two functions can now assess the visibility of the `point' with
  respect to actual rendered opacity values at that location for the layer
  in question and/or overlaying layers.  `find_point' is also able to
  enumerate all layers in which a given location is contained and visible
  with respect to a visibility threshold.  These features are used by
  `kdu_macshow' and `kdu_winshow' to improve the sensitivity of the user
  interface to visibility and opacity.
* Made some very slight changes to the way "kdu_macshow" and "kdu_winshow"
  respond to double clicking of imagery which is associated with metadata
  shown in the catalog sidebar and double clicking of catalog nodes which
  are associated with imagery to improve navigation in complex situations.
  Also made some minor improvements to metadata editing features of these
  demo apps.
* Improved the efficiency of `kdu_region_compositor' in rendering
  dynamically changing metadata overlays by fixing some problems with the
  code which detects and avoids redundant processing.
* Added a "-com" feature to the "kdu_expand" demo, which allows for the
  command-line printing of textual COM marker segments found in the codestream.
* Fixed a minor bug in "image_in.cpp" in the reading of Palette tables for
  palettized TIFF images.  In some cases, the compiled code may have reversed
  the order of the palettes for individual components, since the table reading
  functions were invoked through an arithmetic expression whose evaluation
  order was not guaranteed to be preserved as Left-Right by the compiler.

Changes from version 6.3 to 6.3.1
---------------------------------
* Fixed a one-line bug in "image_out.cpp" which affected the way raw and Tiff
  files were written out by the "kdu_expand" demo application for reversibly
  compressed 8-bit data.
* Fixed a bug in the core system which prevented the fragmented compression
  option from being used successfully with images whose boundaries were not
  aligned with tile boundaries on the canvas coordinate system.
* Modified the way IPTC metadata is read from and written to TIFF file which
  contain the IPTC TIFF tag, in an attempt to maximize compatibility and
  re-usability with other TIFF generators/readers.
* Slightly changed the code in "kdu_server" which checks for target name
  consistency across requests, so that the comparisons are performed in a
  case insensitive manner if the server has been configured to use case
  insensitive file names.
* Fixed a bug in "jp2.cpp" which affected the generation of
  JP2/JPX dimensions boxes in the case where a Part-2 codestream has more
  multi-component transform output components than codestream components.
* Fixed a bug in "kdu_region_decompressor", which could result in an access
  violation when processing codestreams which use a multi-component transform
  to synthesize extra image components.
* Fixed a bug in the core system which could manifest with Part-2 codestreams
  in which the number of codestream components is 2 or more fewer than the
  number of multi-component transform output components.

Changes from version 6.2.1 to 6.3
---------------------------------
* Completely overhauled the `kdu_region_decompressor' object so that it
  supports pretty much any image resampling operations desired, allowing
  images with too few DWT levels to be rendered at vastly lower resolutions
  than those natively available.  Disciplined interpolation/anti-aliasing
  strategies are employed, along with SIMD instruction acceleration, where
  appropriate.  The result is that you can now render original content at
  arbitrary resolutions, with very high quality and throughput.
* Simultaneously upgraded `kdu_region_decompressor' with an internal floating
  point processing path and the capability to retrieve floating-point
  precision data via two additional overloads of the `process' function.  The
  internal processing paths are now: a) 16-bit fixed-point; b) 32-bit floating
  point; and c) native pass through from the codestream decompression engine
  (where no signal processing/colour conversion is required).  The
  internal modes are automatically matched to the required precision and
  the available data types from the `kdu_multi_synthesis' engine, which
  may vary from tile-to-tile if the image was compressed in a strange way.
  All required conversions are provided transparently so that any `process'
  function may be called.  The customizable "white stretching" capability,
  which expands the dynamic range of low bit-depth imagery to match the
  exact minimum and maximum values associated with the precision requested
  via `process', now even works with palettized imagery.  Moreover,
  the floating-point versions of the `process' function always expand the
  dynamic range of whatever original components (or palettes) are involved
  to match the range configured by the application.  This means, for example,
  that a 4-bit image (values from 0 to 15) and a 12-bit signed image (values
  from -2048 to 2047) can all be consistently shifted and scaled into floating
  point values with a range of, say, 0.0 to 1023.0, or, as another example,
  0.0 to 1.0.  This is very important when creating high precision compositions
  from multiple image components or even multiple codestreams.
* Modified `kdu_region_compositor' to take advantage of the new arbitrary
  resampling features in `kdu_region_decompressor'.  As a result,
  image compositions are rendered crisply at any resolution, regardless of
  what transformations are required for the individual image layers.  The
  external API hides all this very nicely and the `find_optimal_scale'
  function correctly and consistently limits its return value to maximum and
  minimum supported scales associated with any composition.  This means
  there is no longer a risk of a crash in "kdu_show" when zooming in or
  out by vast amounts.
* The `kdu_region_compositor' object has also been augmented to support
  high precision processing throughout.  In particular, all you have to do
  to get `kdu_region_compositor' to perform all decompression, blending,
  scaling and composition at floating point precision is to override its
  `allocate_buffer' function with one which allocates a buffer of interleaved
  floating point samples (4 per pixel), in place of a buffer of 8-bit sample
  (32-bits per pixel).  Although high precision data is been available from
  lower level API's in Kakadu for some time, this new innovation means that
  all high level interfaces in Kakadu now support high precision rendering
  for scientific, medical or other applications, while offering the full set
  of integrated features.
* Substantially changed the way `kdu_region_compositor' paints metadata
  overlays so that far fewer calls to the `process' function are required
  to reflect changes.  Region-sensitive metadata is now managed within
  segments which tile the rendering canvas, so that changes in metadata
  may be limited to the segments which are affected and metadata painting
  may be strictly ordered according to region size, without running the
  risk of large sorting costs (because the segments are kept small).  There
  are lots of benefits to the new implementation.  At the same time, the
  `kdu_region_compositor' object now provides a lot more flexibility in
  the way overlay painting can be configured.  Overlays can be dynamically
  weighted as they are being rendered and the weights can be changed
  by the application and repainted with very low computational overhead.
  This allows overlay animation to be implemented efficiently -- as
  demonstrated by the "kdu_winshow" and "kdu_macshow" applications.
  -- Note carefully that to realize the above enhancements, one function
     `kdu_region_compositor::customize_overlays' has been changed in a
     manner which is not backward compatible.  This is deliberate, in
     order to draw attention to developers who have been using this
     function with previous releases -- no functionality has actually
     been lost, however.
* Added a new global macro KDU_NO_SSSE3, which may need to be defined when
  building in compilation environments which do not provide the
  "tmmintrin.h" header required for Intel SSSE3 instruction intrinsics.
* Provided a formal type definition (`kdu_exception') that should be used for
  all exceptions thrown or caught across Kakadu interfaces.  All Kakadu
  objects comply with this definition.  Currently, `kdu_exception' is just an
  `int', so all the advice given in earlier versions about throwing/catching
  integer exceptions remains valid.  However, in case we choose to change
  this definition in the future, it would be a good idea to explicitly use
  `kdu_exception' rather than `int' in your catch clauses and when throwing
  exceptions from an error handler.  Moreover, Kakadu now provides some
  explicit exception values which are recommended.  In particular, you are
  encouraged to throw `KDU_ERROR_EXCEPTION' from within an error handler.
  In addition to this, Kakadu now catches and converts `std::bad_alloc'
  exceptions into the special `KDU_MEMORY_EXCEPTION' value (of type
  `kdu_exception') for the purpose of passing such exceptions across
  programming interfaces and between threads in a multi-threaded processing
  environment (`kdu_thread_env'). These exceptions are automatically converted
  back to `std::bad_alloc' if rethrown from wihtin Kakadu, but a special
  function `kdu_rethrow' is provided to help you rethrow any converted
  exceptions if they occur in a context for which you are responsible -- e.g.,
  when recovering the type of an exception in a call to
  `kdu_region_decompressor::finish' or `kdu_thread_entity::terminate'.  All
  of this basically means that exceptions are now handled and transported in a
  methodical and readily extensible manner, throughout Kakadu.
* When throwing and catching exceptions within Java, all the right things
  happen.  In particular, the `kdu_jni/KduException' class has been extended
  to take an optional exception code, which is used to preserve the
  `kdu_exception' value thrown within a native method.  When throwing
  exceptions from Java implementations of Kakadu callback functions you are
  also recommended to pass an appropriate exception code (otherwise
  `KDU_NULL_EXCEPTION' will be used) to the `kdu_jni/KduException' constructor.
  In particular, in a Java error handler, you should throw an exception
  constructed with `Kdu_global.KDU_ERROR_EXCEPTION' as the code.  See the
  "KduRender.java" demo for an example -- more explanation in the end notes.
* Carefully reviewed all the places where dynamic memory allocation occurs
  within Kakadu, in order to ensure: a) that a `std::bad_alloc' exception
  should always be thrown if there is insufficient memory; and b) that in
  the event of such an exception being thrown, the relevant objects should be
  left in a state which can be properly cleaned up via their destructors,
  allowing an application to respond to the exception and continue, if desired.
* Added a convenient feature to the "kdu_merge" demo app, which allows
  a large number of input files to be specified implicitly where the
  files following a common numbering scheme (file name + integer suffix +
  extension).
* Slightly modified the "-composit" argument to "kdu_merge" so that it can
  accept arbitrary scaling factors when building layer compositions, as
  opposed to just integer-valued scaling factors.  Kakadu has never been
  limited to integer-valued scaling of compositing layers when forming
  JPX compositions; only this option to "kdu_merge" had the limitation.
* Added a command-line option to "kdu_server", allowing the user to configure
  it for case sensitive or case insensitive treatment of target file names in
  client requests.  Specifically, you can specify "-case_sensitive yes" or
  "-case_sensitive no".  The default behaviour is set to case sensitive for
  non-Windows systems and case-insensitive for Windows.  For more information
  on the impact of case sensitivity attributes, consult the usage statement.
* Slightly modified the `kdu_client' object so that any `kdu_client_notifier'
  object installed in the client prior to a call to `connect' will remain
  installed, at least until the network management thread terminates.  This
  allows you to have a notifier up and ready to go right from the beginning
  of a connection.  Interface documentation has been expanded to explain this
  and also point to the fact that there will be an additional, much more
  specific notification capability provided by future version of the client.
* Augmented the "kdu_render" demo application with a new rendering example,
  accessible as "-demo 6", which demonstates the use of `kdu_region_compositor'
  with floating-point precision frame buffers and high precision processing
  throughout.  Also added a "-scale" argument to this application, allowing
  images to be rendered at any desired scale (not just powers of 2) -- all
  6 examples (those based on `kdu_region_decompressor' and those based on
  `kdu_region_compositor') illustrate robust techniques for rendering with
   arbitrary scaling factors and they are all still very brief.
* Significantly improved the way in which focus boxes in "kdu_winshow" and
  "kdu_macshow" are mapped between different image elements (frames,
  compositing layers and codestreams) during interactive navigation
  (especially the type of navigation which involves a combination of
  the metadata catalog sidebar and the image view).
* Fixed some problems in the core system which could arise in the event
  that the maximum number of DWT levels (32) is used to encode an image --
  not that this makes much sense for just about any practical application.
* Fixed a couple of small yet important bugs in the implementation of
  the Part-2 multi-component dependency transform (probably the least
  utilized of the MCT transform types) in the core system.
* Fixed a very small yet critical bug in the "kdu_server" application
  which would almost certainly have caused crashes in large scale
  deployment scenarios where separate clients close after having contended
  for resources.
* Fixed a minor bug in `kdu_region_compositor' which could sometimes cause
  the `cull_inactive_layers' function to remove a shared object which is
  being used to render a single image component from a single codestream.
* Fixed a minor problem in "kdu_winshow" with the use of scrollbars within
  the catalog sidebar while image content is arriving dynamically from a
  remote server via JPIP.
* Fixed some problems with both "kdu_winshow" and "kdu_macshow" in the
  way the "Duplicate Window" menu action is handled, so that the frame
  index is correctly preserved through duplication and the duplicate window
  is more consistently resized.
* Corrected the language compliance problems which generated a large
  number of warnings under the most recent version of GCC (gcc 4.2).

Changes from version 6.2 to 6.2.1
---------------------------------
* Made some minor changes to "kdu_winshow" so as to bring its user interface
  fully into line with that of "kdu_macshow".  In particular, the "ENTER",
  "BACKSPACE" and arrow keys now have exactly behaviour in both viewers,
  when applied within the catalog sidebar -- as a consequence, the tooltips
  which explain useful keys within the sidebar are now also the same in both
  the Windows and Mac viewer.  The "kdu_winshow" application now provides more
  obvious indication of when the catalog sidebar has keyboard focus -- closely
  resembling the appearance of the Mac viewer.  Finally, "kdu_winshow" has
  been modified so that it compiles correctly when the Unicode character set
  is selected globally (a Microsoft-specific thing -- bit of a nuisance in my
  opinion) as well as when the Multi-byte character set is selected.  Lots of
  function interfaces change when the Unicode character set is selected
  (defines _UNICODE globally, which affects the interpretation of a huge
  number of MFC interfaces).  However, this is the only way to ensure that
  all control windows support the display/editing of Unicode text.  Now, when
  "kdu_winshow" is compiled with the Unicode character set (shipped that way
  by default), foreign scripts display (and edit) properly within the
  metadata editor as well as the catalog side bar. This sort of thing
  happened almost for free on the Mac.
* Enhanced the user interface of "kdu_winshow" and "kdu_macshow"; in
  particular, double-clicking an entry in the metadata catalog (or hitting
  enter while an entry is selected), as well as double-clicking within the
  image view itself, now allows you to not only highlight any relevant
  region of interest but also sequentially advance the image view through
  all of the image entities which are associated with the same metadata.
  Also, when navigating the image view based on metadata which is specific to
  an entire image entity (e.g., a compositing layer) in composited frame
  mode, the relevant image entity as a whole is highlighted in the display,
  unless it occupies the entire visible region.
* Modified the "-album" switch in the "kdu_merge" demo application so as to
  create multiple index pages (as required) at the front of a photo album
  and (more interestingly) to automatically add some initial metadata which
  can conveniently be edited within the "kdu_show" demo app to create
  interesting annotated photo albums.
* Fixed a couple of minor bugs in the implementation of `kdu_serve' which
  were accidentally introduced while bringing the behaviour into line with
  the current JPIP standard in v6.2 -- specifically, when byte limits are
  applied to a request, the attempt to satisfy the byte limits precisely,
  while counting the cost of variable length headers, had the potential to
  erase a section of bytes from a codestream header data-bin or to miss the
  delivery of some trailing bytes from a metadata-bin under unusual conditions
  (v6.2 only) and this has now been fixed.
* Fixed minor bugs in `kdu_region_decompressor' and `kdu_region_compositor'
  in which the role of the numerator and denominator variables was accidentally
  interchanged in each of the two `find_render_dims' functions, when
  calculating half-pixel shifts.  There is a slim possibility that this bug
  could indirectly trigger an access violation during vertical interpolation
  within `kdu_region_decompressor'.

Changes from version 6.1.2 to 6.2
---------------------------------
* Substantially re-implemented all the communication-related aspects of the
  "kdu_server" application, with two major implications:
  1) The new implementation is based on a collection of platform-neutral
     communication primitives, which allows the server to be deployed on
     Windows, Unix, Linux and Mac platforms in a uniform manner.
  2) The new implementation supports multiple JPIP channels within a session,
     allowing for much more efficient communication with clients that need
     to open multiple simultaneous windows into an image and manage multiple
     interactive windows of interest.  Previously, the only way to achieve
     this with the Kakadu server was to interleave requests from each window
     of interest.
  The new server design also solves a number of minor compliance issues with
  respect to the latest version of the JPIP standard (IS15444-9):
  a) No longer issues multiple "Request Pre-empted" EOR response messages
     within a single response chunk under the HTTP-TCP transport (in an earlier
     committee draft of the standard this was legal).
  b) No longer issues "JPIP-len" response headers as a means of changing a
     client's requested response length limit (such changes are not allowed).
  c) No longer ignores "pref" request fields.  The new implementation correctly
     responds to all preference requests.  It can also correctly handle
     bandwidth slicing and global bandwidth limiting preferences.
  d) Now performs hex-hex decoding of query strings on a field-by-field basis
     so that hex-hex encoded '&' characters (e.g. in a `target' string) have
     no opportunity to be mistaken for field separators.

* Completely re-implemented the `kdu_client' object, with the following
  two major implications:
  1) The new implementation is based on a collection of platform-neutral
     communication primitives, which allows the server to be deployed on
     Windows, Unix, Linux and Mac platforms in a uniform manner.
  2) The new implementation offers applications multiple request queues, each
     of which can have its own independent window of interest.  The client
     can manage any number of request queues, regardless of server
     capabilities (it can even be stateless) and for both the HTTP and HTTP-TCP
     transport protocols.  However, the client does ask for multiple JPIP
     channels from the server, and if the server grants them this generally
     makes the communication more efficient.  These features go hand in hand
     with the new Kakadu server to deliver the best performance for
     applications which require multiple windows of interest into a single
     image, video or other server-side resource.
  In addition, the new client implementation provides a number of less major
  enhancements:
  a) Status information associated with recent requests is now made available
     to the application.
  b) The application can now explicitly choose whether the client should be
     used for ongoing communication or just issuing a one-shot request.
  c) The client rigorously performs hex-hex of URL's or URL-components (e.g.,
     server name, resource, initial query fields) it receives from the
     application and hex-hex encoding of all relevant strings passed to the
     server so as to ensure strictly URI-legal behaviour -- the client
     API is carefully documented to make it clear what strings it expects to
     have contain URI-legal hex-hex encodings from the application.

* Augmented the `jpx_meta_manager' and `jpx_metanode' interfaces with the
  ability to manage JPX cross-reference boxes and to recognize cross-references
  to recognized metadata within the same source file as semantic links.  You
  can also create links, or copy them from another file's metadata via
  `jpx_metanode::add_copy' or `jpx_meta_manager::copy', and all the correct
  links will be discovered and even saved correctly as new cross-reference
  boxes in a generated JPX file.  These features work correctly even when
  cross-references are discovered (or copied) incrementally, which might
  happen when browsing a remote source via JPIP or when editing a source --
  the internal machinery is capable of deferring its discovery of links until
  all the relevant boxes are available.  Links provide a mechanism for
  creating rich metadata indices, cross-referenced against codestream/frame
  components, regions of interest, or each other, providing an interactive
  user with rich means for navigating a complex source.  All of this works
  correctly when the source is remotely located and served via JPIP.
  Kakadu interprets cross-reference links within a JPX file as belonging
  to one of three different semantic categories, based on how the link is
  formed -- grouping links; alternate child links; and alternate parent links.
  The Kakadu interpretation is natural with respect to the underlying JPX
  constructions and enables the introduction of functions which can query
  the relationship between arbitrary metadata items in a meaningful way.
  A generic function `jpx_metanode::find_path_to' is introduced to perform
  powerful relationship queries within the metadata graph, navigating links
  in a manner which is robust to self-referential loops.

* Extended the metadata overlays feature of `kdu_region_compositor' to support
  limiting the displayed regions of interest based on an arbitrary
  sum-of-products expression involving relationships in the metadata
  graph.  This feature uses the generic `jpx_metanode::find_path_to'
  function mentioned above, which is very efficient.  This allows you to
  interactively hide or unhide regions of interest based on their relationship
  to other metadata (e.g., text labels, index terms, etc.) in a natural
  way.

* Augmented the `jpx_metadata_manager::enumerate_matches' method with the
  ability to specify whether codestream or compositing layer constraints
  can be ignored for numlist boxes which do not specify any codestreams
  or compositing layers.  This feature is now used by `kdu_region_compositor'
  for determining the regions of interest which should be displayed as
  overlay information -- specifically, it allows regions of interest to
  be displayed only if they match the relevant JPX compositing layer, unless
  the associated numlist boxes do not specify any compositing layers.  The
  original JPX standard makes no comment on how the different types of
  entities in a numlist box should be interpreted when both codesteams and
  compositing layers are specified, but the new interpretation by
  `kdu_region_compositor' is more natural and useful than the previous one
  which displayed everything with a matching codestream, regardless of the
  compositing layer information provided by its numlist box.

* Improved the `kdu_region_compositor::search_overlays' function so that
  it returns a match only if the point of interest lies within the actual
  region of interest, as opposed to the ROI description box's bounding
  rectangle.  The enhanced implementation searches within the individual
  rectangular and/or elliptical regions of the ROI description box after first
  finding a match with the bounding rectangle..

* Extended the "kdu_macshow" demo applications in the following ways:
  1) Introduced full support for remote browsing of images via JPIP; in
     fact, JPIP browsing on the MAC is more exciting than on Windows machines
     with "kdu_show", since the MAC viewer allows multiple open windows to
     the same remote resource to share a common client-side cache and manage
     separate request queues for a separate interactive focus box in
     each window.
  2) Introduced a new feature which allows open windows to be duplicated
     (you can use the Command-D accelerator for this) and shrink wrapped
     around any focus box.  This allows you to rapidly create windows for
     regions of interest within an image, and works perfectly in conjunction
     with JPIP for remote image browsing.
  3) Added extensive support for editing, visualizing and navigating with
     metadata links (these are realized using cross-reference boxes, as
     described above).  The metadata editor itself and the metadata catalog
     side bar are considerably enhanced for this purpose.  For examples
     of richly annotated images built using these tools, visit the "Demos"
     section of the Kakadu web-site.
  4) Extended the metashow tool so that it properly displays non-ASCII
     UTF8 label content and so that it also displays the absolute file
     location of the header of each box.  When the image is accessed remotely
     via JPIP, the "metashow" tool also reveals the identifiers of
     the cache data-bins in which each box header (H...) and the box body
     contents (B...) are found -- shown in the box header within curly
     braces {}.  This is a most invaluable tool for identifying and debugging
     any problems with metadata communication by JPIP clients and servers.
  5) Improved metadata rendering in the "Catalog" sidebar by: a)
     properly supporting truncation of long non-Ascii UTF8 text labels; b)
     avoiding the instantiation of memory to represent labels until they
     are explicitly visualized within the catalog bar; c) colour coding
     different types of catalog entries; d) adding tooltips; e) adding
     an auto-collapse feature to prevent the catalog from becoming too
     unwieldly when auto-expansion of relevant metadata is triggered by
     user interactions within the image window; f) added features to
     automatically reorganize the prefix labels associated with large
     metadata collections, when changes of any type occur (e.g., editing or
     receiving data from a JPIP server); g) added a feature to stabilize
     the visual presentation of the catalog while changes occur in the
     background, so that a selected item always appears in the same visual
     position, even if the catalog structure is reorganized around it;
     h) added a feature to collapse catalog entries with the same name and
     data type (i.e., the same visualization) into a single entry, together
     with a visual position counter which the user can readily adjust using
     the arrow keys, in order to select any one of the visually identical
     catalog entries; i) added forward/backward buttons for navigating
     historically selected entries in the catalog (as found on all web
     browsers and most file-system browsers); j) gave the metadata catalog
     the ability to generate metadata requests, to be appended to imagery
     requests during JPIP image browsing (the JPIP implementation logic
     even issues metadata-only requests for high priority metadata updates
     required to keep the catalog responsive, whenever it has keyboard
     focus -- the catalog thus behaves almost exactly like a web-browser,
     except that content, requests and navigation are tightly coupled to
     the imagery).

* Completely overhauled the original Windows "kdu_show" application so
  that it support virtually all the same features as "kdu_macshow".  Along
  the way, the source structure has been completely reorganized so that
  it mirrors that of "kdu_macshow" and the Windows viewer is now renamed
  as "kdu_winshow".  Once compiled, both applications are known as "kdu_show"
  on their respective operating systems.  Unfortunately, a lot of things
  are quite a bit more difficult in Windows, compared to the Mac.  One
  most unfortunate thing is that Windows does not properly support
  international strings (foreign scripts) unless the entire application
  is compiled for UNICODE, which is a huge mess for applications which
  prefer to use ASCII text most of the time.  By contrast, on the MAC,
  everything can be handled beautifully as UTF-8, which is equivalent to
  UNICODE when strings are internationalized.  I tried to use unicode
  variants of important text rendering tools for "kdu_winshow" -- in
  particular, the catalog side bar text is first converted from UTF-8
  to unicode.  However, unfortunately, Windows needs unicode enabled deep
  down in the core (when Window classes are built) in order for its
  Windows to actually handle foreign character sets.  This is also
  OS-version dependent.  As a result, the new "kdu_winshow" application
  will display foreign scripts correctly in the catalog side bar when
  running under Vista -- but this had to be disabled under XP.  If only
  Microsoft had gone for UTF-8 rather than creating 2 parallel versions
  of all their GUI messages, controls, notification and a whole ton of
  standard C/C++ functions, selected globally at compile time.  16-bit
  unicode isn't quite complete anyway, so UTF-8 is a much better solution.
  Anyway, licensees can feel free to develop their own unicode-specific
  apps, since Kakadu itself does properly handle foreign scripts -- as
  demonstrated best by "kdu_macshow".

* Augmented the "kdu_merge" demo application with the ability to import
  metadata from one JPX file into another -- this is done with the new
  `-jpx_meta_swap' command-line argument which allows you to take imagery
  from one set of input files and metadata from another (potentially
  intersecting) set of files.  This is a very useful feature, since it
  provides you with a simple means of modifying the compressed imagery in
  a file while reusing all the existing metadata.

* Also augmented "kdu_merge" with the ability to import raw codestreams.
  This allows you to merge any number of raw codestream files into a single
  JPX or MJ2 file, or add raw codestreams to existing JPX or MJ2 files.
  You can also create JPX files which reference the original raw codestreams,
  JP2 or MJ2 files from which their content was sourced, via links.  Unlike
  other types of content which can be imported by "kdu_merge", the number
  of raw codestreams which can be imported in a single invocation of the
  tool is not limited by any operating system limits on the number of open
  file descriptors, so you can merge a massive number of them in one go.

* Increased the maximum line length for switch files ("-s") imported via
  `kdu_args' (all the Kakadu command-line demo executables use this) from
  2kB to 100kB, just to cover extreme cases in which machine-generated switch
  files are used to supply a massive number of arguments to one of these
  tools.

* Modified `kdu_cache::acquire_lock' and `kdu_cache::release_lock' so that they
  actually lock/unlock an internal mutex rather than leaving this to a derived
  object.  This way, all `kdu_cache' instantiations can safely add cache
  contents and retrieve cached data from separate threads, without having to
  bother with implementing exclusion devices in a derived object.  You can
  still override these function to implement your own mutex, which might be
  useful if for some reason you are compiling Kakadu on a platform with
  `kdu_mutex' does not do anything.

* Added a `kdu_threadsafe_family_src' class, derived from `jp2_family_src'
  which provides the mutually exclusive locking services of
  `jp2_family_src::acquire_lock' and `jp2_family_src::release_lock'.  This
  is not itself a big deal, but it encourages applications to use the
  thread safe version.  This is very important for applications which use
  a multi-threaded environment (`kdu_thread_env') to perform multi-threaded
  decompression and rendering functionality in the background while another
  thread (typically the main thread) accesses boxes via the `jp2_family_src'.
  This can happen somewhat transparently if you are using the services
  offered by `jpx_source' to dynamically parse the metadata of a complex
  file that might be ultimately sourced from a dynamically growing cache,
  as an example.

* Extended the core system `kdu_codestream_comment' interface and associated
  internal machinery to support binary codestream comments, in addition to
  ASCII text comments.

* Added support for the BigTIFF file format to the "kdu_compress" and
  "kdu_expand" demo applications by modifying Kakadu's `kdu_tiff' class
  to support both regular TIFF and the new BigTIFF format -- modifications to
  "image_in.cpp" and "image_out.cpp" to support BigTIFF were almost negligible,
  but modifications to the `kdu_tiff' class that they use for TIFF support
  were more substantial.  In any event, you can now read and write TIFF files
  much larger than 4GB in size.

* Added support for cropping input files on the fly in "kdu_compress". This
  has many potential uses, but perhaps the most important is the ability to
  perform fragmented compression of a massive input image (could easily exceed
  1 Terabyte) by simply invoking "kdu_compress" multiple times on the file,
  each time generating 1 or more tiles from the original input image,
  which incrementally build up the comprete implementation.  Using this
  "-icrop" option, together with "kdu_compress"s existing "-frag" option,
  you could, for example, compress a 1 Terapixel (1M by 1M) image with 256
  invocations of "kdu_compress", each generating a single 4 Gigapixel
  (64K x 64K) tile of the result, with say 12 decomposition levels -- this
  can be done on a regular desktop computer without much difficulty.  At the
  lowest viewing resolution, the entire image would have a dimension of only
  256x256 due to the use of only a very small number of tiles.

* Added an automatic interpolation/extrapolation procedure to "kdu_compress"
  to fill in missing distortion-length slope thresholds, if the "-slope"
  argument is used to supply fewer slope thresholds than there are quality
  layers.  The automatic algorithm is documented briefly in the usage
  statement.

* Slightly modified "kdu_compress" and "kdu_expand" so that they read/write
  XMP and IPTC metadata found in TIFF files and JP2 files, as suggested
  by Greg Coats.

* Added the resolution scaling adjustments suggested by Greg Coats to
  GeoJP2 information written back to TIFF files by "kdu_expand", so that
  any resolution reduction is taken into account.  Simultaneously modified
  the way in which display resolution attributes are adjusted, so that
  resolution information recorded in the output file (BMP or TIFF) remains
  correct under both resolution reduction and cropping to a region of interest.

* Introduced a new core `kdu_message'-derived object, `kdu_message_queue',
  which can be used (amongst other things) to build application-specific
  messaging services to be supplied to `kdu_customize_errors' and/or
  `kdu_customize_warnings'.  The new object takes some of the pain out of
  implementing robust error/warning handlers for multi-threaded environments.
  This is particularly useful, for applications in which messages can only
  be rendered within a specific thread -- e.g. in GUI environments with
  graphical services which are not completely thread safe.  Even thread-safe
  graphical environments can face difficulties if they try to render an
  error message within one thread which relies upon the existence of a
  window that might die in another thread.  The new object queues messages
  until they can be handled by the preferred thread.  In most cases,
  applications need only override one function
  (`kdu_message_queue::pop_messages') to do everything required -- render
  messages if in the right thread, or signal the right thread to do the same.
  The "kdu_show" and "kdu_macshow" applications both now use this object as
  the base for all their error/warning handling, which is robust to all kinds
  of multi-threadign issues.

* Added methods `jp2_output_box::get_box_length' and
  `jp2_output_box::get_header_length' which can be meaningfully invoked on
  a `jp2_output_box' immediately before or after closing it.

* Removed the older VC6 and .NET2003 project workspaces, since it
  is getting difficult to maintain them along with newer releases of the
  Microsoft development tools.

* Added a .NET2008 project workspace to complement the .NET2005 version.

* Set up compilation directives in such a way as to avoid the incorrect
  warnings issued by the .NET C# compiler when building Kakadu's managed
  native interfaces ("kdu_mni.dll") in the "managed" work space.

* Removed the DSTO-contributed port of the earlier Kakadu client-server
  technology to Unix, since the new `kdu_client' and `kdu_server'
  components are now platform neutral, and considerably expanded from
  their earlier realization.

* Fixed a bug in `kdu_cache' which prevented temporary holes in the
  available bytes for a data-bin from being correctly coalesced, when the
  missing data later appeared.  This was not previously all that important,
  because few if any applications caused holes to appear.  However, with
  the new server and client implementations, supporting multiple JPIP channels
  and associated parallel communication channels, data-bin byte ranges do
  frequently arrive in non-sequential order and need to be merged properly
  within the cache.

* Fixed a bug in `jp2_output_box::write_header', which caused boxes with length
  between 2^32 and 2^33 to be written with a truncated box length and also
  caused boxes with long headers to be written incorrectly if they were
  marked with `write_header_on_close'.  These errors may have impacted the
  generation of JPX and MJ2 files with very large embedded codestreams or
  (for MJ2) a large number of embedded codesteams.  Interestingly, the problem
  affected massive MJ2 files generated with "kdu_v_compress", for example, but
  not generally those generated with "kdu_merge" -- the difference lies in
  whether box header lengths could be known up front or had to be rewritten
  once the box was closed.

* Fixed a minor bug in `kdu_region_compositor' which caused errors when
  trying to render metadata overlays for extremely small elliptical regions
  of interest.

* Fixed a minor bug in `kdu_resolution::get_precinct_packets' which allowed
  it to return a negative number of `packet_samples' in some circumstances
  when working with compressed data sources which are interactive caches
  (almost invariably means JPIP communications).  This bug affected the
  performance of progress indicators in the "kdu_show" application during
  initial browsing.

* Fixed a bug in `kdu_client' which caused JPIP metadata requests which are
  expressed relative to a JPIP data-bin to write the data-bin id incorrectly
  in the request string.

* Fixed a bug in `kdu_serve' which could under some conditions cause the
  server to fail to honour its obligation under the JPIP standard, to
  transmit the entire contents of metadata-bin 0 to a client.

* Fixed quite a few subtle bugs in the metadata serving logic of
  `kdu_serve' (and associated `kdu_servex' support machinery).
  These bugs were not catastrophic, but could cause the server to
  produce much more metadata in response to a request than was actually
  required.  In particular: a) non-intersecting region-of-interest metadata
  may previously have been served in response to window-specific metadata
  requests; b) recursive JPIP metadata requests (those with a ":r" suffix)
  were previously served in a manner which recursed fully into the hierarchy,
  ignoring any requested depth limitations; and c) JPIP metadata-bin
  requests which specify a root data-bin were previously interpreted in such
  a way as to include the box whose contents are spanned by the data-bin,
  which is potentially much looser than the correct interpretation which
  applies such requests only to boxes found within the data-bin.
  It was previously very difficult to comprehensively test out these logic
  flaws, because Kakadu did not have a demo application which could
  illustrate rich metadata browsing alongside image browsing.

* Fixed a subtle bug in the `jx_metaloc_manager' which is used to
  implement dynamic discovery and recording of box locations in the
  `jpx_meta_manager' machinery -- this is critical for the discovery of
  links between elements in the metadata hierarchy via cross-reference
  boxes.  The bug was aroused only while testing dynamic delivery of
  cross-linked metadata via JPIP.

* Corrected deficiencies in the URL interpreting code used by `jpx_input_box'
  to transparently open external file references as if they were embedded
  JPX boxes.  Previously, URL's were stored and used in a very loose way,
  as arbitrary absolute or relative file paths.  Now, references to local
  files are stored as RFC2396-compliant URL's, with hex-hex encoding and
  a "file:///" prefix (protocol + empty authority).  Facilities are also
  provided to extract local file pathnames from ingested URL's, stripping
  the protocol prefix and performing hex-hex decoding, as required.  These
  features are offered via the new `jp2_data_references::get_file_url' and
  `jp2_data_references::add_file_url' functions and an enhanced version of
  `jpx_codestream_target::add_fragment'.  You can still read/write raw URL's,
  without error checking, but the new features enable you to properly write
  and/or ingest URL's which represent local file pathnames.  All the relevant
  demo applications implicitly or explicitly use these new features.  As a
  result, when writing JPX files with linked codestreams via "kdu_merge"
  or "kdu_show"/"kdu_macshow", the generated files contain more correctly
  encoded URL's for the file references, but when reading or serving files
  using any of the existing tools, loosely formatted URL's are still
  acceptable.  It is worth noting that applications based on older versions
  of Kakadu may not correctly read files with the new compliant URL
  formatting, but this cannot really be helped.

Changes from version 6.1.1 to 6.1.2
-----------------------------------
* Modified the order in which the EBX register is set inside __asm blocks
  within the "msvc_dwt_mmx_local.h" and "msvc_colour_mmx_local.h" source
  files, so that EBX is not altered until after the last stack variable has
  been loaded into registers.  This overcomes a potential problem created by
  the VC 9 compiler in .NET 2008, when compiling the Microsoft inline assembly
  code for debug mode -- it appears that the compiler uses the EBX register
  for stack frame manipulation.

* Corrected an oversight in v6.1 and v6.1.1 whereby the names of the
  "kdu_aux" DLL produced by the "managed" Microsoft build environments was
  left as "kdu_a60.dll" instead of "kdu_a61.dll". This had no consequence on
  functionality or usability, though.

* Corrected an error in the marshalling of boolean arrays when building
  Java native interfaces.

* Corrected an error in the destruction of `kdu_params' objects within the
  core system, which could result in a memory leak for codestreams with
  component-specific and tile-component coding parameters.

Changes from version 6.1 to 6.1.1
---------------------------------
* Introduced almost immediately after version 6.1 to fix the following minor
  issues:
  -- Failure to build properly under XCODE 3.1 on MAC platforms due to
     identification of the PIC register %rbx as clobbered in "kdu_arch.cpp".
     Problem fixed by saving temporarily to another register.  May possibly
     have caused problems on other 64-bit Unix builds.
  -- Difficulty building "kdu_macshow" for backward compatibility with
     OS-X 10.4, due to changes in the Cocoa interface definitions between
     10.4 and 10.5 SDK's.  This has been resolved by including some
     conditional compilation directives.
  -- Minor changes to documentation, help and MAC package maker files.

Changes from version 6.0 to 6.1
-------------------------------
* Provided a complete set of XCODE build environments for the MAC, to
  complement the existing Makefiles and Microsoft Visual Studio build
  environments.

* Dramatically improved the metadata editing capabilities of "kdu_show",
  while adding new options to save edited files and maintain as much
  metadata as possible in JP2 files (JPX is, of course, the preferred
  format to save images with rich metadata).

* Introduced a new viewing utility, "kdu_macshow", for the MAC (runs under
  OSX 10.5 on G4, G5 and Intel processors; should also run under OSX 10.4).
  This utility is more elegant than its long standing Windows cousin,
  "kdu_show".  It contains all the same features as "kdu_show" (with the
  exception of JPIP support, which will be enabled in v6.2), but adds
  automatic metadata cataloging and navigating features for JPX sources,
  manages multiple open windows and allows for synchronized commands to be
  delivered to multiple windows (e.g., start playing video in all windows at
  once).  It uses mostly the same accelerator keys as "kdu_show".
     It is worth mentioning that the "kdu_macshow" application is built using
  Kakadu's platform independent API's, together with Cocoa (basically,
  NextStep).  As a result, it seems likely that interested parties could port
  the application quite easily to Linux and other environments, via GnuStep.

* Improved JPX metadata management considerably, as follows (these features are
  all used by the new "kdu_macshow" application and, to a lesser extent by
  "kdu_show", for sophisticated metadata editing, navigation and integration,
  not previously offered by a Kakadu viewer).
  1. Introduced new member functions to `jpx_metanode' to make metadata editing
     more convenient.  These allow the type and contents of an existing node to
     be changed and the parent of a node to be changed.  Previously, these
     operations required sub-trees of the metadata hierarchy to be copied, in
     order to preserve all nodes other than the one that was being changed.
  2. Altered the internal operation of `jpx_meta_manager' so that metanodes are
     not physically deleted until the `jpx_source' or `jpx_target' object
     itself is destroyed.  This provides increased robustness to
     editing applications which might not take care of eliminating
     references to objects which have been deleted.  More importantly,
     it allows applications to explicitly retain references to nodes
     which have changed or been deleted and to discover such facts by
     calling `jpx_metanode::is_changed', `jpx_metanode::is_deleted' or
     `jpx_metanode::parent_changed'.
  3. Added a facility to keep track of original file locations (or
     copy source) associated with the metadata managed by
     `jpx_metanode' objects, providing the application with the
     ability to efficiently locate metanodes based on the original box
     locations (see `jpx_meta_manager::locate_node').  In the future,
     these same internal mechanisms will be used to recover the semantic
     associations (as links) implied by the use of cross-reference
     boxes and to preserve them through editing and copy operations so
     that interactive users can add, remove and navigate such links.
  4. Modified the code which reads metadata into a `jpx_source' object so
     that errors encountered in non-essential metadata can be non-fatal,
     just eliminating the affected metadata nodes from the generated
     metadata tree.
  5. Added a facility to allow applications to save an arbitrary state
     reference internally with each metadata node, to allow them to
     conveniently reconcile changes in the metadata structure (due to editing
     or JPIP delivery) with application-defined metadata structure.
  6. Added a service to efficiently and conveniently identify the set of
     metadata nodes which have been changed, deleted, added, or recently
     parsed into a `jpx_source' (e.g., because they became available in a
     dynamic cache, during JPIP browsing).  The application can now efficiently
     scan all such newly available nodes using the
     `jpx_meta_manager::get_touched_nodes' function.

* Modified the "kdu_compress" and "kdu_expand" applications to allow images
  with a given declared sample data precision to be read or saved as though
  they had a different sample data precision.  One reason for doing
  this is to overcome a weakness in the support offered
  by some third party TIFF reading/writing applications, in not properly
  supporting the packing/unpacking of sample values with non-power-of-two
  precisions.  Another reason for adding these features is that 16-bit TIFF
  files are commonly used to store data with substantially lower precision
  (e.g., only 11 or 12 bits), which will appear almost entirely "black"
  after regular compression with Kakadu (since all the values are close to
  the lower bound of the declared sample data range).  You can now conveniently
  instruct "kdu_compress" to treat the file as though the sample values had
  a lower precision.
     These modifications have nothing really to do with Kakadu
  proper, since TIFF file reading/writing in "kdu_compress" and "kdu_expand"
  is just for demonstration purposes, but a number of users requested these
  capabilities.  Be sure to carefully read the usage statement for the new
  `-fprec' (force precision) argument provided with each of "kdu_compress"
  and "kdu_expand".

* Slightly modified the summary data printing portion of the "kdu_expand" and
  "kdu_compress" demo applications so that they print large numbers
  (kdu_long's) in a nicer way.  This is achieved by augmenting the core
  `kdu_message' object with a special operator<< overload for the type
  "kdu_long".

* Provided a new core system feature to keep track of the number of compressed
  bytes in each quality layer, in each tile, image component and resolution.
  This can help applications to make intelligent choices regarding the number
  of quality layers they might choose to decode, where speed is particularly
  important.  The new feature centres around the function
  `kdu_tile::get_parsed_packet_stats', which comes with extensive comments
  to explain performance implications with different types of codestreams and
  compressed data sources.  The "kdu_expand" application is also augmented with
  a \"-stats\" argument to demonstrate the collection and printing of such
  parsed packet statistics.

* Augmented `kdu_codestream::flush' with the ability to impose constraints
  not only on the overall compressed size of each quality layer, but also on
  the size of the quality layer at each resolution and/or each leading subset
  of image components.  This is useful primarily for ensuring the generation
  of legal codestreams for Digital Cinema applications. The new feature is
  conveniently controlled via the codestream parameter system, using various
  flavours of the "Creslengths" parameter attribute.  This allows the feature
  to be added to just about any existing application without adding any
  (or hardly any) lines of code.  Along with this new feature, support for the
  Digital Cinema profiles CINEMA2K and CINEMA4K has been greatly upgraded,
  to include proper checking for legal digital cinema codestreams and
  automatic selection of suitable coding and structural parameters, where
  defaults must be supplied.  For example, you can now generate a legal 4K
  24fps digital cinema codestream using the following quite straightforward
  invocation of the kdu_compress demo app:
    >> kdu_compress -i in.tif -o out.j2c Sprofile=CINEMA4K \\
       Creslengths=1302083 \
       Creslengths:C0=1302083,1041666 \
       Creslengths:C1=1302083,1041666 \
       Creslengths:C1=1302083,1041666

* Significantly improved robustness of the core system to illegal or highly
  unusual conditions which might require the allocation of massive amounts
  of memory.  The core system should be able to correctly clean itself up
  (assuming the application calls `kdu_codestream::destroy') even after
  throwing an exception from a call to `new' which exceeds the available
  memory.

* Modified the function `kd_tile::read_tile_part_header' to check for
  tile-parts whose total length is 12 (an illegal value) and change it to
  14.  Lengths of 12 were accidentally recorded by Kakadu versions 6.0 and
  earlier, for empty tile-parts, when generating TLM information.

* Fixed the above-mentioned problem with creation of empty tile-parts with
  the illegal length value of 12 (changed to 14).

* Modified `kdu_resolution::get_precinct_packets' to take a `kdu_thread_env'
  pointer as its second (optional) argument, which is relevant only if the
  `parse_if_necessary' argument (previously the second, optional argument)
  is true.  This modification is deliberately intended to generate compilation
  errors for any application which was previously using the function in a
  manner which was potentially not thread-safe.  Read the interface description
  for more information.

* Fixed a bug in `kdu_codestream::trim_compressed_data' which has been in the
  core system since Part 2 arbitrary decomposition styles were introduced in
  version 5, but was only detected in version 6.0.  This bug could have
  potentially caused a memory access violation, but apparently hardly
  ever did so.

* Fixed a minor core system bug in `kd_thread_local.h', in which thread queue
  memory was released using `delete' rather than `free'.

* Fixed a long dormant bug in `kd_marker' which could manifest itself when
  a `kdu_codestream' object is restarted with a new set of coding parameters
  and an error is subsequently encountered in error resilient mode.

* Eliminated a potential race condition in the dereferencing of precincts
  which are candidates for recycled within the core system (one thread might
  recycle them while another is testing to see if they can be safely accessed)
  by using a volatile reference.

* Augmented the descriptions of `kdu_codestream::destroy',
  `kdu_codestream::share_buffering' and `kdu_codestream::open_block' to
  spell out the potential pitfalls associated with sharing buffering
  between multiple codestreams in multi-threaded mode (such as when one
  codestream is destroyed while another is still in use by a different
  thread).  Of course, applications which might do this are probably quite
  rare.

* Very slightly modified the return condition for
  `kdu_region_compositor::process' so that it adheres precisely to the
  explanation given in the header file (and hyperdoc API documentation).

* Fixed a more serious bug in `kdu_region_compositor', which caused it to
  generate a memory fault if a codestream, previously used to write alpha
  data for a compositing layer was recycled for use in single-component
  viewing mode.

* Fixed some bugs in the `jpx_meta_manager' and associated objects
  (i.e., `jpx_metanode').  Internally, there was a bug in the counting of
  descendants which could cause an error with some usages in the past.
  There was also an error in `jx_regions::write' which caused all ROI
  description boxes to be written with a height equal to the width.  These
  bugs have now been fixed.

* Fixed a bug in `kdu_client' (and the DSTO Unix/Linux port thereof) in which
  100-series responses (e.g., HTTP 1.1 "100 Continue" responses from
  intermediate proxies) were not properly passed over.

* Incorporated temporary fixes provided by a third party into the original
  DSTO Unix/Linux port of the "kdu_server" utility, so as to accommodate
  more than two clients.  The original Windows version of "kdu_server" is
  still the most robust implementation, until its methods get properly
  ported to a single more platform neutral realization (expected in
  Kakadu v6.2).

Changes from version 5.2.6 to 6.0
---------------------------------
* Added fast SIMD DWT and colour transformation code for the 32-bit
  precision sample processing path, which largely mirrors that already
  available in the 16-bit precision sample processing path.  This code
  can greatly accelerate high precision image compression/decompression,
  particularly at low bit-rates.  The accelerations are also available to
  DWT-based Part 2 multi-component transorms, which are more likely to
  require higher precision implementation.  These extra accelerations are
  available only on X86 platforms with SSE2 support or above.  Moreover,
  to gain access to the full set of accelerations, you should compile with the
  `KDU_X86_INTRINSICS' symbol defined.  This is automatic for 64-bit
  Windows/Linux/Mac compilations.
* Introduced additional methods on the core `kdu_thread' object, to support
  the assignment of thread priorities and the binding of threads to specific
  CPU sets (CPU affinity) in a platform independent way.  CPU binding can
  also now be optionally specified in calls to `kdu_thread_entity::create'.
* Added a new core codestream management feature which allows decompressed
  image quality to be traded for computational speed by stripping away final
  coding passes from selected code-blocks.  This has a similar effect to
  discarding quality layers, but works even when the original codestream
  was created with only one quality layer.  The mechanism is non-destructive
  so that the same codestream can be successively decompressed with different
  "truncation" policies, drawing from a single "persistent" `kdu_codestream'
  manager.  The feature is accessed via `kdu_codestream::set_block_truncation',
  which may be called at any point, allowing the quality/speed tradeoff to
  be managed dynamically, even while a single image is being decompressed.
  The feature may be demonstrated by the new "kdu_vex_fast" demo application,
  described below.
* Introduced several efficiency improvements to the management of
  compressed data in the core system, with the upshot that a speed
  improvement of several % is achieved when working with high bit-rate
  imagery, particularly if compressed data sources offer the new
  `KDU_SOURCE_CAP_IN_MEMORY' capability -- see the comments appearing
  with `kdu_compressed_source' for a full description.
* Substantially modified the `kdu_region_decompressor' object which is
  also the workhorse of the `kdu_region_compositor' object, so that these
  objects are both able to process horizontal strips of tiles together,
  in each call to their respective `process' functions.  This speeds up
  rendering of heavily tiled images, while the number of tiles processed
  together is still regulated so that limits on the amount of processing
  done in each call to `process' are respected.  For images with large
  tiles, the function still processes a user-controlled number of lines
  of one tile in each `process' call.  Along with this improvement in
  tile processing, the `kdu_region_decompressor' (and hence
  `kdu_region_compositor') object is now able to process multiple tiles
  in parallel, on machines with multiple CPU's (in addition to processing
  code-blocks within a tile in parallel); moreover, it is able to
  automatically start processing code-blocks in a new tile or strip of
  tiles once a current tile or strip of tiles nears completion so that
  some CPU's might otherwise become idle.
* Augmented the capabilities of the `jp2_input_box' and `jpx_input_box'
  objects to support pre-loading of their entire contents into memory
  via the new `load_in_memory' member function.  This method may be used
  to provide a compressed data source which offers the new
  `KDU_SOURCE_CAP_IN_MEMORY' option for the most efficient handling of
  compressed data throughout the core codestream management sub-system.
  The new capability is of principle interest for high performance
  video applications, including digital cinema.
* Augmented the "kdu_v_expand" demo application to offer a new "-in_memory"
  command-line option, which exercises the capabilities described in the
  above two bullet-points.  This allows you to explore the impact of
  in-memory compressed data sources on video rendering speed.
* Introduced a new demonstration application, "kdu_vex_fast", whose purpose
  is to demonstrate the fastest possible means of rendering JPEG2000
  compressed video content for real-time applications, including
  software-only digital cinema.  When compiled on Win64 and Win32 platforms,
  this application provides a DirectX 9 display interface with real-time
  frame rate control for high quality tear-free display.  If you don't
  have DirectX 9 support on your platform, you should undefine "KDU_DX9"
  when compiling this application.  When you do this, or when you compile
  under GCC, you get everything except display.  The application can
  write decompressed data to VIX files, exactly like "kdu_v_expand".  Also,
  as for "kdu_v_expand", omitting the output file from the command line
  argument allows you to assess the true decompression speed, without
  the limitations of file writing -- no other steps are skipped when you
  omit an output file name.  This application uses in-memory data sources
  and provides you with the flexibility to choose how processing threads
  will be distributed between frame processing engines and whether or
  not they should be tied to specific CPU's in NUMA environments -- see
  the "-usage" statement under the heading "-engine_threads" for more on this.
* Included a Unix/Linux port of the "kdu_server" utility, based on a port
  originally provided by Australia's Defense Science and Technology
  Organization (DSTO).  In the future, this port may be absorbed into
  the regular "kdu_server" application, but for now the ported files and
  relevant Make files are located under the separate "contrib" directory.
* Modified the implementation of `kdu_rijndael' in "kdu_security.cpp", fixing
  a minor error in the interpretation of the AES standard (not that it really
  matters for the way Kakadu uses AES) and correcting a platform byte order
  dependence, so that "kdu_server_admin" and "kdu_server" can interact
  successfully across different platforms; this is important now that the
  server can be compiled to run on Sparc and other processors which use a
  big-endian byte order.
* Modified "kdu_hyperdoc" to correctly cross-reference all global
  constant definitions, so as to make their accessibility and use more
  obvious, particularly for Java and C#/VB developers.
* Modified the way "kdu_hyperdoc" builds the "kdu_jni.cpp" source file
  for Java bindings, so that the class loading code is protected by a
  global mutex.  It is unclear whether any race conditions have ever
  been observed within the 3 machine instructions where the class loading
  code is potentially vulnerable; however, the mutex protection should
  guarantee that such race conditions are not possible.
* Added two new low level classes `kdu_compressed_source_nonnative' and
  `kdu_compresssed_target_nonnative' which can be inherited by foreign
  language classes (typically Java and C#) to provide fully custom
  compressed data sources and targets in those languages -- e.g., memory
  mapped compressed data sources/targets.
* Modified the make "managed/make" makefiles so as to put all
  relevant kakadu object code into a single shared library for both
  "libkdu_jni.so" and "libkdu_a60R.so", so that these libraries can
  be imported more reliably by other applications, such as JVM's (for
  the JNI bindings).  Moreover, special precautions are now taken to
  ensure that these libraries do not use X86 compiler intrinsics on 32-bit
  Linux platforms, due to the stack alignment problems which can arise.
  At the same time, though, the regular applications built with GCC do
  get to benefit from the most comprehensive collection of processor
  speedups, which are implemented via the X86 compiler intrinsics.
  The -fPIC option is now used on all relevant builds, which should also
  maximize portability.
* Created top-level build environments for Linux, MAC and Solaris operating
  systems, which build all applications, libraries and managed interfaces
  in one hit.  These are found under the new top-level "make" directory.
  Moreover, the Makefile-MAC-x86-all" makefile builds both 32-bit and 64-bit
  binaries for Intel MAC's and then joins the 32-bit and 64-bit JNI library
  into a unified library which can be used from both 32-bit and 64-bit
  JVM's -- this is done in preparation for the arrival of Leopard, in case
  it has a 64-bit Java Virtual Machine.
* The "managed/make" makefiles and the .NET build environment in the
  "managed" directory all now find the base location of the Java SDK
  by expanding an assumed environment variable "JAVA_HOME", which may
  well already be defined on your system.  This saves you having to
  modify the build environments for Java with each new Kakadu release.
  See the "Compilation_Instructions.txt" file for more on "JAVA_HOME".
* Added a "-com" command-line argument to the "kdu_compress" demo application
  which allows one or more user-supplied COM marker segments to be inserted
  into the codestream.
* Added a "-cpu" option to "kdu_render", which can be used to properly
  evaluate the computational throughput of the `kdu_region_decompressor'
  object, which is central to image rendering with Kakadu.  The speed of
  this object is identical to that of `kdu_region_compositor' for simple
  (non-composit) images.
* Fixed a subtle bug in the core system, which could have caused
  spurious error messages during codestream parsing (e.g.,
  "Illegal inclusion tag tree encountered ...").  This problem was
  occasionally reported with large tiled imagery, in conjunction with
  "kdu_server", but not previously resolved.  The errors were caused by the
  interaction of one non-fatal bug with one subtle oversight.  The non-fatal
  bug is that recycled typical tile resources did not have their
 `kd_precinct_pointer_server' object's re-initialized so that random
  access packet length information in the codestream was ignored.  This
  left the situation where some tiles could have seekable precincts, while
  others do not -- something that could happen anyway in legal, but
  unreasonable codestreams.  The fatal problem occurred when random
  access was made to precincts of one tile, while another purely sequential
  tile was still being actively parsed.  This type of problem could have
  been excited by the "kdu_show" or "kdu_server" demo applications only.
* Fixed a bug in "kdu_threads.cpp", which caused any dormant queues to be
  incorrectly shutdown by calls to `kdu_thread_entity::terminate' and
  `kdu_thread_entity::destroy'.
* Fixed a subtle bug in the core system functions `kd_tile::initialize'
  and `kd_tile::recycle', in which `sum_depths' was downshifted by 2 instead
  of 1, leading to incorrectly computed quantization step sizes for
  irreversible processing with "Cderived=yes".  Interestingly, this bug
  virtually never caused incorrect decompression, since an exact power of
  2 error in the quantization step size is always compensated by a
  corresponding incorrectly computed K_max value, representing the number
  of nominal bit-planes in a subband's code-blocks.  Moreover, the error
  can only occur in the higher frequency subbands.  Nevertheless, this bug
  may have caused incorrect treatment of ROI regions if the "Max Shift"
  ROI encoding method had been used with derived quantization.  This bug
  was accidentally introduced with the introduction of Part-2 quantization
  kernels and found by Roddy Shuler.
* Fixed a bug in the implementation of the `kdu_event' platform-independent
  synchronization object, for the case of auto-reset events on pthreads
  platforms (i.e., Unix/Linux and Mac OS) -- this problably had no impact
  on pre-existing applications.
* Identified and resolved another obscure bug with the way in which
  non-symmetric DWT kernels (only allowed in JPEG2000 Part-2) are handled
  in the case of certain boundary conditions.
* Fixed two non-compliance problems with the way Kakadu handles
  Part-2 multi-component transforms.  The first problem relates to an
  oversight in previous versions of Kakadu, whereby the presence of
  matrix- and/or dwt-based multi-component transforms was not signalled
  in the COD marker segment -- this is an entirely redundant signalling,
  but required by Part-2 of the JPEG2000 standard.  The second problem
  is that previous versions of Kakadu stored the transform coefficients
  for reversible matrix-based (SERM) multi-component transforms in a
  transposed order from that specified by the standard; the problem in
  this case originated from the fact that the main figure in
  Annex J of IS15444-2 suggests the transposed order, whereas the true
  order is only revealed by the order of subscripts in the equations.
  To fix these compliance problems, while remaining as compatible as
  possible with past versions of Kakadu, the following steps have been
  taken:
  1) A new `Cmct' coding parameter attribute has been introduced to the
     `COD_params' object, which reflects the required MCT-dependent
     modifications to the COD marker segment for Part-2 codestreams.  This
     parameter attribute is automatically generated during finalization
     of the `COD_params' object(s) so you don't need to explicitly worry
     about it.
  2) A new `MATRIX' option has been created for the `Mxform_blocks'
     attribute, to be used in describing matrix-based multi-component
     decorrelation transforms, in place of the old `MAT' option.  In source
     code, these correspond to `Cxform_MATRIX' and `Cxform_MAT', respectively.
     You should no longer use `Cxform_MAT'.  If you have an existing
     application which uses `Cxform_MAT' with Kakadu, it will continue
     to compile successfully, but you will receive an informative error
     message when you attempt to generate a codestream using this option.
     This is our way of making you aware of the need to transpose any
     reversible multi-component matrix transform coefficient array that
     you might be using in an application compiled against previous
     versions of Kakadu.
  3) Codestreams generated by previous versions of Kakadu, including
     reversible multi-component matrix decorrelation transforms can still
     be successfully decoded; indeed, the relevant transform blocks will
     show up as the old type `MAT' (`Cxform_MAT') in place of the new
     `MATRIX' (`Cxform_MATRIX') if you inspect the coding parameter
     attributes explicitly after the codestream has been ingested.  Kakadu
     detects the presence of the old-style non-compliant conventions by
     observing the absence of the `Cmct' parameter attribute when a
     multi-component transform is being used.  It should be noted, however,
     that other applications (other than Kakadu) are unlikely to correctly
     decompress codestreams generated by previous versions of Kakadu
     which incorporated reversible matrix-based multi-component transforms.
* Fixed a bug in the `kdu_thread_env' core multi-threading framework which
  allowed race conditions when multiple output codestreams are repeatedly
  opened and closed, with a single multi-threaded environment processing
  those codestreams.  Along the way, the implementation has been improved
  so that it is no longer strictly necessary to terminate all thread queues
  via the `kdu_thread_entity::terminate' function before destroying any
  codestream, so long as all work on that codestream has ceased -- in
  particular, this means that it is sufficient to wait for just those
  thread queues which are associated with a codestream to terminate, while
  other threads may be getting on with processing other codestreams; this
  is true for input, output and interchange codestreams.
* Fixed a bug in "kdu_serve.cpp" which could produce a divide-by-zero error
  for certain illegal JPIP requests, thereby compromising the integrity of
  the server application.
* Fixed another bug in "kdu_serve.cpp" which prevented JPIP "metareq"
  requests which contain box-type wildcards (*) from being correctly processed.
* Fixed a bug in `kdu_client' which caused the wrong JPIP syntax to be
  used for meta-data-only requests -- the server was changed to support the
  correct syntax back in early 2006, but the corresponding fix in the client
  was long overlooked.
* Also fixed an error in the way `kdu_client' communicates with HTTP
  proxies, so as to properly conform to HTTP/1.1 conventions.
* Fixed a bug in the TIFF reading code used by the "kdu_compress" demo
  application, which causes LZW compressed tiled images to be read
  incorrectly.  Problem reported by Greg Coats, with fix provided by
  Margaret Lepley.
* Fixed a bug in the server delegation code in Kakadu's "kdu_server" app.
* Fixed a bug in the implementation of `kdu_window::contains' so as to
  avoid possible erroneous treatment of novel client windows as non-novel
  in either the server or the client.
* Fixed a couple of bugs in rarely used data processing paths within
  `kdu_region_decompressor', which may have affected applications with
  high rendering bit-depth requirements.

Changes from version 5.2.5 to 5.2.6
-----------------------------------
* Fixed a minor bug in `tif_in::tif_in' within "image_in.cpp" which
  caused Planar Configuration TIFF files to be misread on some
  platforms, depending on how uninitialized member variables of
  classes are treated.
* Fixed a bug in `kdu_region_compositor::find_point' which could cause
  dereferencing of a NULL pointer when trying to match a screen location
  to a codestream during JPIP browsing, when the server has sent enough
  information to identify the compositing layer, but not enough information
  to identify its codestreams.  This is a very rare condition, but
  could be excited in the "kdu_show" application by depressing "ctrl"
  in the early phases of browsing a large JPX photo album.
* Made the destructor for `kdu_cache' virtual -- this was an oversight,
  since derived objects do have virtual destructors.
* Extended the cases under which tiles are considered "typical" for the
  purpose of recycling their structures.  This helps speed up the
  decompression of heavily tiled images at reduced resolution, where
  each tile can be very small.  The new conditions help with cases
  where the tiles may have different quantization or ROI parameters.
  In the process, a potentially bug-prone condition was uncovered
  and fixed.

Changes from version 5.2.4 to 5.2.5
-----------------------------------
* Fixed some backward compatibility problems with Visual Studio 6
  in the way two new message handlers were added by .NET to the
  "kdu_show" demo app.
* Fixed some minor problems with the "managed/managed_2005.sln" workspace
  and related projects, for Visual Studio 2005 users.
* Fixed a minor initialization problem in the "kdu_stripe_compressor" and
  "kdu_stripe_decompressor" objects, which could impact applications
  relying on the `get_recommended_stripe_heights' function.

Changes from version 5.2.3 to 5.2.4
-----------------------------------
* Fixed a minor bug in "kd_tcp_transmitter::configure_flow_control"
  which caused the function's arguments to be ignored.  This function is
  used to adjust min/max RTT times based on command-line instructions to
  the server.
* This version comes with separate build environments for Visual Studio
  .NET 2003 and Visual Studio .NET 2005, the latter including both Win32
  and Win64 build configurations.  Some very minor changes were introduced
  into the code to eliminate errors/warnings during Win64 builds.
* The "kdu_hyperdoc" utility has been upgraded to allow conformance with
  either the older style (V1) or the newer style (V2) syntactic conventions
  for Microsoft's Managed Extensions to C++.  The newer style conventions
  are used by default, but these are appropriate only when building with
  Visual Studio 2005.  To access the older style conventions, for builds
  under Visual Studio 2003, use the "-old_managed_syntax" argument.  If
  you use the standard build environments ("managed_2003.sln" or
  "managed_2005.sln") everything is built automatically for you, using
  the appropriate conventions.
* The multi-threading environment managed by `kdu_thread_env' now comes
  with support for the maintainance of initially dormant queues, which
  are automatically moved into the foreground for processing once the
  system seems to have entered a state in which the available processing
  threads will otherwise be permanently under-utilized.  This can be used
  to build multi-threaded applications which keep all physical processors
  active even more of the time than with previous versions (which were
  already very good at utilizing available processing resources).  The
  new capability is described in connection with the optional
  `bank_idx' argument accepted by the `kdu_thread_entity::add_queue' function.
  So far, this capability is exploited only by the "kdu_v_expand" demo
  application, which offers a new "-overlapped_frames" command-line
  argument.  Try playing around with various combinations of the
  "-overlapped_frames" and "-double_buffering" arguments on your
  multi-processor system (particularly if it has more than 2 CPU's).

Changes from version 5.2.2 to 5.2.3
-----------------------------------
* Modified the CPUID testing code in "kdu_arch.cpp" yet again, this time
  so as to protect the EBX register on x86 platforms, since that register
  is reserved for position independent code when compiling with the "-fPIC"
  option under GCC, and marking it as a clobber variable does not work in
  that context.
* Modified the tile-part header marker segment reading code in
  "kdu_compressed.h" to ignore tile-part numbers, since Adobe encoders
  get them wrong, causing premature termination of the decoding process
  in compliant decoders.
* Modified the `kdu_thread_env' thread management code to allow multiple
  codestreams to be processed simultaneously by a single Kakadu multi-theaded
  environment.

Changes from version 5.2.1 to 5.2.2
-----------------------------------
Minor changes as follows:
* Arranged for .NET build environments to write the debug ".pdb"
  files associated with the core system (kdu_v52D.pdb) and
  managed DLL's (kdu_a52D.ddb, kdu_jni.pdb and kdu_mni.pdb) into
  the "bin" directory instead of the temporary "v5_generated"
  directory, so as to facilitate debugging from other workspaces.
* Added the "KDU_AUX_EXPORTS" to the virtual functions which are
  offered by the "kdu_a52?.dll" auxiliary DLL, in addition to the
  existing non-virtual functions.  This allows Windows users to
  access these functions directly, rather than having to indirectly
  access them through function pointers.  No impact on Unix builds.
* Corrected the source of compilation warnings in GCC regarding
  base functions hidden by derived objects.
* Corrected and expanded the set of foreign language member access
  functions provided for the `kdu_sampled_range' object.
* Added an optional argument to `kdu_region_compositor::set_buffer_surface',
  allowing you to set (or change) the background colour (and transparency)
  of the rendering surface onto which imagery is composited.  The value
  is relevant only when imagery does not fully cover the compositing
  surface or when it is partially transparent.  The default (backwards
  compatible) value is an opaque white background.  You can use this
  new feature to blend rendered imagery onto another buffer managed
  by the application.
* Added `add' and `subtract' member functions to the `kdu_coords' object,
  as substitutes for the `operator+=' and `operator-=' functions which
  cannot be exported to foreign languages.

Bug fixes as follows:
* Fixed a bug in "kdu_server" which could cause the server
  to hang when used with delegation over HTTP transport channels.
* Fixed a bug in the core system which could cause a crash in the
  event that a tile had only empty tile-parts, when working in
  random access mode with seekable codestreams.
* Fixed a minor bug in `kdu_region_decompressor' caused by
  `post_convert_colour' and `pre_convert_colour' not always being initialized.
* Modified the test inside an "assert" statement found inside
  `kd_decoder::init' and `kd_encoder::init' to allow for subbands of 0
  width in multi-threaded processing -- no impact on release code used in
  final applications.
* Fixed a couple of bugs in "kdu_threads.cpp" which manifested themselves
  only when multiple synchronization conditions were simultaneously
  installed in a multi-threaded system.  This condition typically only
  happened when incremental flushing was used with a multi-threaded
  compressor, so that the synchronized flush worker job and the synchronizing
  event associated with closing down a tile processing engine could be
  registered simultaneously (depending on the circumstances).  This and
  related problems should now all be fixed.
* Fixed a minor bug in the incremental flushing routine implemented in
  the `kdu_stripe_compressor' object, which manifest itself only in
  multi-threaded environments.  The implementation has also now been
  made more efficient.
* Fixed a minor bug in the use of the `Kdu_coords.Minus' function in
  "KduRender2.java" and "KduRender2.cs" (Java and C#) demo apps, which
  affects imagery not centred at the origin.
* Made minor fixes to the code executed during Windows compilations with
  KDU_NO_SSE defined.

Changes from version 5.2 to 5.2.1
---------------------------------
This is just a bug fix release, as follows:
* Fixed a bug in "kdu_hyperdoc" which caused it to generate duplicate
  delete statements for local copies of size-incompatible buffers
  passed across JNI interfaces.
* Fixed a bug in the `kdu_region_decompressor' object which could
  cause it to crash when non-initial components are used in isolation
  from a codestream.  This bug was accidentally introduced in v5.2 by code
  which tests for the usability of some accelerated buffer manipulation
  paths.
* Fixed a remaining bug in Kakadu's server delegation feature, for the
  HTTP-TCP protocol.  The bug was in the way the `kdu_client' object
  processed notifications of server address changes.
* Fixed a couple of GCC complaints in relation to virtual objects with
  non-virtual destructors -- these were for objects which have no
  real destructors and are never actually derived.
* Fixed remaining compiler warnings for WIN64 builds.
* Modified the implementations in the X86 intrinsics headers to provide
  pure SSE/SSE2 implementations of all processor speedups, with a compiler
  switch to disable all of the older 64-bit MMX speedups.  This is done
  to cater for WIN64 builds which refuse to link 64-bit MMX instructions
  (possibly because WIN64 does not save 64-bit MMX state during context
  switches -- 64-bit LINUX has no such difficulties mixing SSE/SSE2 and MMX).
* Fixed the Macintosh makefiles (for both PowerPC and X86 processors) so
  that they build dynamic rather than static libraries, and hence can be
  immediately used with Kakadu's Java native interfaces.  The new makefiles
  were developed in collaboration with Greg Coates.
* Fixed the feature checking code in "kdu_arch.cpp" for GCC builds on
  X86 platforms -- the assembly code there previously clobbered several
  registers without letting GCC know, which sometimes caused problems in
  the initialization sequence.  In some cases, this might have led to
  GCC builds not recognizing MMX/SSE/SSE2 support, so that the SIMD
  optimizations would not be applied.
* Fixed a minor source of numerical inaccuracy in the fixed-point
  implementation of irreversible wavelet kernels when SIMD processor
  speedups are not available -- not very often, seeming as they are
  available for X86, PowerPC and Sparc processors.

Changes from version 5.1.1 to 5.2
---------------------------------
* Previous deficiencies of Kakadu on Win64 systems have been corrected.
  Most importantly, the .NET compiler cannot use Microsoft inline assembly
  for 64-bit builds, which had forced developers to disable the MMX/SSE/SSE2
  speedups provided by Kakadu when building 64-bit systems.  This problem
  did not exist for 64-bit Unix builds.  The problem has been remedied
  by providing new implementations of the SIMD speedup code using
  processor intrinsics.  These are contained in header files with names
  of the form "x86_xxx_local.h".  Conditional compilation logic selects
  the most efficient, compliant implementation based on the targeted
  machine.  For more information, see the "Compiling_Instructions.txt"
  file, which has been substantially revised.
* Additional makefiles have been provided for MAC systems with Intel
  processors.  Also, the makefile naming conventions have been revised
  for increased clarity, as have the names of the separate directories
  into which they write their results, sitting under the "lib" and
  "bin" directories.  The new names should be self-explanatory.
* "kdu_hyperdoc" now builds C# and Visual Basic interface bindings, along
  with a more comprehensive set of Java bindings.  All of these interface
  bindings and corresponding foreign language examples are now found in
  the new "managed" directory, which contains makefile, MSVC and .NET
  build environments to build everything for you -- after building and
  running "kdu_hyperdoc" from the "apps" directory in the usual way.
* One new Java application example and two C# application examples
  have been provided in the "managed/java_samples" and
  "managed/csharp_samples" directories.  The examples are in one-to-one
  correspondence between the two languages.  They demonstrate use of
  the "kdu_region_decompressor" object and now the more generic
  "kdu_region_compositor" object to render images in these languages.
* "kdu_hyperdoc" now creates directories as required, if they do not
  already exist.  This should avoid problems encountered by new users
  who do not have the correct directory layout configured on their system.
* "kdu_hyperdoc" now copies all public API header files to a common
  location, "managed/all_includes" for convenience of application
  developers.  It also writes an auxiliary DLL / shared library
  named "kdu_a52R.dll" (debug version "kdu_a52D.dll") / "libkdu_a52R.so",
  which includes all useful generic classes from the "apps" directory.
  You can simply include this DLL/shared library along with the core
  system DLL/shared library ("kdu_v52R.dll", "kdu_v52D.dll" or
  "libkdu_v52R.so" as appropriate) to access all of the functionality
  declared in the headers in "managed/all_includes".  To build this
  auxiliary DLL/shared library, use the makefiles or Microsoft
  compiler build environments found in the "managed" directory.
* "kdu_hyperdoc" now allows you to explicitly specify which classes,
  global functions or even member functions you would like to receive
  Java, C# or Visual Basic language bindings via the new "-bind"
  option.  Of course, if no "-bind" argument is used, you will get
  bindings for the whole lot, as before -- actually more than before.
* `kdu_region_decompressor', `kdu_region_compositor' and all things
  which depend uon them now perform premultiplied alpha blending, in
  addition to regular alpha blending which has been available for
  a very long time.
* `kdu_region_decompressor' provides a new mode-setting function,
  `set_white_stretch', which may be used to control how low bit-depth
  imagery is rendered into higher bit-depth buffers.  In particular,
  the use of this option allows you to ensure that low bit-depth
  images will exactly span the maximum dynamic range of 0 to 2^{B-1}
  associated with a B-bit rendering buffer, at the expense of some
  computation.  The function can set a threshold at which stretching
  happens, so that very few cases incur the small computational
  increment.  The default value of this threshold in `kdu_region_decompressor'
  ensures exact backward compatibility with previous versions of the object,
  but `kdu_region_compositor' sets the threshold for optimal rendering
  to 8-bit/sample displays.
* The "kdu_server" application has been augmented with a "-cd"
  command-line option, which allows you to select a different directory
  for storing the ".cache" files which are created to ensure consistent
  serving of image files.
* A nice summary of all compilation directives now appears in the
  "Compiling_Instructions.txt" file, which has been substantially
  restructured to make things more accessible.
* Bug fixes as follows:
  -- In "kdu_serve.cpp", used by the "kdu_server" JPIP server application,
     a minor bug in the computation of bytes associated with previously
     served packets was corrected.  Specifically, in function
     `kd_serve::simulate_packet_generation', the line
       "for (cum_packets=1; cum_packets < tp->num_layers; cum_packets++)"
     is changed to
       "for (cum_packets=1; cum_packets <= tp->num_layers; cum_packets++)"
  -- In "kdu_hyperdoc", an earlier fix for a bug in the source parsing
     code accidentally caused bare functions (i.e., functions without
     classes) to be omitted from the automatically generated documentation
     and in the construction of Java native interfaces.  This has now
     been created.
  -- In "kdu_region_decompressor" in function `interpolate_and_convert',
     an unlikely condition which could cause the final column
     of a rendered region to be unwritten, in the event that the fast
     SIMD speedups are used, has been corrected.
  -- Fixed a bug in the "kdu_server" application's delegation feature,
     which has been present for some time but manifested itself only
     when the delegated server has a different IP address to the delegating
     server (as opposed to just a different port number).  The cause of the
     problem was a single typo, where the delegating server used syntax
     "hostname=" instead of the correct JPIP syntax "host=" which is
     expected by the client.
  -- Fixed a bug in the core coding parameter management system, which
     has been in existence since version 4.2, where sparse parameter
     instantiation on the tile-component grid was introduced in order to
     dramatically cut the cost of parameter management for heavily tiled
     (or componented) images.  Unfortunately, the instantiation logic did
     not pick up on the fact that separate instances of the quantization
     parameter object `qcd_params' need to be instantiated for image
     components whose precision, as recorded in the SIZ marker segment,
     differs from that of the first component, when reversible compression
     is involved, due to dependencies between precision and reversible
     dynamic range parameters.  The problem was very easy to fix by
     slightly modifying `qcd_params::finalize'.

Changes from version 5.1 to 5.1.1
---------------------------------
* Minor features added as follows:
  -- Upgraded the `jp2_colour_converter' object to allow for the conversion
     of 4 colour spaces to RGB, which means that "kdu_show" and other
     tools now automatically handle rendering of CMYK to RGB.
  -- Significantly modified the TIFF reading and writing code used by the
     demo applications kdu_compress and kdu_expand, so as to correctly handle
     a much wider range of TIFF files.  The code now handles both tiled and
     untiled images, with both planar and contiguous pixel organizations.
     In addition, a bug in the palette handling was removed -- this but
     was accidentally introduced in v5.1 when generic baseline TIFF handling
     was provided through native Kakadu tools.  TIFF reading and writing is
     still not really considered part of the Kakadu toolset, since it is
     used only for demonstration purposes; however, a lot of people have
     asked specifically for this, so they can more conveniently use the
     demo apps.
* Bug fixes as follows:
  -- The "kdu_tiff" object introduced in version 5.1 contained a
     subtle yet important bug when handling tags whose value is
     rational, where the file byte order differs from that of the
     native machine.  The fix involved restricting the swapping
     of 4-byte words to double precision floating point data types,
     rather than all 8-byte data types, as suggested by Michael
     Wildermoth.
  -- Two fixes to the raw file reading code in "kdu_compress", both
     suggested by Margaret Lepley.
  -- Removed the second writing of TIFF tag RowsPerStrip when
     generating GeoJP2 boxes in "image_in.cpp", which was a typo.
     This fix was provided by Greg Coats.
  -- Removed a typo in the call to `_addr_to_kdu_int32' in
     `simd_upshifted_interleave' within "gcc_dwt_altivec_local.h".
  -- Fixed a memory leak in `jpx_codestream_source::access_dimensions'
     where the `kdu_codestream' object which was created by this function
     in order to completely finalize compatibility information, was not being
     destroyed.
  -- Fixed a foolish error in `kd_serve::process_window_changes' where
     a `kdu_window' object was directly assigned to another rather than
     using the `kdu_window::copy_from' function.  This caused `kdu_server'
     to crash on connection closure, which is a behaviour that should have
     been caught prior to release of v5.1 were it not for the fact that the
     bug was introduced immediately prior to release, as a fix for another
     much more subtle error.
  -- Modified the colour space interpretation code in "kdu_compress" so
     as to allow proper representation of 4-colour spaces such as CMYK --
     of course, this is just a demo, but it's nice for the demo to be general.

Changes from version 5.0 to 5.1
-------------------------------
* The main new feature in Kakadu v5.1 is the inclusion of extensive
  multi-threading facilities to exploit the computing resources
  available on multi-processor, multi-core and hyperthreading platforms.
  This is achieved through two new core system objects, `kdu_thread_entity'
  and `kdu_thread_env'; the latter is derived from the former.  You can
  completely ignore these objects, compiling your applications exactly
  as before, in which case only one thread will be involved in processing.
  Alternatively, the simplest way to reep advantages on multi-threaded
  platforms, is to create a `kdu_thread_env' object and pass it (as an
  optional argument) to the objects you are using to perform your
  processing.  All of the high level sample processing objects
  (`kdu_multi_analysis', `kdu_multi_synthesis', `kdu_stripe_compressor',
  `kdu_stripe_decompressor', `kdu_region_decompressor' and
  `kdu_region_compositor') provide simple mechanisms to include the
  processing resources offered by a `kdu_thread_env' object you have
  created.  You control the number of working threads by using the
  `kdu_thread_entity::add_threads' function.  In most cases, you should
  arrange for the total number of working threads to be equal to the
  number of distinct real or virtual processors -- a value which Kakadu
  attempts to find for you via its `kdu_get_num_processors' function.
     Of course, there are lots of lower level ways to use the multi-threading
  framework, but the high level objects will be simplest to understand
  and require only a few extra lines of code.  To learn more about how
  to program with Kakadu's multi-threading environment, consult the
  demo applications -- "kdu_render", "kdu_compress", "kdu_expand",
  "kdu_buffered_compress", "kdu_buffered_expand", "kdu_v_compress",
  "kdu_v_expand" and "kdu_show" all provide facilities to use (or not use)
  the multi-threading environment and to control the number of threads used
  (typically a `-num_threads' argument).
     There is a lot of scope to do more with multi-threading than the demo
  applications can illustrate.  For example, the "kdu_v_expand" and "kdu_show"
  applications process each frame in a video sequence completely
  (synchronizing on the completion of all threads) before moving on to a
  new one.  This incurs some start-up and close-down costs where only one
  thread can execute.  It is possible, however, to arrange for overlapped
  processing of multiple frames.  This may be demonstrated explicitly
  in future versions of Kakadu.  Depending on the number of processors
  available on your platform, you may also find it useful to play around
  with double buffering options -- see, for example, the `-double_buffering'
  options to "kdu_compress" and "kdu_expand", which are used to enable and
  tweek double buffering features offered by `kdu_multi_analysis' and
  `kdu_multi_synthesis' to parallelize the processing of multiple
  tile-components.
     Use of the multi-processing framework with multiple threads incurs a
  memory overhead for the double buffering of code-block samples, since
  block encoding and decoding operations provide the greatest
  opportunity for parallelism in JPEG2000.  The extra memory is allocated
  automatically, using an algorithm which backs away from full double
  buffering as the image dimensions become very large.  For very large
  single-tiled images, the memory penalty associated with multi-threaded
  processing reaches around 30%.  If you are interested in playing with
  the internal algorithm which controls these costs, take a look at the
  `kd_encoder::init' and `kd_decoder::init' functions -- in particular,
  you may adjust the way in which the `num_jobs_per_row' and
  `buffer_height' member variables are initialized for each subband's
  `kdu_encoder' or `kdu_decoder' object.
     During compression, it is possible to overlap incremental codestream
  generation with ongoing processing, when using Kakadu's incremental
  flushing features.  This is demonstrated in "kdu_compress" and is also
  implemented for you within `kdu_stipe_compressor'.  It is achieved by
  defining so-called "synchronized jobs", which are scheduled at the first
  convenient opportunity after a synchronization condition is reached
  (typically, the processing of all samples pushed into the DWT engines
  so far).  The positioning of synchronized flushing jobs for maximum
  parallelism is a little tricky, so it is best either to use
  `kdu_stripe_compressor' or to read the explanation appearing within the
  "kdu_compress" function `compress_multi_threaded'.
     One nice feature of Kakadu's multi-threading environment is that you
  can compile and test applications based on the `kdu_thread_env' and
  `kdu_thread_entity' objects even if your platform does not support
  multi-threading.  To compile without actual multi-threading support,
  define the `KDU_NO_THREADS' symbol.  All that will happen in this case
  is that attempts to add additional threads to a `kdu_thread_env'
  (equiv. `kdu_thread_entity') object which you create will fail, leaving
  you with only one thread of execution.  This single thread (the one
  your program started in) will then be scheduled onto the various tasks
  automatically, as required.  The same thing happens if you never add
  extra threads to a `kdu_thread_env' object before using it.
    For true multi-threading, the Kakadu implementation supports both
  POSIX threads (Pthreads) and Windows threads.  If neither is available
  on your platform, you should define the `KDU_NO_THREADS' symbol, as
  mentioned above.

* Added new `push_stripe' and `pull_stripe' interface functions to the
  `kdu_stripe_compressor' and `kdu_stripe_decompressor' objects, respectively,
  to support 32-bit integer and floating point image sample values, in
  addition to the existing 8- and 16-bit precision interfaces.  These
  additional interfaces ensure that the high level stripe-oriented
  objects support all of the data precisions that are supported by the
  underlying core Kakadu system.

* Added the Digital Cinema profiles (CINEMA2K and CINEMA4K) to the list
  of profiles recognized in the codestream SIZ marker segment.  These
  profiles were added about a year ago as an ammendment to Part 1 of
  the JPEG2000 standard.

* Added significant support for TIFF and GeoTIFF image I/O to "kdu_compress"
  and "kdu_expand".  Even though image I/O is not really part of the scope
  of Kakadu, it is required for demonstration purposes and lack of
  comprehensive TIFF support has been a source of concern for some new users.
     This has been complicated by the fact that GeoJP2 files include a JP2
  box which is really an encapsulated TIFF file.  To solve both problems in
  one go, this new release of Kakadu comes with a simple yet general native
  TIFF parser/creater, which is not based on any external libraries and
  integrates well with the other abstract I/O services endemic to Kakadu.
  As a result, TIFF files can now be read and written by the "kdu_compress"
  and "kdu_exand" demo applications without the need to link against LIBTIFF
  (as before).  GeoTIFF tags are also extracted by "kdu_compress" and used
  to create a GeoJP2 box if appropriate.  GeoJP2 boxes may be unpacked (if
  required) and/or written back to GeoTIFF files by "kdu_expand".
     The native TIFF managing code is small and does not explicitly manipulate
  imagery; it only manages TIFF directories -- see "kdu_tiff.h" or lookup
  the hyperdoc documentation for `kdu_tiffdir'.  Kakadu is actually quite
  agnostic about the GeoJP2 format, but since a GeoJP2 box is just a UUID
  box which reads like a TIFF file, any such box can be supplied directly
  to `kdu_tiffdir::opendir', giving you read/write/modify access to its
  embedded tags -- double-precision world coordinates and such.
     The demo code in "kdu_compress" and "kdu_expand" only reads and writes
  uncompressed TIFF files -- although it handles arbitrary bit-depths from 1
  to 32 bits/sample and both signed and unsigned sample formats.  If you
  need to read compressed TIFF files, this is still possible by defining
  `KDU_INCLUDE_TIFF' and linking against the public domain LIBTIFF library.
  If you do this, the services of LIBTIFF are used only for decompressing
  source TIFF files (and then only when the file indicates that it uses a
  compressed sample format) -- all other interaction with the TIFF directory
  is managed natively.

* "kdu_hyperdoc" now generates a much more extensive set of Java
  interfaces to Kakadu functions.  In particular, most combinations of
  default arguments in the C++ interface functions are now converted
  into distinct Java bindings, for programming convenience.  Also,
  functions which accept or return opaque pointers (i.e., pointers to
  objects whose definitions are not publically visible) are now
  mapped to Java bindings, with the pointers represented as Java "long"
  arguments.  There are some important high level interface objects,
  such as "jpx_frame_expander" which manipulate such opaque pointers and
  so were not previously available in Java.

* Quite a number of minor bugs have been fixed (should include all bugs
  reported to date).  These include:
  -- Fixed a minor bug in the parsing of open-ended ranges (e.g., unbounded
     codestream indices) in JPIP communications.
  -- Fixed a minor syntax error in the parsing of the "metadata-only"
     qualifier for JPIP requests -- this qualifier consists of a "!!" found at
     the end of "metareq" request field, where kakadu previously expected
     ",!!".
  -- Made some minor improvements to the distortion-rate slope prediction
     algorithm at the heart of the EBCOT block encoding machinery in the
     core system; this algorithm is used to prematurely truncate the
     encoding process where a minimum slope or an overall maximum length
     threshold has been provided to the codestream management machinery.
     The previous implementation very occasionally truncate code-blocks
     much too early.  The new implementation is much less likely to do
     this, but sacrifices nothing in speed.
  -- Fixed a minor bug in the "kdu_hyperdoc" utility which caused it to
     append "[SYNOPSIS]" style comments found against protected or private
     class members to the descriptions of preceding public class members.
  -- Fixed a minor race condition in the "kdu_client" object, which could
     manifest itself when both server and client try to close a connection
     in very close proximity.
  -- Fixed a minor bug in the order in which Kakadu's JPIP server component
     processed cache model manipulation statements from the client.  This
     caused model addition statements to be discarded if they were
     delivered on a first request which accessed a particular codestream.
  -- Fixed a minor oversight in "kdu_compress" which prevented
     fragmented compression from working correctly with JPX files -- the
     problem was that the contiguous codestream box was not being placed
     into the "rubber length" mode when writing JPX files (any application
     can easily select this mode), so that extra fragments which appended
     this box were not included in the indicated box length.
  -- Fixed a minor bug accidentally introduced into the Altivec (PowerPC)
     speedup code in version 5.0.  The bug involved 64-bit pointers being
     cast to 32-bit integers for parity checking, which fails to achieve
     the desired result in the big-endian PowerPC architecture.  The bug
     only manifested itself near the left edge of some regions when using
     Kakadu's region-based decompression features.

Changes from version 4.5.2 to 5.0
---------------------------------
* Four new dead-easy demo code fragments implemented inside the
  "kdu_render" demo app, to get you up and running as quickly as
  possible.  The most sophisticated of these (embodied in function
  `render_demo_4') renders any raw codestream, JP2 file, JPX file,
  JPX animation frame or MJ2 file to a memory buffer, performing
  colour space conversions as required, inverting multi-component
  transforms as required, etc., etc. -- it is less than 50 lines long,
  but if you like one-liners, just consider a call to `render_demo_4'
  as your one line solution.
* Windows builders note that some applications compiled against the
  core system DLL may need to define the symbol "CORESYS_IMPORTS".  It
  is good practice to do this universally in all your applications which
  import the Kakadu core system DLL.
* Windows builders using Microsoft VC6 need to ensure that they have the
  processor pack installed (see "Compilation_Instructions.txt").
* Added full support for Part-2 arbitrary transform kernels
* Added full support for Part-2 arbitrary decomposition styles, including
  wavelet packet decomposition structures and unbalanced sub-sampling
  structures in which successive resolution levels might have identical
  horizontal dimensions or identical vertical dimensions.
* Added full support for Part-2 multi-component transforms, including
  all possible transform block types (reversible/irreversible decorrelation,
  dependency and DWT transform blocks with arbitrary kernels).  Also
  ensured that all other elements from rendering to interactive distribution
  with JPIP work correctly when multi-component transforms are employed.
  Note that JPIP syntax only allows requests for multi-transformed components
  to be signalled when they are wrapped as JPX compositing layers -- use
  the `-jpx_layers' argument to "kdu_compress", together with "-jp2_space sLUM"
  if you want to create one compositing layer for each multi-transformed
  image component (say in a medical volume) and then interact with it using
  JPIP.  See "Usage_Examples.txt" for more ideas or read the updated overview
  document, "kakadu.pdf" -- see especially the new Section 4 in this updated
  document.
* Modified codestream parameter sub-system to use correct Part-2 codestream
  syntax for codestreams which use the alternate code-block alignment
  required for compressed-domain rotation and flipping (as performed by
  "kdu_transcode" for example) -- previous versions included this only as
  an experimental feature, without correct marker segment syntax.
* Improvements to platform-dependent speedups allow throughput increases of
  around 20% at low to moderate bit-rates on Pentium-4 platforms, while the
  Altivec speedups for the Power PC now function correctly under all
  conditions, including regoin-of-interest decompression -- previous versions
  had an alignment-induced problem under some conditions, but the new version
  aligns all critical accesses on 16-byte boundaries, on all platforms.
  This will also improve cache performance when multi-processor speedups are
  introduced in a future release.
* Rendering speed at low to moderate bit-rates via `kdu_region_compositor' or
  `kdu_region_decompressor' has increased by 40% to 50% on Pentium 4 platforms,
  due to the selective inclusion of SSE speedups in the decompressed data
  processing path -- smaller speedups should already be observed on other
  platforms, but it should be very easy indeed for interested persons to port
  the small Pentium 4 speedup routines in "msvc_region_decompressor_local.h"
  and "msvc_region_compositor_local.h" to other platforms (e.g. the Power PC),
  following the general approach taken in the core system (see gcc
  platform-specifics in "coresys/transforms" for examples).
* Fixed a bug in the fragmented codestream compression feature, whereby
  the quality layer target lengths supplied to `kdu_codestream::flush'
  were incorrectly scaled when multiple fragments were used.  Some
  licensees chose to avoid this problem by using slope thresholds to control
  quality layer sizes, which is probably the best strategy for fragmented
  codestream compression.  Others pre-scaled their target layer lengths to
  compensate for the internal bug.  This latter group will now need to
  remove this pre-scaling, since the internal problem has been corrected.
* Introduced a heuristic adjustment to the efficient rate control strategy
  implemented by the "kdu_v_compress" demo application so as to prevent
  occasional hicups reported by others.
* Added the capability for "kdu_compress" to read raw files in little-endian
  word order (just use files whose suffix is ".rawl" rather than ".raw").
  Also added the ability to read raw files which contain multiple
  concatenated image components (for simplicity when compressing image
  volumes).
* Added a `-codestream_components' option to "kdu_expand", so that users
  can specify whether they want to produce only the codestream components
  (these are the components produced after block decoding and inverse
  wavelet transformation) or the complete output components (these are
  the components produced after any specified colour or multi-component
  transformations have been performed).  If you do not specify this option,
  you will get output components.
* Incorporated 3'rd party fixes to the "kdu_hyperdoc" utility to generate
  Java (JNI) interfaces which avoid a rare race condition previously
  encountered in multi-threaded Java apps.

Some existing applications may require the following changes in order
to fully support Part-2 codestreams.
1) In order to ensure correct generation of JPX image header boxes
   when Part-2 codestream features may be used, your application must now
   be sure to invoke the `jp2_dimensions::finalize_compatibility'
   function, after finalizing all `kdu_params' objects and before invoking
   `jpx_target::write_headers'.  This is a new requirement.  Previously, it
   was sufficient to just call `jp2_dimensions::init' any time after
   the `siz_params' data was available.  While not strictly necessary, it
   is a good idea also to call `jp2_dimensions::finalize_compatibility'
   right before generating a regular JP2 file's header (via
   `jp2_target::write_header') so that the logic can verify that the
   codestream being embedded in the JP2 file is indeed a Part-1 codestream,
   since Part-2 codestreams must be embedded in JPX, rather than JP2 files.
2) If your application uses `jp2_dimensions::copy' to copy imagery from
   an existing JPX file to a new one, you should ideally first
   invoke the `jp2_dimensions::finalize_compatibility' function on the
   source object.  To facilitate this process, the
   `jpx_codestream_source::access_dimensions' function now takes an optional
   `finalize_compatibility' argument, which you should set to true when
   accessing the source `jp2_dimensions' interface which you intend to supply
   to `jp2_dimensions::copy'.  Except where such copying is required, it
   is more efficient to leave the `finalize_compatibility' argument equal to
   its default value of false.
3) If your application needs to directly access codestream subbands, you
   should use the new `kdu_resolution::get_valid_band_indices' function
   to obtain the range of subband indices which are valid for any given
   resolution level.  In previous versions, the application could assume
   that resolution level 0 had only the subband with index `LL_BAND' while
   all other resolution levels had subbands with indices `HL_BAND',
   `LH_BAND' and `HH_BAND'.  This is still true for Part-1 codestreams, but
   you need to be more careful with Part-2 codestreams.  Very few applications
   should need to directly access subbands.  The one notable exception is
   transcoding applications, which typically copy/transcode the relevant
   code-blocks one-by-one, accessing them via their containing subband
   interfaces.
4) If you want your application to work correctly on images which have
   been compressed using JPEG2000 Part-2 multi-component transforms, some
   minor changes may be required as follows.  I believe that these represent
   everything you need to consider.  Also, none of these changes
   are required if you don't need to decompress images which use Part-2
   multi-component transforms, and many applications might not require any
   changes at all.
   a) If you are using `kdu_stripe_decompressor', `kdu_region_decompressor'
      or `kdu_region_compositor' you don't need to do anything differently;
      however, if you are creating `kdu_pull_ifc' derived objects
      (`kdu_decoder' or `kdu_synthesis') directly to construct decompression
      engines, you will only end up decompressing the raw codestream
      components.  To migrate to a decompression engine which also correctly
      inverts the multi-component transform is actually very easy.
      Essentially, you just need to replace all of your independent
      component processing engines with a single `kdu_multi_synthesis'
      object, which does the whole thing -- then you no longer need to look
      out for whether a YCC colour transform needs inverting either.  If in
      doubt, take a look at how the compression engine creation and usage
      code has been changed in the "kdu_expand" demo app.
   b) For completely general decompression, you need to bear in mind that
      there can be a difference between output image components (produced
      after any multi-component transform is inverted) and codestream
      components (the ones you may have previously been associating directly
      with decompression engines).  The difference may involve differences
      in order (and hence possibly dimensions) and number (there can be
      more or less multi-component transform output components than raw
      codestream components, in the general case).  For backward compatibility,
      Kakadu's interface functions which return information related to
      components or indexed by component indices actually return information
      about the raw codestream components by default.  However, all of these
      functions now contain an optional final argument (`want_output_comps')
      which you should set to true if your application wants information
      about final output components (more likely than not this is the case).
   c) If your application directly invokes
      `kdu_codestream::apply_input_restrictions' you need to know that there
      are now two versions of this function.  If you call the original
      version in the way you did before, it will make multi-component
      transforms appear not to be present (this is required for backward
      compatibility with applications which might have been directly
      creating low level decompression engines).  To avoid this, set the
      optional final `component_access_mode' argument to
      `KDU_WANT_OUTPUT_COMPONENTS', or consider using the second, much
      more flexible version of the function which now allows you to
      restrict your region of interest to any arbitrary set of image
      components, optionally including permutations.

Changes from version 4.5.1 to 4.5.2
-----------------------------------
This is just a bug fix version.
* Fixed a minor bug in "kdu_transcode" which arises when extracting
individual image components.
* Fixed a minor bug in `kd_message_block::peek_block' which caused
problems when the Kakadu client is communicating with a server via
a proxy which rechunks the HTTP response.
* Fixed a couple of accidentally introduced bugs in `kdu_region_compositor'
and the way it is used by "kdu_show", which caused the objects to be
used in a manner which was much less responsive to interactive control
than it had been in versions prior to v4_5.
* Made a minor change to `CKdu_showApp::save_as_jpx' to ensure that
large files could be re-saved without unnecessary intermediate buffering
of the embedded codestream box.

Changes from version 4.5 to 4.5.1
---------------------------------
NB: This version exists only to correct a few of minor issues
discovered immediately after the release of Version 4.5.

* Minor bug fix in `kdu_region_compositor' to avoid a problem
which can cause the number of available rendering resolutions to
be reduced by 1 (bug was accidentally introduced in v4.4 with
the move to arbitrary rational composition scaling factors).
* Minor correction to `kdu_region_decompressor' to ensure that
high bit-depth imagery which requires colour conversion will
be handled correctly, even if not with maximum accuracy.
* Added the `JPX_SF_sYCC' feature flag to "jpx.h" in accordance
with a recent ammendment to the IS 15444-2, which describes
the JPX file format.  This should not impact any existing
applications.

Changes from version 4.4 to 4.5
-------------------------------
* Introduced all the support required to internationalize and/or
  customize all error/warning messages produced by the Kakadu
  system or derivative applications.  This is done in an almost
  seamless manner, which remains backward compatible with previous
  uses of the "kdu_error" and "kdu_warning" services and which
  is entirely platform independent.  Original error/warning text
  remains in the source files where it is easiest to follow and
  edit.  However, new constructors are provided for "kdu_error"
  and "kdu_warning" which allow text to be registered with unique
  identifiers.  This is done using macros, so that there is
  very little editing of the existing error/warning calls.  The
  same macros are used by a new tool, "kdu_text_extractor", to
  collect all the registerable text into separate language files.
  You can create as many versions of these as you like, translating
  text into new languages (and even using unicode for languages
  with large alphabets).  To use any of these external language
  files, you compile the original code (core system, applications,
  etc.) with the `KDU_CUSTOM_TEXT' macro defined.  You then simply
  include the language files of interest into your end application
  and the translation process is complete.  If you like, you can
  construct separate language-specific DLL's (Windows) or
  shared librares (Unix) containing the language files.  Your
  application then just needs to load the language DLL or shared
  library of interest at run time.
    By default, `KDU_CUSTOM_TEXT' is not defined, and everything
  behaves exactly as it did in previous versions of Kakadu.  In
  this case, there is no need (indeed no point) including the
  language files.
    For more information, see the "Compilation_Instructions.txt" file.
* Extended `kdu_region_compositor' to support Motion JPEG2000 data sources,
  in addition to JP2/JPX files and raw codestreams.  The new support includes
  the ability to composit and individually reorient tracks in accordance
  with the movie specifications found in the Motion JPEG2000 source.  Some
  of these new features are demonstrated by new features in "kdu_show".
* Extended "kdu_show" to handle Motion JPEG2000 files, in addition to its
  current input formats.  Also added playback control which operates in the
  same way for Motion JPEG2000 movies and JPX animations.
* Updated the `mj2_source' and `mj2_target' objects to bring them into
  compliance with aspects of the Motion JPEG2000 standard which were
  changed/clarified by a corrigendum.  The interface functions offered by
  `mj2_source' now offer the support required for handling data sources
  which are fed by an asynchronous dynamic cache (for JPIP browsing
  applications); however, the internal implementation does not yet support
  caching sources.  This can wait until we have a full JPIP implementation
  for video browsing.
* Extended the "kdu_merge" demo application, to support writing of both
  JPX and MJ2 files, based on input from one or more JP2/JPX/MJ2 files.
  The new features allow you to create MJ2 files from a sequence of JP2
  files, from the compositing layers in a sequence of JPX files, and/or
  from a sequence of existing MJ2 tracks.  You can write multi-track
  MJ2 files and you can even merge fields together to form interlaced
  MJ2 tracks -- however, note that "kdu_show" will currently only play
  the first field (correctly scaled) in each frame of an interlaced track.
     These services, while simple, close an important hole in
  the demo applications relating to Motion JPEG2000.  Many people had
  difficulty using the "kdu_v_compress" and "kdu_v_expand" tools in the
  past because they were not sure how to create "vix" files.  Now you
  can create MJ2 files from JP2 files and use them to write sample VIX
  files to play around with -- VIX is a mindbogglingly simple format though.
* Fixed a minor bug in `kdu_region_decompressor' which could affect the
  image produced when using rational expansion factors.  Also, replaced
  nearest neighbour interpolation with an efficient bilinear interpolation
  strategy for improved visual performance when non-integer rational
  expansion factors are used.
* Fixed a minor bug which was introduced into `kdu_region_compositor' in
  version 4.4 -- the bug could generate an assertion failure when zooming
  into rotated compositions.
* Fixed two minor bugs in the core system which affect transcoding.  You
  can now use incremental codestream output with `kdu_codestream::trans_out'
  for transcoding applications (lots of memory saving potential) -- a simple
  demonstration of this is included with the "kdu_transcode" application.
* Fixed a minor bug in the core system which has always been in Kakadu --
  it had the potential to cause an assertion failure (or memory overwrite)
  when re-opening a precinct with an increased quality layer threshold,
  but only in persistent mode where the codestream contains no PLT
  information, and then only with certain uncommon combinations of
  parse/read sequences.
* Removed the call to `kdu_error' within `kd_encoder::~kd_encoder' if not
  all lines were pushed into the compression engine.  This allows you
  to abort compression processing without errors or memory leaks.

Changes from version 4.3 to 4.4
-------------------------------
* Support for arbitrary scaling factors has been introduced into
  `kdu_region_decompressor' and `kdu_region_compositor'.  This means
  that you are no longer restricted to integer expansion factors and
  power-of-2 decimation.  It also means that correct interpolation
  can be applied prior to colour transformation or alpha blending, in
  the case of uncommon chrominance subsampling arrangements or low
  resolution alpha specification.  Most importantly, however, it means
  that `kdu_region_compositor' can correctly composit multiple
  codestreams onto a JPX compositing surface, taking into account their
  respective scaling requirements to produce correct alignment under
  all circumstances.
* Added capabilities to derive progress information in JPIP
  interactive browsing applications, based on the number of quality
  layers which are available at any given time for the codestream
  precincts required to reconstruct a region of interest.  This support
  is introduced by the low level functions `kdu_resolution::get_precinct_area'
  and `kdu_resolution::get_precinct_packets', and the much higher level
  function, `kdu_region_compositor::get_codestream_packets'.  The
  new functionality is explicitly demonstrated by the provision of a
  progress status indicator for "kdu_show", which shows up when the status
  bar is toggled into the download statistics mode during JPIP remote
  image browsing.
* Implemented all the machinery required to create and handle fragmented
  and externally referenced codestreams in a JPX file.  This is demonstrated
  by the provision of a "-links" argument to "kdu_merge" which can be
  used to create a JPX file whose codestreams reside in other files (e.g.
  a photo album whose contents reside elsewhere).  All
  demonstration applications correctly handle such JPX files, including
  "kdu_show" and "kdu_server".  When JPX files with external links are
  served using JPIP, streaming equivalents are used to make them appear
  local.
* Moved the MMX optimizations for Linux builds from assembler source files
  (*.s) to gcc inline assembly code.  This should have no impact on
  performance, but avoids the various problems which have been encountered
  in the past when compiling with different versions of GAS and GCC.
  Compilation for Linux should now be a hassle-free process.
* Added "-log" and "-wd" arguments to "kdu_server", following the suggestion
  of Michael Owen, so that it can be invoked as a registered Windows service.
* Some minor adjustments have been made to the definitions in
  "kdu_elementary.h" to facilitate compilation under 64-bit Solaris
  environments; also, revised makefiles for the sunpro compiler have
  been contributed by Margaret Lepley.
* Fixed a number of bugs accidentally introduced by the more extensive
  changes in v4.3.  Notable among these is a bug which manifests itself
  if `kdu_codestream::augment_buffering' is used.
* Fixed a number of bugs connected with more exotic colour rendering.
* No known bugs remain.

Changes from version 4.2.1 to version 4.3
-----------------------------------------
* Significant improvements in dynamic memory management for tiled images
  -- Previous versions of Kakadu provided for dynamic unloading of
     unused precincts, subject to the provision of appropriate pointer
     information in the code-stream.  This functionality has now been
     extended to the automatic unloading (and on-demand reloading) of unopen
     tiles from heavily tiled images.  While this is most efficient if the
     code-stream contains TLM marker segments to point to the tiles from
     its main header, the code-stream machinery now builds the TLM
     information on the fly if necessary, while parsing through seekable
     compressed data sources.
     + See `kdu_codestream::augment_cache_threshold' to understand how
       dynamic tile unloading/reloading has been integrated into the
       existing automatic memory management features -- by and large,
       applications should be able to get the benefits of these features
       without any implementation changes, remaining blissfully unaware
       of the numerous code-stream structures which could be presented
       to the internal machinery.
     + The new function `kdu_codestream::set_tile_unloading_threshold'
       gives you additional control over dynamic tile unloading, if you
       should require it.
     + The `kdu_codestream::get_compressed_state_memory' function has
       been slightly redefined to take advantage of more comprehensive
       memory accounting by Kakadu's code-stream memory management
       system.  In particular, the cost of tile, tile-component,
       resolution, subband and precinct address structures are now
       included along with the cost of precinct and code-block structures
       which were previously reported by this function.  By and large,
       applications which were using this function for gathering memory
       statistics, can continue to work as before, except that the statistics
       will be more comprehensive.

* Improved the efficiency with which finely tiled images are compressed
  and manipulated at very low resolutions, where there can be an enormous
  number of tiles.  This is achieved by maintaining a cache of "typical"
  tile objects within the code-stream management machinery, rather than
  instantiating a distinct object each time a tile is opened.  This
  functionality leverages off the changes first introduced in v4.2,
  wherein the coding parameters of finely tiled images are represented
  very efficiently wherever possible.

* The core codestream generation machinery can now generate TLM marker
  segments itself, rather than having to defer this process to a postprocessing
  phase using the "kdu_maketlm" program.

* The core codestream generation machinery can now compress tiled images
  in fragments, where each fragment is compressed independently and can
  consist of any subset of the tiles in the full codestream.  Fragments
  are automatically stitched together and a correct set of TLM marker
  segments can be generated for the full set of fragments, by rewriting
  selected segments of the main header.  These capabilities extend
  Kakadu's ability to compress large images at least into the tens of
  tera-bytes, and are demonstrated with the aid of the new `-frag'
  argument to the "kdu_compress" demo utility (see "Usage_Examples.txt").

* The `kdu_region_decompressor' object now offers even more flexibility
  in the way its `process' functions set the dynamic range and signed/unsigned
  attributes of the samples which they produce.  You can supply a
  `precision_bits' argument of 0 to get the `process' function of interest
  to derive its precision and signed/unsigned decisions from the original
  image sample attributes recorded in the codestream and/or file-format
  headers as appropriate.  The `kdu_channel_mapping' object's configuration
  state is expanded to handle this information and also allow
  application-specific overrides for each individual logical rendering
  channel.  These changes seem to address the concerns of a wide range
  of applications, as expressed on the Kakadu mail reflector.

* The JPX file format reader can now read JPX files which were mistakenly
  written without the reader requirements box by Algovision -- JPX
  files without any reader requirements box were technically illegal, but
  the reader requirements box is so poorly defined as to be next to
  useless to practical applications, so it is better not to require its
  existence, rather than fail to read existing non-compliant files.

* The JPIP client/server implementation now allow a TCP transport
  channel to be preserved between JPIP sessions, so you can start
  and close multiple JPIP sessions without having to close and
  re-open the TCP link (good for apps which need to make lots of
  JPIP requests without looking like they are trying a denial of
  service attack).  To do this, you must pass a non-default value
  for the `keep_transport_open' argument to `kdu_client::disconnect'.
  The operation should succeed in preserving the channel until the
  next call to `connect' so long as you are using the HTTP transport
  method and the connection was idle when you called `disconnect'.
  You can test the functionality out in "kdu_show" by explicitly
  clicking File->Disconnect, after you see the idle status appear in
  the statusbar.  The next connection attempt will try to use the
  same TCP channel; if the server has dropped the channel already
  (e.g., due to timing out on waiting for more communication on it --
   "kdu_server" uses a default timeout of 5 seconds), a new one
  will be opened.  Note, however, that in this mode there is no
  checking to see if the new channel belongs to the right server, so
  you may get an error message back if you are really trying to
  request a resource from a different server.  The application is
  supposed to avoid chaining requests which belong to different
  servers on the same TCP channel.

* Fixed a number of bugs, including
  -- one subtle bug in "kdu_server" (provided by a licensee), which could
     cause assertion failure under some circumstances when the same image
     is being accessed from multiple threads;
  -- several other rare bugs in the synchronization logic used for
     the producer-consumer threads in "kdu_server" -- these fixes may
     have resolved previously reported occasional server hangups;
  -- a number of inter-related bugs affecting complex colour conversion
     processes in the `jp2_colour_converter' object -- in particular, Lab
     colour space conversion now works correctly.
  -- a bug introduced accidentally in v4.2 which prevented RGN marker
     segments from being written out during code-stream generation.  This
     bug was responsible for problems encountered in the generation
     of images with explicit ROI features.


Changes from version 4.2 to version 4.2.1
-----------------------------------------
* This is a bug-fix release.  Below is a summary of some of the bugs fixed:
  -- A bug in the SPARC optimizations (VIS instruction set) which occurred
     when decompressing regions of interest.
  -- Various bugs in the "kdu_server" utility.  Most of these manifested
     themselves only when serving certain types of image files; however,
     I believe I have fixed a long standing bug which would cause the
     server to sometimes experience a memory fault after running for a
     month or so.
  -- A minor bug in the "kdu_show" application which manifested itself
     sometimes while closing and re-opening JPIP sessions.
  -- Various problems with the TIFF reading code originally contributed by
     a third party to the "kdu_compress" demo app -- substantially rewritten.
  -- Some bugs in the `kdu_stripe_decompressor' object and associated
     demo apps, which caused some types of data precision conversions to
     be performed incorrectly.

Changes from version 4.1 to version 4.2
---------------------------------------
* JPIP Client-Server extended to fully support JPX
  -- the client-server implementation has been updated to support the new
     "codestream-context" request field defined by JPIP.
  -- using codestream contexts and context translation, the client-server
     implementation now allows for highly efficient browsing of complex
     JPX images, containing multiple compositing layers, with different
     scale factors, cropping, and placement.  Also supportes efficient
     browsing of remote images which use multiple codestreams, potentially
     at different scales, to construct each image layer.
  -- the client-server implementation now fully supports the interactive
     transfer of metadata, including textual labels, XML, etc.  The
     server automatically figures out what metadata is relevant to any
     given requested image region.  Moreover, it is able to do this in
     the case of complex images, having multiple codestreams at different
     scales, cropping, and placement, each with its own codestream-registered
     region specific metadata.  The bare minimum amount of metadata is
     delivered to satisfy any particular request.  Moreover, all
     metadata-specific JPIP requests ("metareq" request field) are
     properly honoured and scoped.
* Dramatic speedups for heavily tiled images
  -- many thanks to Margaret Lepley for pinpointing the source of serious
     slow downs which have been observed when compressing/decompressing
     images with a very large number of tiles.  The problem was in the
     linear list searching performed in the codestream parameter sub-system
     ('kdu_params' and its derived objects).  This sub-system was actually
     the very first code implemented when Kakadu was originally developed.
     The internal implementation of "kdu_params" has been significantly
     changed to dramatically reduce access times (by up to 3 or 4 orders
     of magnitude) and memory consumption, while leaving the interface
     functions almost entirely unchanged.  No changes should be required
     by any existing application-level code, although some applications
     which use the binary methods to access parameters could stand to
     save memory now by supplying a value of "true" for the new "read_only"
     argument to `kdu_params::access_relation'.  See the interface
     documentation for this function if you wish to take advantage of this.
* SIMD platform-specific speedups to the wavelet and colour transform code
  are now provided for the UltraSparc (Sparc VIS instruction set) and the
  PowerPC G4 (Altivec vector processing instruction set), to complement
  the existing SIMD speedups provided for Intel processors (MMX instruction
  set).  Many thanks to Monroe Williams for contributing the Altivec
  code, while working on Linden Lab's "Second Life" project.
* To further demonstrate the sophisticated JPX features, the "kdu_merge"
  application has been extended to include:
  -- a "photo album builder" function, whereby an arbitrary collection of
     JPEG2000 compressed images can be combined into a single photo album,
     which can then be served as a monolithic entity for highly efficient
     interactive remote image browsing; and
  -- merging of metadata from all source images into the single output JPX
     image, after correctly adjusting and/or inserting cross-references
     between metadata items and image elements.
* The "kdu_show" application has been slightly updated to allow image-wide
  metadata to be displayed and edited interactively.  In the simplest
  instance, this allows the user to view labels which may be attached
  by a content provider to each compositing layer in an image.  In the case
  of photo albums built using the "kdu_merge" tool, each image in the
  album is a separate compositing layer, so the labels might be image
  descriptions.  Of course, these all get delivered correctly by the JPIP
  client-server machinery, during interactive remote image browsing.
* Removed the Intel SSE instruction, PEXTRW, from the code used to accelerate
  image blending in "kdu_region_compositor.cpp", so that plain-old MMX
  compilers and processors will be able to use the accelerations.  This
  results in a very slight slow-down in the accelerated compositing code.
  The blending accelerations have not yet been implemented for other
  SIMD architectures (e.g., Sparc VIS and Altivec), so these will just use
  the default platform-independent implementation.
* "kdu_compress" now supports reading of TIFF images, due to some
  code contributed by John Novak.  To enable TIFF reading, you need to
  define the macro, KDU_INCLUDE_TIFF; you need to obtain the "libtiff"
  package yourself and link against it, but this is very simple.

Changes from version 4.0.3 to version 4.1
-----------------------------------------
* All-new extensive support for JPX files, including the following:
  -- read and write all aspects of virtually all interesting JPX files,
     with only the following exceptions (these should both be included in
     the next release):
     * cross reference boxes and fragment tables not yet implemented
     * no explicit parsing or writing of desired reproduction boxes
  -- read and write multiple codestreams and multiple compositing layers
     * optimizes the use of default parameters where possible, during
       writing.
     * generates a comprehensive reader requirements box
     * maximizes potential for generated files to be JP2 compatible
  -- supports the extended colour descriptions offered by JPX, and manages
     multiple alternate colour descriptions.
  -- extended colour conversion capabilities:
     * allows conversion of the vast majority of JPX colour representations
       to sRGB; of the large number of enumerated JPX colour spaces, only
       CIEJab cannot currently be converted; of the embedded ICC profiles,
       only those involving 3D lookup tables cannot be converted.
     * application can control the trade-off between accuracy and speed
       for colour conversion.
     * application can request extended-gamut conversion
  -- supports all aspects of the JPX composition box, including composition,
     cropping, scaling, positioning and animation.
  -- extensive metadata management facilities
     * imposes a scale-space hierarchy on top of all spatially-sensitive
       metadata to facilitate efficient access.
     * uses scale-space structure to write metadata in a manner which is
       optimized for remote client-server delivery via JPIP.
  -- designed from the ground up to work with client-server systems
     * all aspects of the JPX interface allow for data to arrive
       asynchronously and out of order; interfaces to codestreams,
       compositing layers, etc, may become available incrementally and
       aspects of the data source are parsed only on demand.
     * capabilities proven and tested in the context of Kakadu's JPIP
       client-server architecture
* v4.1 provides a new very high level object, `kdu_region_compositor',
  which builds enormously on the capabilities of `kdu_region_decompressor'.
  This object provides the following new services:
  -- wraps up all the buffer management logic which was currently embedded
     in "kdu_show" within a single platform independent object.
  -- provides full support for compositing multiple images onto a single
     rendering surface, implementing the instructions represented by a
     JPX composition box; supports animation of complex image compositions.
  -- provides an efficient platform-independent alpha-based compositing
     engine.
  -- allows up to two codestreams to be used to build each compositing
     layer (one for colour and one for alpha), and any number of compositing
     layers to be combined on the rendering surface; all of this works
     together with interactive navigation, and dynamic delivery of image
     data from a remote JPIP server, if relevant.
  -- provides extensive integration of metadata (labels, xml, etc.) with
     the imagery, including overlays to symbolically represent the presence
     of spatially sensitive metadata; these are exploited and demonstrated
     by "kdu_show".
* many new features for the "kdu_show" demonstration viewer
  -- exploits all the new JPX features and the many features of the new
     `kdu_region_compositor' object.
  -- allows interactive stepping through animated image compositions,
     individual compositing layers, and even individual components or
     individual codestreams; these capabilities are also hyperlinked to
     the metashow tool.
  -- indicates the presence of spatially sensitive metadata via overlays
     (all implemented by `kdu_region_compositor'), including dynamically
     changing (blinking) overlays.
  -- dynamically overlays metadata text labels as the interactive user
     moves the mouse cursor (while control key is depressed).
  -- provides metadata editing and inspection facilities, with the ability
     to save images whose metadata has been edited; this provides an easy
     to use facility for marking up regions on an existing image.
* a new tool, "kdu_merge", allows complex JPX files to be generated by
  combining information from multiple JP2, JPX and/or MJ2 files
  -- can create composited and animated image sequences in a single JPX file.
  -- can build custom new JPX compositing layers by combining information
     from individual (or even multiple) codestreams in existing files with
     user-defined colour-space specifications.
* new JPX features reflected in various other demo applications, including
  "kdu_compress" and "kdu_expand".
* some minor fixes to the "kdu_server" application
  -- note, however, that kdu_server has yet to be upgraded to a JPX aware
     server.  One reason for this is that some changes to the JPIP standard
     are expected at the December 2003 WG1 meeting, particularly to meet
     the needs of serving JPX files properly.  While JPX files can be served,
     complex files (e.g., composited or animated images) will not
     automatically be served in a manner which meets the expectations of an
     interactive user.
* a memory growth problem has been fixed in the MJ2 file format writing logic.
* some fixes have been included for 64-bit addressing environments; these
  should fix some erratic behaviour during decompression of massive
  compressed images (4GBytes and up, compressed).
* slightly augmented the `kdu_region_decompressor::process' interface so
  as to allow the application to control whether or not the buffer row
  gaps are expressed in terms of pixels or samples -- the changes are
  transparent to existing users of the interface, but allow new applications
  to write directly to buffers with non-pixel-oriented alignment constraints
 (e.g., Windows bitmap buffers).

Changes from version 4.0.2 to version 4.0.3
-------------------------------------------
* Updated the JPIP implementation to make it current with the FCD produced
  out of the Strasbourg ISO/IEC JTC1/SC29/WG1 (JPEG) meeting.  There are,
  unfortunately, a couple of non-backward-compatible changes, so the client
  and server components of this version are not compatible with those of
  version 4.0.2.  The only significant respect in which the current
  implementation is not quite compliant with the JPIP specification is that
  the "mset" request field is not yet implemented -- there is no point in
  implementing this until I have a demo involving targets (e.g. video) with
  a large number of code-streams.  It is unlikely that the "mset" feature
  will be used for regular image browsing applications.  Preference and
  capability signalling by the client are also currently ignored, but in
  most respects Kakadu contains a very comprehensive implementation of the
  JPIP standard.
* Corrected an accidental change introduced to the behaviour of
  `kdu_codestream::flush' made in v3.4, in which the automatic layering
  heuristic introduced 1 layer per octave change in bit-rate -- the
  interface documentation states that the heuristic inserts 2 layers
  per octave (factor of 2) change in bit-rate where upper and lower
  bounds are not explicitly supplied by the application.  The documented
  policy has been restored.
* Included some changes provided by Michael Owen, to add an alternate
  interactive panning mechanism to "kdu_show".  Pressing the shift
  key together with the left mouse button, you can drag the view
  window around with the mouse in a natural way.
* Added a hex dump facility to the "metashow" tool in "kdu_show".
* Added arguments to "kdu_v_expand" to control the quality (number of
  layers), resolution (number of DWT levels) and spatial region associated
  with decompressed video frames.
* Added an optional array argument to the sample processing interfaces
  offered by the high level support objects, `kdu_stripe_compressor' and
  `kdu_stripe_decompressor' so that the application can explicitly control
  whether each of the image components uses a signed or an unsigned
  representation.
* Some minor bug fixes.

Changes from version 4.0.1 to version 4.0.2
-------------------------------------------
* Fixed a bug in the core system which gets excited under
  some configurations of tile opening and closing operations
  when running in the non-persistent mode.  This bug had always
  existed, but could not be uncovered by any of the demo
  applications prior to "kdu_buffered_decompress".
* Fixed a minor bug in the determination of whether or not
  a precinct is significant (in calls to `kdu_precinct::size_packets')
  which sometimes caused the Kakadu server to deliver certain image
  regions very late.  This bug was introduced by some new code to
  improve the efficiency with which small precincts are delivered
  by Kakadu's JPIP server.  Significance determination was not
  available prior to version 4.0.
* Added some new code to avoid needless reading of packets and
  tile parts which are known to be irrelevant to the application,
  where the image is tiled.  Kakadu has always been able to
  selectively parse the code-stream based on the available coding
  options, the application's needs and the availability of random
  access information, but these capabilities have principally been
  targeted toward untiled code-streams.  Since certain military
  applications prefer to use tiles, Kakadu is now much more careful
  about abandoning the parsing of tile data as soon as possible
  when operating in non-persistent mode (persistent mode provides
  no guarantees as to what elements of the code-stream may be
  required at a later stage, in which case efficient access is
  possible only if the code-stream is also endowed with sufficient
  pointer marker segments).

Changes from version 4.0 to version 4.0.1
-----------------------------------------
* Fixed a bug in 1-line of code in "kdu_params.cpp" which adversely
  affected any application which required transcoding services in
  which the original image used precincts with different dimensions
  in at least three different resolution levels.

Changes from version 3.4 to version 4.0
---------------------------------------
This is a MAJOR upgrade, introducing many new features, and providing
an implementation of the new JPIP standard for interactive imaging, which
is now becoming more stable, having reached CD (Committee Draft) status.
A summary of some of the key new features appears below, with the more
important changes listed last.
* All reported bugs have been fixed. For many of these, incremental workarounds
  have previously been published on the Kakadu public news forum, unless
  they were particularly esoteric.  Amongst the various bug fixes are the
  following:
  -- A bug was detected with the compression of tiled video frames using
     "kdu_v_compress".
  -- A bug was reported a long while ago with the construction of Java
     native interfaces.
  -- A bug was reported in connection with the use of certain image
     offsets (canvas coordinates) during compression -- bug was introduced
     accidentally while implementing the incremental code-stream flushing
     feature.
  -- While not exactly a bug, a number of users have reported occasional
     artefacts created while compressing images using Kakadu's block
     truncation prediction algorithm (not everyone is aware of the fact
     that this algorithm is enabled by default).  Although this algorithm
     is an optional speed enhancement, and not guaranteed to be 100% reliable,
     the new version contains a small code fix which dramatically improves
     its reliability with negligible impact on compression speed.
* The `kdu_codestream::set_max_bytes' function now contains an optional
  `simulate_parsing' argument which can be used to simulate the effects of
  parsing away any image components, layers or image resolutions which are
  rendered irrelevant by previous calls to `apply_input_restrictions', before
  applying a byte limit to the size of the resulting code-stream.  Otherwise,
  the byte limit simply truncates the compressed code-stream on input.
  This mode is useful for simulation work.  It may be enabled from the
  "kdu_compress" application, by specifying the `-simulate_parsing' command
  line option.
* "kdu_compress" now supports 32-bit BMP files, the definition of an
  explicit alpha channel (via `-jp2_alpha'), and inclusion of arbitrary
  additional meta-data in JP2 files (via `-jp2_box').
* The widely used `kdr_region_decompressor' object, originally designed as
  the platform independent component of the interactive rendering application,
  "kdu_show", has been given a major face lift.  It is now known as
  `kdu_region_decompressor' and found in the "apps/support".  The object
  now supports the decompression of additional (non-imagery) channels
  such as alpha channels, plus a wide range of buffer structures and data
  precisions to suit just about any region-based decompression application.
  The need to customize this object to meet specific application requirements
  should no longer exist.
* Two new high-level support objects, `kdu_stripe_compressor' and
  `kdu_stripe_decompressor' now wrap up almost all the steps required
  to build a memory-based compression application or a memory-based
  decompression application (although for interactive rendering, you will
  find `kdu_region_decompressor' more convenient than
  `kdu_stripe_decompressor').  They support a wide range of memory buffer
  architectures and data precisions to make application development much
  simpler than before.  Optional configuration arguments allow virtually
  all important Kakadu features to be accessed via these high level objects,
  with the image being compressed/decompressed from/to either a single
  buffer which holds the whole image or incrementally in stripes.
* The "simple_example_c" and "simple_example_d" introductory demo applications
  have been modified to use the `kdu_stripe_compressor' and
  `kdu_stripe_decompressor' objects mentioned above, making them much simpler
  again, while also more powerful.
* Two new example applications, "kdu_buffered_compress" and
  "kdu_buffered_expand" have been added to provide more comprehensive
  (though not complete) demonstration of the `kdu_stripe_compressor' and
  `kdu_stripe_decompressor' objects, and to bridge the gap between the
  very simple examples, "simple_example_c" and "simple_example_d", and
  the much more complex and comprehensive examples, "kdu_compress" and
  "kdu_expand".  The "kdu_buffered_expand" application can be significantly
  more computationally efficient than "kdu_expand" if you are working with
  a huge image which has been tiled.  This is because "kdu_expand" always
  tries to decompress one line at a time from the image, no matter how many
  tiles that line may span -- a reasonable approach in most circumstances,
  but certainly not the only way to use Kakadu.
* The "kdu_show" application has been augmented by the inclusion of a
  meta-data viewer (you can enable the "meta-show" facility by using the
  'm' accelerator, or from the view menu).  While the results might not
  be particularly interesting at present, this code demonstrates walking
  through an arbitrary JP2 family file, caching references to its elements
  and, MOST IMPORTANTLY, dynamically updating the meta-data display as content
  is dynamically transferred by a JPIP server.  The tool has been used to
  carefully test Kakadu's support for dynamic dissemination of both
  imagery and meta-data within the context of the new JPIP standard.  Without
  knowing much about JPIP, you might not be able to see much happening in
  the "meta-show" window yourself right now, but it is a key tool in the
  future road map for Kakadu.
* Kakadu's support for the JP2 file format has been completely revamped from
  the inside out.  If you are just creating and reading JP2 files, you will
  notice only one difference from previous versions.  You no longer open
  the JP2 source or target object directly.  Instead, you first open a
  `jp2_family_src' or `jp2_family_tgt' object, and then pass this to the
  `jp2_source::open' or `jp2_target::open' function.  Behind the scenes,
  however, the new architecture is very different:
  -- Firstly, Kakadu now provides a generic solution for all JP2 family
     file formats, by providing powerful `jp2_input_box' and `jp2_output_box'
     objects, which allow an application to navigate the contents of an
     arbitrary JP2 family file format.  Whenever a code-stream box is
     encountered, it may be passed directly to `kdu_codestream::create'
     to interact with or create the relevant imagery.  Multiple boxes may
     be open into a single file at any given time, allowing the application
     to interact with multiple code-streams simultaneously (amongst other
     things).
  -- Most important of all, the new `jp2_input_box' and `jp2_output_box'
     objects provide complete support for interacting with any object which
     is filling the role of a JPIP interactive image client.  They recognize
     and handle the special facilities provided by the new JPIP standard
     for hierarchically resequencing and streaming the content of an arbitrary
     JP2 family file, along with all of its meta-data.  These mechanisms
     are concealed from applications and file format parsers which are
     built on top of the `jp2_input_box' and `jp2_output_box' objects so
     that full support for efficient, user-sensitive, dynamic interactive
     services is automatically incorporated into any file format for which
     a parsing utility exists at the client side.  Currently, Kakadu provides
     the full set of parsing tools only for the JP2 file format, but the
     next version should bring JPX fully to life.
  -- One consequence of the above is that it is now a simple matter to
     add extra custom boxes to a JP2 file, and to parse these boxes out,
     opening any code-stream boxes as separate images.  These possibilities
     are demonstrated by the "kdu_compress" and "kdu_show" example
     applications, respectively.
* The `kdu_serve' object, representing the platform independent component
  on which Kakadu's image serving capabilities are built, has been thoroughly
  revamped to provide generic support for the various JP2 family file
  formats, interactive delivery of files with multiple code-streams
  (including video), and very much reduced memory consumption when
  deliverying huge images (those in the multi-Gbyte range).  It provides
  automatic as well as client-hinted scheduling of meta-data and image
  data in accordance with file-format and application-specific information
  provided by an auxiliary object, derived from the new `kdu_serve_target'
  interface.  Currently, only one `kdu_serve_target' object is implemented,
  which provides the intelligence necessary to digest raw code-streams and
  JP2 files, but all that is required to support custom JPIP service
  applications or new file formats is to derive an appropriate object
  from `kdu_serve_target'.  We expect to provide such an object for the
  JPX file format with the next release of Kakadu (v4.1).
* The platform dependent client and server components of Kakadu have
  been modified to conform with the syntax and conventions of the new
  JPIP standard (JPEG2000, Part 9), which is currently at CD (Committee
  Draft) status.  For more on JPIP and Kakadu's support for JPIP, consult
  the "jpip-links-and-info.html" file found in the "documentation" directory.
* The various documents, particularly the survey document "kakadu.pdf", have
  been updated to include introductory descriptions and overview diagrams
  for many of the new features offered by version 4.0.

Changes from version 3.3 to version 3.4
---------------------------------------
* The most significant change here is the introduction of incremental
  flushing of the code-stream during compression.  This allows platforms
  with only modest memory resources to compress truly massive images,
  provided you are careful to select the appropriate spatially progressive
  packet progression sequence.  Incremental flushing works with all modes
  of code-stream generation, although to use this feature efficiently you
  should review the extensive documentation which appears in the description
  of the `kdu_codestream::flush' function.
* The rate control policies have also been modified slightly to ensure that
  the most appropriate behaviour occurs when requested quality layer bit-rates
  span many orders of magnitude (e.g., "-rate -,0.001 Clayers=30").
* 64-bit data types and 64-bit file I/O are now supported when compiling
  under GCC, as well as Win32 environments where they have long been
  supported.  Be careful to set _FILE_OFFSET_BITS to 64 in the relevant
  make file, if you want this feature.  The Unix makefiles now also generate
  shared libraries.
* A significant problem has been fixed with the `kdu_server' application.
  The application had no memory or handle leaks per se, but some resources
  were not properly cleaned up until the server was in the process of being
  shut down due to failure to call the key clean-up function regularly.  The
  server also now responds more appropriately to HTTP received from browsers
  which are not JPIP capable.

Changes from version 3.2 to version 3.3
---------------------------------------
* The most significant change is the provision of new
  client and server implementations and support services.
  These are embodied principally through the platform
  independent objects, "kdu_cache2" and "kdu_serve2"
  and the derived platform dependent (WIN32) objects,
  "kdu_client2" and "kdu_server2".  The older versions
  of these objects (those without the "2" suffix),
  supporting the original JPIK protocol, are still shipped
  but are being deprecated.  The new client and server
  implementations support our proposal to the JPIP standard,
  which is compatible with the current working draft produced
  at the ISO/IEC JTC1/SC29/WG1 meeting in Boston, July 2002.
  For more information, the reader is referred initially to
  the supplied document, "jpip-kakadu.pdf", and then to the
  object interface descriptions, and the "Usage_Examples.txt"
  file.
* A couple of bugs in the Java interface building tools
  supplied in v3.2 have been fixed, particularly relevant to
  multi-threaded Java applications.  The fixes were
  contributed by one of our clients who is working extensively
  with Java.
* The "kdu_message" object has been given a new virtual (callback)
  member function, "start_message", which is particularly useful for
  synchronizing message delivery in multi-threaded environments.  This
  is used by the "kdu_show" application to ensure reliable message
  delivery from both the image processing thread and a network
  management thread.
* Minor changes have been made to the JP2 file interfaces to allow
  cleaner support for distributing JP2 files over networks.
* The `jp2_colour::init' functions which generate ICC profiles for
  custom luminance and RGB spaces have been modified to include all
  mandatory ICC tags, rather than just those which are described in
  connection with the JP2 file format.
* Core system services have been included for including user-defined
  comments into the code-stream and recovering them, if necessary.  The
  "kdu_show" application's "File->Properties" menu item now displays
  all text comments it finds in the code-stream.
* The `kdu_codestream::flush' function now supports the inclusion of
  a special COM (comment) marker segment, embedding information about
  the rate-distortion slopes and sizes of the code-stream's quality
  layers, for use in other applications.  "kdu_compress" uses this
  option by default, unless it is explicitly disabled with the `-no_info'
  flag.  The information recorded in the marker segment, if present,
  is used by "kdu_server" to optimize the delivery of data to
  interactive clients using Rate-Distortion performance criteria.
  A brief description of this new feature may be found toward the
  end of the "jpip-kakadu.pdf" document.
* The "kdu_hyperdoc" utility now shortens the generated HTML file names
  to fit within the 33 character limit imposed by some browsers deployed
  in the Apple Mac environment.  A command-line argument may be given
  to restore the original fully expanded file naming convention.
* Some minor bugs have been fixed, relating to parameters with very
  large values, on the order of 2^31 or greater.  This includes a fix
  for problems sometimes encountered when compressing images larger
  than 2GB in size.
* A bug has also been fixed in the processing of encode-time ROI's
  (max-shift method) having large up-shifts (Rshift values).  The
  bug caused incorrect calculation of the distortion contributions
  associated with blocks whose up-shift required a dynamic range
  exceeding 32 bits to fully accommodate both foreground and
  background regions.

Changes from version 3.1 to version 3.2
---------------------------------------
* The most significant new feature is the introduction
  of Java native interface bindings to virtually all
  exposed Kakadu objects and public functions.  The
  Java bindings are automatically generated by the
  "kdu_hyperdoc" utility, which also includes details
  of the Java interfaces with the extensive HTML
  documentation it builds.  See the file,
  "java-interfaces.pdf", for more on building and
  using the Java interfaces.
* Introduced some additional interface functions to
  a number of the existing Kakadu classes, where the
  existing functions do not bind well to Java (or
  perhaps other languages).  This ensures that key
  functionality is available from Java (and maybe later
  other languages) through at least some method.
* Completely eliminated all C++ stream I/O dependencies
  from the core Kakadu system, rationalizing the
  error and warning message handling mechanisms in a way
  which is amenable to foreign language bindings.  This
  has additional advantages, since support for the C++
  I/O stream libraries is shaky or missing on some
  target platforms used by our customers.
  DEVELOPERS SHOULD CAREFULLY REVIEW the new declarations
  of `kdu_customize_errors' and `kdu_customize_warnings'.
* Added an overloaded version of the `kdr_region_decompressor::process'
  function which writes decompressed region data to a compact
  array of pixels, each having a 32-bit representation.  This is
  more efficient for Java applications and also for native
  language implementations which do not keep their entire viewport
  in a single contiguous buffer.
* Included a Java demonstration application,
  "KduRender.java", which uses the Java native interfaces
  to decompress and render an image incrementally to a
  display.  This should work across multiple platforms,
  although we have not yet tested it on anything other
  than the Windows platform.
* Corrected a couple of minor, yet potentially
  dangerous bugs, accidentally introduced in the
  upgrade from version 3.0 to version 3.1.

Changes from version 3.0.8 to version 3.1
-----------------------------------------
* Added a "restart" option to the "kdu_codestream"
  interface, which enables the internal code-stream
  management machinery to be efficiently restarted,
  with particular application to video compression
  and decompression.
* Incorporated MMX optimizations into the compressor
  as well as the decompressor, and fixed a bug in
  the previous MMX implementation which prevented
  the irreversible 5/3 transform (actually a JPEG2000
  Part 2 feature, not Part 1) from behaving correctly.
* Incorporated substantial support for the Motion
  JPEG2000 (MJ2) file format, which complements and
  leverages off the existing JP2 file format support.
  The new implementation should be easier to extend
  to other members of the JP2-family, e.g., JPM or JPX.
* Three new demonstration applications are included:
  -- "kdu_render" shows how the platform independent
     "kdr_region_decompressor" object can be used
     to build JPEG2000 decompression into a whole
     host of applications at a very high level,
     with a minimal amount of effort.
  -- "kdu_v_compress" and "kdu_v_expand are video
     compression and decompression applications.
     There is no interframe compression involved here.
     Instead, only frame-by-frame compression is
     supported, using JPEG2000 code-streams for each
     frame (or field, for interlaced video).  The
     applications use either the Motion JPEG2000
     file format or a much simpler video file format
     which has been created purely for demonstration
     purposes.
* The customizable error and warning message services
  have changed slightly with the addition of a
  user-definable argument to the callback functions.
* Fixed a number of minor bugs

Changes from version 3.0.7 to version 3.0.8
-------------------------------------------
* There is only one significant change here: all
  of the documentation which was distributed throughout
  the publically includable header files is now automatically
  built into a fully integrated documentation system.  You
  should find that the new "kdu_hyperdoc" utility compiles this
  documentation as soon as it is built (using the Makefiles
  or the Visual C++ build environment).  If not, you can always
  build the documentation system manually by running the
  command found in "documentation/hyperdoc.bat" -- you should
  change into the "documentation" directory first.  The
  integrated documentation system is accessed through the
  "index.html" file in the "documentation" directory.
* The names of three member functions have been changed -- the
  previous names were particularly unhelpful.  Specifically,
  "kdu_params::describe_string" and "kdu_params::describe_strings"
  are now called "kdu_params::describe_attribute" and
  "kdu_params::describe_attributes", while
  "jp2_palette::get_num_components" is now called
  "jp2_palette::get_num_luts".
* Some minor bug fixes.

Changes from version 3.0.6 to version 3.0.7
-------------------------------------------
* Corrected a mis-interpretation of ambiguous text
  in the description of the JP2 palette box.  The
  entries appear in entry-major order, as opposed to
  component-major order which previous versions implemented.
  This is an important correction for compliance.
* Fixed a minor bug in "kdu_compress" which made it difficult
  to specify component-specific subband weights.
* Added code to read and write profile identifiers from the
  Rsiz code in the SIZ marker segment.  More importantly,
  Kakadu carefully checks and reports and violations of the
  restrictions associated with individual profiles.  Warning
  messages are generated rather than error messages, allowing
  non-complying code-streams to be read and, optionally,
  transcoded into streams which do comply the a stated
  profile restriction.
* Added support for reading BMP files with incomplete palettes.

Changes from version 3.0.5 to version 3.0.6
-------------------------------------------
* Changed the code for the sYCC space in the JP2
  colour box to 18 from 22 to reflect a recent
  ammendment to the standard.
* Fixed a bug in which marker codes in the range
  FF30 to FF3F (not defined by JPEG2000) were not
  ignored as they are supposed to be.
* Minor enhancements to the "kdu_show" GUI including
  migration to BitBlt to support WinCE users.
* Fixed (I hope) a bug in the "kdu-server" application
  which occasionally caused the server to go into an
  active polling state (rather than blocking on a
  pending condition), thereby chewing up masses of
  CPU time.

Changes from version 3.0.4 to version 3.0.5
-------------------------------------------
* Minor bug fixes in the error resilience section of the
core system, plus some minor efficiency improvements.

Changes from version 3.0.3 to version 3.0.4
-------------------------------------------
* Significant improvements in efficiency of the "kdu_server"
application.  A single machine should now be able to serve
up to perhaps 100 clients simultaneously.  The flow control
protocol has also been substantially improved so as to
more nearly achieve the network capacity while retaining
(improving) interactive responsiveness.

* The "kdu_show" application can now write out compressed
images as JP2 or J2C files, which is particularly useful
when browsing images using JPIK.  Other controls have been
added to access all of the available image components (not
just the first 4) and to directly input JPIK URL's from
the menu.

* A "focus box" capability has been added to the "kdu_show"
application, which allows the user to specify a region of
interest, independently of the view port established by the
window dimensions, scroll and zoom settings.  When used,
the focus box provides the focus for zoom operations and
the region of interest for JPIK remote image browsing
sessions.  When connected to a JPIK server, the size of
the focus box may be limited to maximum dimensions specified
by the server, enabling it to bound the memory and disk
access resources required to serve large regions of interest
to clients.

* A number of minor bugs have been fixed: one which prevented
the "kdu_server" application from behaving properly with
images having certain coding parameters; and one very subtle
bug in the core system which manifested itself on very rare
occasions.

Changes from version 3.0.2 to version 3.0.3
-------------------------------------------
* Minor changes in the "kdu_server" and "kdu_show" applications
  to fix bugs and simplify the description of the JPIK protocol.
* Fixed a bug in the core system which appeared when decompressing
  images containing PLT marker segments and certain tile-part
  organizations.
* Implemented the second form of the "jp2_target::open" function,
  which was accidentally omitted.
* Fixed a minor memory error reported by one client using purify.

Changes from version 3.0.1 to version 3.0.2
-------------------------------------------
* Minor changes in the "kdu_server" and "kdu_show" applications to
  fix a problem which was causing a lot of firewalls to reject the
  traffic.
* Delegation and remote administration added to the capabilities of
  the "kdu_server" application.  It is now possible to review status,
  upload files to and/or shutdown the "kdu_server" application
  remotely under password protection.  It is also possible to serve
  files from a cluster of networked machines with a single public
  IP address.

Changes from version 3.0 to version 3.0.1
-----------------------------------------
* Minor changes in "kdu_server" and "kdu_show" to support a variant
  of the JPIK protocol which replaces the mixed TCP/UDP transport with
  a TCP-only transport.  Opening a URL with the suffix, ":TCP", using
  "kdu_show" will request the TCP-only transport for ongoing
  communications with the server.  The server can support multiple
  clients, where some might use UDP and others not.

Changes from version 2.2.3 to version 3.0
-----------------------------------------
Summary:
        This is a MAJOR upgrade, introducing many new features and
     achieving most of what the Kakadu architecture was originally
     designed for.  The most major new features may be summarized as:
     1) Lots of support for random access into very large compressed files;
     2) The introduction of "interchange" code-streams, with direct access
        to JPEG2000 packet data.
     3) A comprehensive demonstration of Kakadu's support for interactive
        client-server applications.

     As in previous releases, the principle form of documentation is the
     descriptions of public object member functions (interface functions)
     appearing in the relevant public header files.  These have been
     carefully organized for you.  The "kakadu.pdf" document has only
     limited information on the new advanced features introduced with
     version 3.0.  Additional typeset documentation for advanced
     capabilities will be forthcoming in the not too distant future.

Assorted Details:
* Kakadu now employs a customizable variable length data type, "kdu_long"
  (64 bits on Win32 and 64-bit Unix architectures at least) for all
  quantities which may be proportional to image area (as opposed to
  image dimensions), including compressed data size, file seek addresses
  and so forth.
* The core "kdu_codestream" object is able to exploit various pointer
  marker segments which may (optionally) be present to gain random access
  into a JPEG2000 code-stream.  In particular, TLM (Tile-Part Length Main)
  and PLT (Packet Length Tile-Part) marker segments are no longer discarded
  by the code-stream parser.  The compressor can generate PLT marker
  segments (or include them during transcoding with "kdu_transcode");
  however, generation of TLM marker segments cannot be accomplished in a
  single pass operation.  For this reason, a separate utility, "kdu_maketlm",
  is provided to introduce TLM marker segments (if you do not intend to tile
  the image, PLT marker segments are much more useful for random access than
  TLM marker segments, although both may be included).
     The conditions under which the information in PLT marker segments can
  be efficiently utilized are somewhat complex.  The compressed data source
  must advertise the KDU_SOURCE_CAP_SEEKABLE capability (as described in
  "kdu_compressed.h") and all packets of each precinct must appear
  contiguously within the code-stream (the default LRCP packet sequence
  generated by the "kdu_compress" application does not have this property;
  use RPCL, PCRL or CPRL).  The system will generally warn you when PLT
  information is present and ought to be usable apart from the selection of
  an adverse organization for the code-stream.
     When PLT marker segments are available and are able to be utilized,
  a "kdu_codestream" object with the persistent mode set (see
  "kdu_codestream::set_persistent") will unload compressed data from
  memory as soon as possible, since the PLT information may be used to
  reload it on demand.  This permits access to massive files with relatively
  little memory consumption.  Moreover, the system supports internal caching
  to minimize the frequency with which data must be reloaded from disk or
  reparsed from raw code-stream packets. The cache size can be directly
  controlled using "kdu_codestream::augment_cache".
* The "kdu_codestream" object can interact directly with a caching
  compressed data source (one which advertises the KDU_SOURCE_CAP_CACHED
  capability), recovering JPEG2000 packet data directly from such a source
  in any order (partially or in full) as needed.
* A "kdu_cache" object is provided, derived from
  "kdu_compressed_source" which is able to efficiently cache compressed
  data which arrives in any order whatsoever from an external source.
* A "kdu_client" object is provided, derived from "kdu_cache", which
  implements all the functionality required for the client side of an
  interactive client-server application.  Unlike the platform independent
  "kdu_cache" implementation, the "kdu_client" layer is multi-threaded
  and hence platform specific (it is currently implemented only for the
  WIN32 platform, although ports to POSIX threads would not be difficult).
* The "kdu_show" application has been augmented with a richer user
  interface and is able to utilize the "kdu_client" object as one of
  its various "kdu_compressed_source" derived input sources.  In this mode
  the "kdu_show" application becomes a fully fledged JPEG2000 image browsing
  application.  See the "Usage_Examples.txt" file for examples of this.
* The "kdu_codestream::create" function now comes in three different
  forms, rather than just two.  You can create the object for "input"
  (i.e., decompression), for "output" (i.e., compression or transcoding), or
  for "interchange" (this is the new one).  A "kdu_codestream" object opened
  for "interchange" has neither a compressed data source nor a compressed
  data target.  You write code-block data to such an object just like an
  output object, but rather than generating a compressed data stream, you
  can assemble and retrieve JPEG2000 compressed packets directly from the
  object, in any order whatsoever.  Various tools are provided to support
  rate control funcionalities which you might find useful.  Interchange
  objects are provided primarily for use with server applications which
  can then ship custom-built packets to a remote client, which uses them to
  augment a a caching compressed data source.
* A "kdu_serve" object is provided which implements the base functionality
  required to serve compressed data packets to a remote "kdu_cache" object.
  Both "kdu_serve" and "kdu_cache" have platform independent implementations.
* A "kdu_server" application is provided to demonstrate client-server
  capabilities.  Building upon the functionality offered by "kdu_serve",
  it talks directly with the "kdu_client" object activated when the
  "kdu_show" application is started with a network address having
  the protocol prefix, "jpik:".  The server is multi-threaded and hence
  not platform independent (the current WIN32 implementation could be
  ported to POSIX threads without too much difficulty).  The server can
  support multiple clients and multiple sources simultaneously.  It
  can share a single compressed data source across multiple clients
  and is able to take full advantage of the presence of PLT and PLM
  marker segments in the source files to avoid loading any more of the
  source file(s) than is necessary.  The server interacts with the
  client to deliver only the information which is relevant to the
  client's current region, resolution and components of interest.
  Moreover, the server automatically performs in-place transcoding of
  source files to allow the data to be transported using the smallest
  spatial precincts (finest spatial granularity) compatible with the
  code-block dimensions used during compression.  This enables any
  JPEG2000 source file whatsoever to be served up to a client in a
  spatially sensitive manner.
* The "jp2_source" and "jp2_target" objects have been enhanced to
  enable JP2 data to be sourced from or delivered to any
  "kdu_compressed_source" or "kdu_compressed_target" derived objects.
  This immediately allows JP2 functionality to be incorporated into
  memory based compressed data streams, or even caching compressed
  data sources such as "kdu_cache".  This, in turn, allows the
  client-server capabilities described above to work with JP2
  files, as well as raw JPEG2000 code-streams.  For more info,
  you might like to initially consult the comments appearing
  in "simple_example_c.cpp" and "simple_example_d.cpp" and then
  look at "jp2_source::open" and "jp2_target::open".
* The compressor now supports much richer tile-part generation
  capabilities.  Even if the image is not tiled, the single image
  tile can still be split into multiple tile-parts based upon
  resolution, component or layer indices, as controlled by the
  new "ORGtparts" compression attribute (see usage statement for
  "kdu_compress").
* A number of minor bugs and irregularities have been fixed.

Changes from version 2.2.2 to version 2.2.3
-------------------------------------------
* Extremely minor changes to avoid mixed use of formatted and
  unformatted calls to "ostream" objects.  These appear to
  excite a bug in GCC version 3.0.  The only file affected is
  "params.cpp" in "coresys/parameters".

Changes from version 2.2.1 to version 2.2.2
-------------------------------------------
  Note: none of these have any impact whatsoever on executable code
  or DLL's.
* Renamed the "core" directory as "coresys".  A trivial change
  and my appologies for those whom this might adversely affect.
  However, the use of the name "core" was causing some people
  difficulties, since it is identical to the name of a Unix core
  dump file.
* Made the Linux MMX-optimized functions "extern C" instead of
  "extern", so as to avoid problems caused by different name
  mangling conventions between various versions of gcc.
* Eliminated multi-line comments from assembler files so as to
  avoid problems created by earlier versions of the gnu assembler.

Changes from version 2.2 to version 2.2.1
-----------------------------------------
* Replaced the C++ I/O routines used for image and compressed data
  transfers with ANSI C I/O functions.  This was motivated by the
  fact that the new-style ANSI C++ I/O package is unbelievably slow,
  at least as implemented in Microsoft's Visual C++ compiler.
  The change has no impact whatsoever on the Kakadu core system;
  it affects only the implementation of a few application level
  objects -- the core system abstracts all such I/O considerations
  through interface classes which are implemented by applications.
  Everything now runs a little faster than it did in version 2.1 and
  quite a bit faster than it did in the first release of version 2.2.
* Made provision for compiling under versions of GCC prior to version 3.0.
  To use this, you should define GCC_VERSION_LESS_THAN_3.

Changes from version 2.1 to version 2.2
---------------------------------------
* Extensive support for ROI (Region of Interest) specification
  at encode time (see "kakadu.pdf" for more on this).
* Migrated from the old-style C++ iostream package to the new standard
  iostream package -- minimal use of "using namespace std" and never used
  in common header files, so this should enhance namespace protection
  and portability.
* Added AT&T style versions of the small amount of  Pentium assembly code
  to enable compilation of a high speed version under GCC as well as
  MSVC.
* Some minor bug fixes.

Changes from version 2.0.2 to version 2.1
-----------------------------------------
* Extensive support for working with the JP2 file format.  The "kdu_show"
  application demonstrates the capabilities required of a conformant
  JP2 reader: palette mapping; interpolation of components to a common
  resolution; application of registration offsets in the CRG marker
  segment; and colour conversion to an appropriate rendering space (sRGB
  here).  The "kdu_region_decompressor" object provides extensive support
  for general purpose interactive rendering applications, performing all
  of the above tasks in a platform independent manner.
* It is now possible to directly control rate-distortion slope thresholds
  used in the construction of quality layers.  This capability may also
  be used to significantly increase compression speed, if a suitable
  threshold is known, since the encoder then incrementally predicts
  the point at which there is no point in coding further coding passes.
* A number of improvements to the "kdu_show" application, including
  the ability to arbitrarily zoom into images.
* A number of minor bug fixes, including one important bug reported by
  Aaron Deever of Kodak, and a bug which occasionally manifested itself
  in the incremental rate prediction heuristic (reported by Jim Andrew
  of CISRA).
* Improved documentation.

Changes from version 2.0.1 to version 2.02
------------------------------------------
* A PDF document (documentation.pdf) has been prepared to guide the new
  user into some of the more important aspects of the Kakadu system.  The
  first draft is included here.
* A very simple compression example and a very simple decompression example
  have been added to assist developers in familiarizing themselves with
  the Kakadu system -- the existing demo apps provide perhaps too much
  functionality to be quickly understood.
* A full BIBO (Bounded Input Bounded Output) numerical analysis of
  the DWT and other processing steps is used to establish the best
  utilization of limited precision sample data representations.
  The new version should not be able to fall prey to numerical
  overflow or underflow difficulties under any circumstances (this
  could just have been possible with the last version).  It also
  provides slightly higher accuracy.
* The automatic heuristic for generating quality layer rate
  targets has been substantially improved.
* A number of minor bugs/limitations were corrected, although these
  had not manifested themselves in any real examples.


Changes from version 2.0 to version 2.01
----------------------------------------
* One line change in each of "kdu_expand.cpp" and "kdu_compress.cpp" to
  correct a minor rare bug in these demo applications.
* Minor changes in "kdu_show.cpp" to correct a rare bug and improve the
  user interface in regard to image rotation.
* Four lines added to each of "encoder.cpp" and "decoder.cpp" to fix
  a minor memory leak.
